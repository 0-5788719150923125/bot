{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pretraining a ModuleFormer\n---\n[AIGen](https://github.com/LuciferianInk/aigen) is a text generation and training library, originally forked from [AITextGen](https://aitextgen.minimaxir.com/) (which is now defunct).\n\nAIGen is also the foundation of [VTX](https://github.com/0-5788719150923125/vtx).\n\nTo use this notebook with Kaggle, one must first enable the \"Internet\" feature. To do so:\n\n1. Find \"Notebook options\" in the sidebar on the right-hand side of this page.\n2. If required, verify your phone number.\n3. Choose \"Internet on\".\n4. Connect to the P100 accelerator.\n5. Setup file persistence.\n\nDo not forget to connect to an accelerator. The P100's are better than the T4's. However, with 2x T4's available, training may benefit from DistributedDataParallel (DDP) training.","metadata":{}},{"cell_type":"markdown","source":"## Configuration\n\nWe would set a bunch of variables here, if we hadn't hardcoded them below for clarity.","metadata":{}},{"cell_type":"code","source":"# Set some variables\nbase_model = 'ibm/MoLM-350M-4B'\nfocus = 'frame'\n\n# ablation controls\nprune = 0.0\n\n# control actions\ntrain_tokenizer = True\n\n# to continue training from a checkpoint, False starts a fresh run\nresume_training = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Update system packages","metadata":{}},{"cell_type":"code","source":"# Kaggle uses an old version of CUDA, so we need to install a version of Pytorch that was built for that version.\n!pip install torch>=2.1.0 --no-build-isolation --index-url https://download.pytorch.org/whl/cu110\n\n# We don't even use this, but have to install it because of Kaggle bugs\n!pip install torchaudio\n\n# Now we install AIGen\n!pip install 'git+https://github.com/LuciferianInk/aigen.git'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train a tokenizer\n\nMost of this isn't truly necessary here. We are training our own, custom tokenizer - rather than using a default pretrained one.","metadata":{}},{"cell_type":"code","source":"# Tweak the tokenizer\nimport os\nfrom tokenizers import Tokenizer\nfrom transformers import AutoTokenizer, PreTrainedTokenizerFast\nfrom aigen.tokenizers import train_tokenizer\n\noutput_dir = \"/kaggle/working/trained\"\n\n!git clone 'https://github.com/SocioProphet/clymer_research' '/kaggle/working/corpus'\n\ndef list_full_paths_excluding_git(directory):\n    \"\"\"Lists full paths of files in a directory, excluding the .git directory.\n\n    Args:\n        directory (str): The path to the directory to process.\n\n    Returns:\n        list: A list of full file paths.\n    \"\"\"\n\n    file_paths = []\n    for root, d_names, f_names in os.walk(directory):\n        # Exclude the .git directory\n        d_names[:] = [d for d in d_names if d != \".git\"]\n\n        for f in f_names:\n            file_path = os.path.join(root, f)\n            file_paths.append(file_path)\n\n    return file_paths\n\n\nfiles = list_full_paths_excluding_git(\"/kaggle/working/corpus\")\nprint(files)\n\ntokenizer_dir = base_model\ntokenizer_config = dict(\n    cache_dir=output_dir,\n    padding=\"max_length\",\n    padding_side='left',\n    use_fast=True,\n    return_overflowing_tokens=True,\n    truncation=True,\n    trust_remote_code=True,\n    vocab_size=12288\n)\n\nif train_tokenizer:\n    tokenizer_dir = output_dir\n    tokenizer = train_tokenizer(\n        files=files,\n        dropout=None,\n        vocab_size=12288,\n        min_frequency=2,\n        save_path=tokenizer_dir,\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, **tokenizer_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretraining\n\nWe want to train a ModuleFormer from scratch, so we import example code from IBM's MoLM and configure the settings.","metadata":{}},{"cell_type":"code","source":"# Install our fork of ModuleFormer\n!pip install 'git+https://github.com/IBM/ModuleFormer.git'\n\n# Pretrain configs for ModuleFormer\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n)\n\nfrom moduleformer import (\n    ModuleFormerConfig,\n    ModuleFormerForCausalLM,\n    ModuleFormerForSequenceClassification,\n)\n\nAutoConfig.register(\"moduleformer\", ModuleFormerConfig)\nAutoModelForCausalLM.register(ModuleFormerConfig, ModuleFormerForCausalLM)\nAutoModelForSequenceClassification.register(\n    ModuleFormerConfig, ModuleFormerForSequenceClassification\n)\n\npretrain_config = AutoConfig.from_pretrained(base_model)\noverrides = {\n    \"universal\": True,\n    \"world_size\": 23,\n    \"activation_function\": 'silu',\n    \"n_layer\": 16,\n    \"n_head\": 2,\n    \"k_att\": 6,\n    \"k_mlp\": 6,\n    \"n_att_experts\": 120,\n    \"n_mlp_experts\": 60,\n    \"n_ctx\": 256, # history_length * n_layer\n    \"n_embd\": 256,\n    \"att_hidden\": 32,\n    \"ffd_hidden\": 64,\n    \"block_size\": 16,\n    \"history_length\": 16,\n    \"gating_size\": 8,\n    \"gate_type\": 'gmm',\n    \"aux_loss_type\": 'mi',\n    \"aux_loss_weight\": 0.1,\n    \"resid_pdrop\": 0.1,\n    \"embd_pdrop\": 0.1,\n    \"attn_pdrop\": 0.1,\n    \"moe_pdrop\": 0.1,\n    \"sample_topk\": 3,\n    \"vocab_size\": 12288,\n    \"tie_word_embeddings\": True,\n}\nsetattr(pretrain_config, \"_name_or_path\", focus)\nsetattr(pretrain_config, \"bos_token_id\", tokenizer.bos_token_id)\nsetattr(pretrain_config, \"eos_token_id\", tokenizer.eos_token_id)\nfor k, v in overrides.items():\n    setattr(pretrain_config, k, v)\nprint(f\"modified pretrain config:\")\nprint(pretrain_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the model\n\nHere we initialize the model with random weights.","metadata":{}},{"cell_type":"code","source":"# Instantiate your model\nimport os\nimport shutil\nfrom aigen import aigen\n\nif resume_training:\n    model = None\n    model_folder = output_dir\n    pretrain_config = None\nelse:\n    model = base_model\n    model_folder = None\n    shutil.rmtree(output_dir, ignore_errors=True)\n\nprototype = aigen(\n    model=model,\n    model_folder=model_folder,\n    tokenizer=tokenizer,\n    cache_dir=output_dir,\n    precision=32,\n    config=pretrain_config\n)\n\nprint(prototype)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameter-Efficient Fine-Tuning (PEFT)\nHere is a basic example of Low-Rank Adapter training. Currently, we've commented-out this code, because it's not used in pre-training.","metadata":{}},{"cell_type":"code","source":"# # Prepare model for PEFT training\n\n# opts = {\n#     \"r\": 4,\n#     \"alpha\": 16,\n#     \"dropout\": 0.01,\n#     \"bias\": \"all\",\n#     \"target_modules\": [\n#       \"embed_in\",\n#       \"query_key_value\",\n#       \"dense\",\n#       \"dense_h_to_4h\",\n#       \"dense_4h_to_h\",\n#       \"embed_out\"\n#     ]\n# }\n\n# prototype.create_adapter(output_dir, opts)\n\n# prototype.model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics\n\nWe want to log training metrics, so we install Tensorboard and expose it via ngrok. This requires an authtoken from ngrok.com, saved in Kaggle's \"Add-ons>Secrets\".","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"NGROK_SECRET\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nimport os\nimport shutil\n\ndirectory = \"/kaggle/working/logs\"\nos.makedirs(directory, exist_ok=True)\n\nif not resume_training:\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        shutil.rmtree(file_path)\n\nif secret_value:\n\n    !pip install ngrok tensorboard\n\n    import threading\n    import subprocess\n\n    def start_tensorboard():\n        subprocess.Popen(\n            [\"tensorboard\", \"--logdir\", \"/kaggle/working/logs\", \"--bind_all\", \"--samples_per_plugin\", \"scalars=999999999\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT\n        )\n\n    tensorboard_thread = threading.Thread(target=start_tensorboard)\n    tensorboard_thread.start()\n\n    import ngrok\n\n    listener = await ngrok.forward(6006, authtoken=secret_value)\n    \n    import time\n\n    time.sleep(1)\n    print(listener.url())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nFinally, we train the model on a dataset streamed from: https://huggingface.co/datasets","metadata":{}},{"cell_type":"code","source":"# Train the model\n\nimport os\nfrom lightning.pytorch import loggers\n\nos.makedirs(f\"/kaggle/working/logs/{focus}\", exist_ok=True)\nlogger = loggers.TensorBoardLogger(\"/kaggle/working/logs\", name=focus, default_hp_metric=True)\n\nprototype.train(\n    devices=1,\n    strategy=\"auto\",\n    prune=prune,\n    streaming_data=[\n        {\n            \"repo\": \"c4\", \n            \"content_key\": \"text\", \n            \"subset\": \"en.noblocklist\",\n            \"sequential\": True,\n            \"buffer_size\": 10000,\n            \"val_samples\": 10000\n        }\n    ],\n    batch_size=32,\n    gradient_accumulation_steps=32,\n    block_size=512,\n    num_steps=10000,\n    val_interval=100,\n    warmup_steps=10,\n    optimizer=\"Lion\",\n    learning_rate=0.000333,\n    weight_decay=0.1,\n    gradient_clip_val=1.0,\n    scheduler=\"cosine\",\n    lookahead=5,\n    loggers=[logger],\n    gradient_checkpointing=True,\n    generate_every=5,\n    save_every=10,\n    checkpoint_every=10,\n    resume=resume_training,\n    progress_bar=True,\n    output_dir=output_dir,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing\n\nFor testing, we just run an interactive inference session.","metadata":{}},{"cell_type":"code","source":"# Test inference\n\nwhile True:\n    print(\"PROMPT:\\n\")\n    prompt = input()\n    completion = prototype.generate(\n        prompt=prompt,\n        do_sample=True,\n        min_length=23,\n        max_new_tokens=111,\n        temperature=0.9,\n        eta_cutoff=0.0003,\n        penalty_alpha=0.6,\n        top_k=4,\n        repetition_penalty=1.023,\n        no_repeat_ngram_size=13,\n        renormalize_logits=True,\n        remove_invalid_values=True,\n        max_time=60,\n        use_cache=True,\n    )\n    print(\"COMPLETION:\\n\")\n    print(completion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}