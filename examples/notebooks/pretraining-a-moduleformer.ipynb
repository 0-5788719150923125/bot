{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VTX\n---\n[AIGen](https://github.com/LuciferianInk/aigen) is a text generation and training library, originally forked from [AITextGen](https://aitextgen.minimaxir.com/) (which is now defunct).\n\nAIGen is also the foundation of [VTX](https://github.com/0-5788719150923125/vtx).\n\nTo use this notebook with Kaggle, one must first enable the \"Internet\" feature. To do so:\n\n1. Find \"Notebook options\" in the sidebar on the right-hand side of this page.\n2. If required, verify your phone number.\n3. Choose \"Internet on\"\n\nDo not forget to connect to an accelerator. The P100's are better than the T4's. However, with 2x T4's available, training may benefit from DistributedDataParallel (DDP) training.","metadata":{}},{"cell_type":"code","source":"# Kaggle uses an old version of CUDA, so we need to install a version of Pytorch that was built for that version.\n!pip install torch>=2.1.0 --no-build-isolation --index-url https://download.pytorch.org/whl/cu110\n\n# We don't even use this, but have to install it because of Kaggle bugs\n!pip install torchaudio\n\n# Now we install AIGen\n!pip install 'git+https://github.com/LuciferianInk/aigen.git'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration\n\nWe could set a bunch of variables here, but we don't. For now, we just hardcode things in the steps below for clarity.","metadata":{}},{"cell_type":"code","source":"# Set some variables\nfocus = 'ode'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretraining\n\nWe want to train a ModuleFormer from scratch, so we import example code from IBM's MoLM and configure the settings.","metadata":{}},{"cell_type":"code","source":"# Install our fork of ModuleFormer\n!pip install 'git+https://github.com/LuciferianInk/ModuleFormer.git@enable-gradient-checkpointing'\n\n# Pretrain configs for ModuleFormer\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n)\n\nfrom moduleformer import (\n    ModuleFormerConfig,\n    ModuleFormerForCausalLM,\n    ModuleFormerForSequenceClassification,\n)\n\nAutoConfig.register(\"moduleformer\", ModuleFormerConfig)\nAutoModelForCausalLM.register(ModuleFormerConfig, ModuleFormerForCausalLM)\nAutoModelForSequenceClassification.register(\n    ModuleFormerConfig, ModuleFormerForSequenceClassification\n)\n\nbase_model = \"ibm/MoLM-350M-4B\"\n\npretrain_config = AutoConfig.from_pretrained(base_model)\noverrides = {\n    \"universal\": True,\n    \"world_size\": 23,\n    \"activation_function\": 'gelu',\n    \"n_layer\": 8,\n    \"n_head\": 2,\n    \"k_att\": 3,\n    \"k_mlp\": 3,\n    \"n_att_experts\": 8,\n    \"n_mlp_experts\": 16,\n    \"n_ctx\": 2048, # history_length * n_layer\n    \"n_embd\": 768,\n    \"att_hidden\": 256,\n    \"ffd_hidden\": 512,\n    \"block_size\": 128,\n    \"gate_type\": 'gmm',\n    \"gating_size\": 64,\n    \"aux_loss_type\": 'mi',\n    \"aux_loss_weight\": 0.1,\n    \"history_length\": 128,\n    \"resid_pdrop\": 0.1,\n    \"embd_pdrop\": 0.1,\n    \"attn_pdrop\": 0.1,\n    \"moe_pdrop\": 0.1,\n    \"sample_topk\": 2,\n    \"vocab_size\": 12288,\n    \"tie_word_embeddings\": True,\n}\nsetattr(pretrain_config, \"_name_or_path\", focus)\nfor k, v in overrides.items():\n    setattr(pretrain_config, k, v)\nprint(f\"modified pretrain config:\")\nprint(pretrain_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load a pretrained tokenizer\n\nThis isn't actually necessary here, but it can be required in some cases.","metadata":{}},{"cell_type":"code","source":"# Tweak the tokenizer\nimport os\nfrom tokenizers import Tokenizer\nfrom transformers import AutoTokenizer, PreTrainedTokenizerFast\nfrom aigen.tokenizers import train_tokenizer\n\n!git clone 'https://github.com/SocioProphet/clymer_research' '/kaggle/working/corpus'\n\ntokenizer_config = dict(\n    cache_dir=\"/kaggle/working/tokenizers\",\n    padding=\"max_length\",\n    padding_side='left',\n    use_fast=True,\n    return_overflowing_tokens=True,\n    truncation=True,\n    trust_remote_code=True,\n)\n\nimport os\n\ndef list_full_paths_excluding_git(directory):\n    \"\"\"Lists full paths of files in a directory, excluding the .git directory.\n\n    Args:\n        directory (str): The path to the directory to process.\n\n    Returns:\n        list: A list of full file paths.\n    \"\"\"\n\n    file_paths = []\n    for root, d_names, f_names in os.walk(directory):\n        # Exclude the .git directory\n        d_names[:] = [d for d in d_names if d != \".git\"]\n\n        for f in f_names:\n            file_path = os.path.join(root, f)\n            file_paths.append(file_path)\n\n    return file_paths\n\n\nfiles = list_full_paths_excluding_git(\"/kaggle/working/corpus\")\nprint(files)\n\ntrain_tokenizer(\n    files=files,\n    dropout=0.9,\n    vocab_size=12288,\n    min_frequency=2,\n    save_path=\"/kaggle/working/tokenizers\",\n    prefix=focus,\n    serialize=True,\n    trim_offsets=True,\n)\ntokenizer_file = f\"/kaggle/working/tokenizers/{focus}/tokenizer.json\"\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=tokenizer_file,\n    bos_token=\"<|endoftext|>\",\n    eos_token=\"<|endoftext|>\",\n    unk_token=\"<|endoftext|>\",\n    pad_token=\"<|endoftext|>\",\n    **tokenizer_config,\n)\n\n\n\n# tokenizer = AutoTokenizer.from_pretrained(\n#     base_model,\n#     cache_dir=\"/kaggle/working/models\",\n#     padding=\"max_length\",\n#     padding_side=\"left\",\n#     use_fast=True,\n#     return_overflowing_tokens=True,\n#     truncation=True,\n#     trust_remote_code=True,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the model\n\nHere we initialize the model with random weights.","metadata":{}},{"cell_type":"code","source":"# Instantiate your model\nimport os\nimport shutil\nfrom aigen import aigen\n\n# Use this to continue training.\nresume_training = False\n\nif resume_training:\n    model = None\n    model_folder = \"/kaggle/working/trained\"\n    pretrain_config = None\nelse:\n    model = base_model\n    model_folder = None\n    shutil.rmtree(f\"/kaggle/working/trained\")\n\nprototype = aigen(\n    model=model,\n    model_folder=model_folder,\n    tokenizer=tokenizer,\n    cache_dir=\"/kaggle/working/models\",\n    precision=32,\n    config=pretrain_config\n)\n\nprint(prototype)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameter-Efficient Fine-Tuning (PEFT)\nHere is a basic example of Low-Rank Adapter training. Currently, we've commented-out this code, because it's not used in pre-training.","metadata":{}},{"cell_type":"code","source":"# # Prepare model for PEFT training\n\n# opts = {\n#     \"r\": 4,\n#     \"alpha\": 16,\n#     \"dropout\": 0.01,\n#     \"bias\": \"all\",\n#     \"target_modules\": [\n#       \"embed_in\",\n#       \"query_key_value\",\n#       \"dense\",\n#       \"dense_h_to_4h\",\n#       \"dense_4h_to_h\",\n#       \"embed_out\"\n#     ]\n# }\n\n# prototype.create_adapter(\"/kaggle/working/trained\", opts)\n\n# prototype.model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics\n\nWe want to log training metrics, so we install Tensorboard and expose it via ngrok. This requires an authtoken from ngrok.com, saved in Kaggle's \"Add-ons>Secrets\".","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"NGROK_SECRET\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nimport os\nimport shutil\n\nif not resume_training:\n    directory = \"/kaggle/working/logs\"\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        shutil.rmtree(file_path)\n\nif secret_value:\n\n    !pip install ngrok tensorboard\n\n    import threading\n    import subprocess\n\n    def start_tensorboard():\n        subprocess.Popen(\n            [\"tensorboard\", \"--logdir\", \"/kaggle/working/logs\", \"--bind_all\", \"--samples_per_plugin\", \"scalars=999999999\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT\n        )\n\n    tensorboard_thread = threading.Thread(target=start_tensorboard)\n    tensorboard_thread.start()\n\n    import ngrok\n\n    listener = await ngrok.forward(6006, authtoken=secret_value)\n    \n    import time\n\n    time.sleep(1)\n    print(listener.url())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nFinally, we train the model on a dataset streamed from: https://huggingface.co/datasets","metadata":{}},{"cell_type":"code","source":"# Train the model\n\nimport os\nfrom lightning.pytorch import loggers\n\nos.makedirs(f\"/kaggle/working/logs/{focus}\", exist_ok=True)\nlogger = loggers.TensorBoardLogger(\"/kaggle/working/logs\", name=focus, default_hp_metric=True)\n\nprototype.train(\n    devices=-1,\n    strategy=\"auto\",\n    streaming_data=[\n        {\n            \"repo\": \"c4\", \n            \"content_key\": \"text\", \n            \"subset\": \"en.noblocklist\",\n            \"sequential\": True,\n            \"buffer_size\": 1000,\n            \"val_samples\": 10000\n        }\n    ],\n    batch_size=32,\n    gradient_accumulation_steps=32,\n    block_size=512,\n    num_steps=10000,\n    val_interval=100,\n    warmup_steps=10,\n    optimizer=\"Lion\",\n    learning_rate=0.000333,\n    weight_decay=0.01,\n    gradient_clip_val=1.0,\n    scheduler=\"cosine\",\n    lookahead=5,\n    generate_every=1000,\n    save_every=5000,\n    loggers=[logger],\n    gradient_checkpointing=True,\n    checkpoint_every=10,\n    resume=resume_training,\n    output_dir=\"/kaggle/working/trained\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing\n\nFor testing, we just run an interactive inference session.","metadata":{}},{"cell_type":"code","source":"# Test inference\n\nwhile True:\n    print(\"PROMPT:\\n\")\n    prompt = input()\n    completion = prototype.generate(\n        prompt=prompt,\n        do_sample=True,\n        min_length=23,\n        max_new_tokens=111,\n        temperature=0.9,\n        eta_cutoff=0.0003,\n        penalty_alpha=0.6,\n        top_k=4,\n        repetition_penalty=1.023,\n        no_repeat_ngram_size=13,\n        renormalize_logits=True,\n        remove_invalid_values=True,\n        max_time=60,\n        use_cache=True,\n    )\n    print(\"COMPLETION:\\n\")\n    print(completion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}