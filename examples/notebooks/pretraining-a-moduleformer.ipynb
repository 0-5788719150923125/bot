{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VTX\n---\n[AIGen](https://github.com/LuciferianInk/aigen) is a text generation and training library, originally forked from [AITextGen](https://aitextgen.minimaxir.com/) (which is now defunct).\n\nAIGen is also the foundation of [VTX](https://github.com/0-5788719150923125/vtx).\n\nTo use this notebook with Kaggle, one must first enable the \"Internet\". To do so:\n\n1. Find \"Notebook options\" in the sidebar on the right-hand side of this page.\n2. If required, verify your phone number.\n3. Choose \"Internet on\"\n\nAs well, do not forget to connect to an accelerator. The P100's are better for training.","metadata":{}},{"cell_type":"code","source":"# Kaggle uses an old version of CUDA, so we need to install a version of Pytorch that was built for that version.\n!pip install torch>=2.1.0 --no-build-isolation --index-url https://download.pytorch.org/whl/cu110\n\n# We don't even use this, but have to install it because of Kaggle bugs\n!pip install torchaudio\n\n# Now we install AIGen\n!pip install 'git+https://github.com/LuciferianInk/aigen.git'\n\n# Install our fork of ModuleFormer\n!pip install 'git+https://github.com/LuciferianInk/ModuleFormer.git@enable-gradient-checkpointing'","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:46:05.825082Z","iopub.execute_input":"2023-12-21T21:46:05.825466Z","iopub.status.idle":"2023-12-21T21:48:33.701966Z","shell.execute_reply.started":"2023-12-21T21:46:05.825436Z","shell.execute_reply":"2023-12-21T21:48:33.700838Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchaudio) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchaudio) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchaudio) (1.3.0)\nCollecting git+https://github.com/LuciferianInk/aigen.git\n  Cloning https://github.com/LuciferianInk/aigen.git to /tmp/pip-req-build-4hiw3rso\n  Running command git clone --filter=blob:none --quiet https://github.com/LuciferianInk/aigen.git /tmp/pip-req-build-4hiw3rso\n  Resolved https://github.com/LuciferianInk/aigen.git to commit d83be748faa77cc4163024daa4428e21d626bbcb\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs (from aigen==0.7.0)\n  Cloning https://github.com/LuciferianInk/lightning-Hivemind.git (to revision ipfs) to /tmp/pip-install-qod3f39o/lightning-hivemind_70043684388043f8b9299536aec12cf3\n  Running command git clone --filter=blob:none --quiet https://github.com/LuciferianInk/lightning-Hivemind.git /tmp/pip-install-qod3f39o/lightning-hivemind_70043684388043f8b9299536aec12cf3\n  Running command git checkout -b ipfs --track origin/ipfs\n  Switched to a new branch 'ipfs'\n  Branch 'ipfs' set up to track remote branch 'ipfs' from 'origin'.\n  Resolved https://github.com/LuciferianInk/lightning-Hivemind.git to commit 5aa57bfb2a5b277b48abf167c8468a54e2baee8a\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from aigen==0.7.0) (0.25.0)\nCollecting datasets>=2.14.5 (from aigen==0.7.0)\n  Obtaining dependency information for datasets>=2.14.5 from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nCollecting deepspeed>=0.11.1 (from aigen==0.7.0)\n  Downloading deepspeed-0.12.6.tar.gz (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fire>=0.5.0 (from aigen==0.7.0)\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting lightning>=2.1.0 (from aigen==0.7.0)\n  Obtaining dependency information for lightning>=2.1.0 from https://files.pythonhosted.org/packages/8c/a1/b2a6c33675510bc3e1ca6d010b244ac0dd9c81fc1723a37e7491aa586041/lightning-2.1.3-py3-none-any.whl.metadata\n  Downloading lightning-2.1.3-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting peft>=0.6.2 (from aigen==0.7.0)\n  Obtaining dependency information for peft>=0.6.2 from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nCollecting pytorch_optimizer>=2.12.0 (from aigen==0.7.0)\n  Obtaining dependency information for pytorch_optimizer>=2.12.0 from https://files.pythonhosted.org/packages/1e/a8/f4a45da4d65d4642f7d67e2097f69418a1556718634438c6c44dab9d74f7/pytorch_optimizer-2.12.0-py3-none-any.whl.metadata\n  Downloading pytorch_optimizer-2.12.0-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from aigen==0.7.0) (2.0.0)\nRequirement already satisfied: transformers>=4.35.2 in /opt/conda/lib/python3.10/site-packages (from aigen==0.7.0) (4.36.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->aigen==0.7.0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->aigen==0.7.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->aigen==0.7.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->aigen==0.7.0) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->aigen==0.7.0) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->aigen==0.7.0) (0.4.1)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (11.0.0)\nCollecting pyarrow-hotfix (from datasets>=2.14.5->aigen==0.7.0)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets>=2.14.5->aigen==0.7.0)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->aigen==0.7.0) (3.8.5)\nCollecting hjson (from deepspeed>=0.11.1->aigen==0.7.0)\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.11.1->aigen==0.7.0) (1.11.1.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.11.1->aigen==0.7.0) (9.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.11.1->aigen==0.7.0) (1.10.12)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.11.1->aigen==0.7.0) (11.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire>=0.5.0->aigen==0.7.0) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire>=0.5.0->aigen==0.7.0) (2.3.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning>=2.1.0->aigen==0.7.0) (0.10.0)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning>=2.1.0->aigen==0.7.0) (1.2.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning>=2.1.0->aigen==0.7.0) (4.5.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning>=2.1.0->aigen==0.7.0) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->aigen==0.7.0) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->aigen==0.7.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->aigen==0.7.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->aigen==0.7.0) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.2->aigen==0.7.0) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.2->aigen==0.7.0) (0.15.0)\nCollecting hivemind<=1.1.10.post2,>=1.1.0 (from lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Downloading hivemind-1.1.10.post2.tar.gz (242 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->aigen==0.7.0) (1.3.1)\nRequirement already satisfied: scipy>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (1.11.4)\nCollecting prefetch-generator>=1.0.1 (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Downloading prefetch_generator-1.0.3.tar.gz (4.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: msgpack>=0.5.6 in /opt/conda/lib/python3.10/site-packages (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (1.0.5)\nRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (2.4.0)\nRequirement already satisfied: uvloop>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (0.19.0)\nCollecting grpcio-tools>=1.33.2 (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Obtaining dependency information for grpcio-tools>=1.33.2 from https://files.pythonhosted.org/packages/3c/7d/00a156dba65c9965e6e94988ab518c4ea88f95e1b70c2b61b34dd65124b5/grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nRequirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.10/site-packages (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (3.20.3)\nCollecting configargparse>=1.2.3 (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Obtaining dependency information for configargparse>=1.2.3 from https://files.pythonhosted.org/packages/6f/b3/b4ac838711fd74a2b4e6f746703cf9dd2cf5462d17dac07e349234e21b97/ConfigArgParse-1.7-py3-none-any.whl.metadata\n  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\nCollecting multiaddr>=0.0.9 (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Downloading multiaddr-0.0.9-py2.py3-none-any.whl (16 kB)\nCollecting pymultihash>=0.8.2 (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Downloading pymultihash-0.8.2-py3-none-any.whl (13 kB)\nRequirement already satisfied: cryptography>=3.4.6 in /opt/conda/lib/python3.10/site-packages (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (41.0.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.1.0->aigen==0.7.0) (68.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.22.0->aigen==0.7.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.5->aigen==0.7.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.5->aigen==0.7.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.5->aigen==0.7.0) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->aigen==0.7.0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.5->aigen==0.7.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.5->aigen==0.7.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.5->aigen==0.7.0) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->aigen==0.7.0) (1.3.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.4.6->hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (1.15.1)\nCollecting protobuf>=3.12.2 (from hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Obtaining dependency information for protobuf>=3.12.2 from https://files.pythonhosted.org/packages/ae/5b/7ed02a9b8e752c8f7bca8661779c0275b9e3e6a903a3045e6da51f796dda/protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata\n  Using cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting grpcio>=1.60.0 (from grpcio-tools>=1.33.2->hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Obtaining dependency information for grpcio>=1.60.0 from https://files.pythonhosted.org/packages/ed/bd/4dbe2ae13ffba7eef2a3bd2dcebbc2255da18d1a972a89952d55e8ad3d4b/grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting varint (from multiaddr>=0.0.9->hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Downloading varint-1.0.2.tar.gz (1.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting base58 (from multiaddr>=0.0.9->hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\nCollecting netaddr (from multiaddr>=0.0.9->hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0)\n  Obtaining dependency information for netaddr from https://files.pythonhosted.org/packages/68/4a/aa1a61a8cdfc533020012594b006bbb3e7680326e1c247f63e711be5ab76/netaddr-0.9.0-py3-none-any.whl.metadata\n  Downloading netaddr-0.9.0-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4.6->hivemind<=1.1.10.post2,>=1.1.0->lightning_hivemind@ git+https://github.com/LuciferianInk/lightning-Hivemind.git@ipfs->aigen==0.7.0) (2.21)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytorch_optimizer-2.12.0-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.8/155.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\nUsing cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nUsing cached grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\nDownloading netaddr-0.9.0-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: aigen, deepspeed, fire, lightning_hivemind, hivemind, prefetch-generator, varint\n  Building wheel for aigen (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for aigen: filename=aigen-0.7.0-py3-none-any.whl size=23042 sha256=60165acb9535fdfe7c9aebf999f2a96732e63c19ae106cc116e4d6a821fddf03\n  Stored in directory: /tmp/pip-ephem-wheel-cache-iy6qb9ey/wheels/21/9f/1c/2a83b6c87de094a900a6302859f2957c3b3c29765e899445d2\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.12.6-py3-none-any.whl size=1306741 sha256=a7939de9603f23d3231cf6c5dd3bd42664608e57d2d48d1c9ce07e6a27fc47cf\n  Stored in directory: /root/.cache/pip/wheels/a3/dc/a2/f585faaed4dec84108916dcc8e8a7c129a216df8202ca32984\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=ec9b0981ea0260d09143a4a1e1329f95d0a216babc1b7c6e9962917353925f48\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for lightning_hivemind (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lightning_hivemind: filename=lightning_Hivemind-0.1.0-py3-none-any.whl size=13124 sha256=132cf31bb93d64525c86059d414e75a9843bd6b86df53c0629465cf4c358c52c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-iy6qb9ey/wheels/15/e6/5d/8c7a0028f31bf444b8e5f542e818a65b14f7632ce3cdeb4fd0\n  Building wheel for hivemind (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hivemind: filename=hivemind-1.1.10.post2-py3-none-any.whl size=9011595 sha256=340df1262757fd39c17c29775e5a728ff64b9fc8192fa9a8a7f9b80a02f8e982\n  Stored in directory: /root/.cache/pip/wheels/53/98/e3/d51ae54ae8302c5a1872fae2e85a8e8291651af19f550965c1\n  Building wheel for prefetch-generator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for prefetch-generator: filename=prefetch_generator-1.0.3-py3-none-any.whl size=4758 sha256=fee569cbc257ff8877eaa717b510cf0d5d0fc3bd89e57ce8354f46606c75f8c8\n  Stored in directory: /root/.cache/pip/wheels/65/65/44/ed059bc23eeda93cfb19b58113423882cd9febae7ebf3f9ddf\n  Building wheel for varint (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for varint: filename=varint-1.0.2-py3-none-any.whl size=1961 sha256=138016e934fdaf625a1f3f26ceec0bd438142f95d8c08aaf44d822071c8dddba\n  Stored in directory: /root/.cache/pip/wheels/39/48/5e/33919c52a2a695a512ca394a5308dd12626a40bbcd288de814\nSuccessfully built aigen deepspeed fire lightning_hivemind hivemind prefetch-generator varint\nInstalling collected packages: varint, pymultihash, prefetch-generator, netaddr, hjson, pyarrow-hotfix, protobuf, grpcio, fsspec, fire, configargparse, base58, multiaddr, grpcio-tools, pytorch_optimizer, hivemind, deepspeed, datasets, peft, lightning, lightning_hivemind, aigen\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.1 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.1 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.1 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aigen-0.7.0 base58-2.1.1 configargparse-1.7 datasets-2.15.0 deepspeed-0.12.6 fire-0.5.0 fsspec-2023.10.0 grpcio-1.57.0 grpcio-tools-1.60.0 hivemind-1.1.10.post2 hjson-3.1.0 lightning-2.1.3 lightning_hivemind-0.1.0 multiaddr-0.0.9 netaddr-0.9.0 peft-0.7.1 prefetch-generator-1.0.3 protobuf-4.21.12 pyarrow-hotfix-0.6 pymultihash-0.8.2 pytorch_optimizer-2.12.0 varint-1.0.2\nCollecting git+https://github.com/LuciferianInk/ModuleFormer.git@enable-gradient-checkpointing\n  Cloning https://github.com/LuciferianInk/ModuleFormer.git (to revision enable-gradient-checkpointing) to /tmp/pip-req-build-ur3m8jwd\n  Running command git clone --filter=blob:none --quiet https://github.com/LuciferianInk/ModuleFormer.git /tmp/pip-req-build-ur3m8jwd\n  Running command git checkout -b enable-gradient-checkpointing --track origin/enable-gradient-checkpointing\n  Switched to a new branch 'enable-gradient-checkpointing'\n  Branch 'enable-gradient-checkpointing' set up to track remote branch 'enable-gradient-checkpointing' from 'origin'.\n  Resolved https://github.com/LuciferianInk/ModuleFormer.git to commit 3824d869537d2d80bf44e68bae830d1b93614f57\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from moduleformer==0.0.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from moduleformer==0.0.0) (4.36.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->moduleformer==0.0.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->moduleformer==0.0.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->moduleformer==0.0.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->moduleformer==0.0.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->moduleformer==0.0.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->moduleformer==0.0.0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->moduleformer==0.0.0) (2023.10.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers->moduleformer==0.0.0) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->moduleformer==0.0.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->moduleformer==0.0.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->moduleformer==0.0.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->moduleformer==0.0.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->moduleformer==0.0.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->moduleformer==0.0.0) (1.3.0)\nBuilding wheels for collected packages: moduleformer\n  Building wheel for moduleformer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for moduleformer: filename=moduleformer-0.0.0-py3-none-any.whl size=24539 sha256=58d48639f416f65d3adb8714dd465fd19ceb809e442f1500e6edd7d6ec077132\n  Stored in directory: /tmp/pip-ephem-wheel-cache-74ot30_6/wheels/6c/50/83/b754dba5f11a2745dc68719543c6d3c656ef7fe2c7bdb3d5a5\nSuccessfully built moduleformer\nInstalling collected packages: moduleformer\nSuccessfully installed moduleformer-0.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Configuration\n\nWe could set a bunch of variables here, but we don't. For now, we just hardcode things inline for clarity.","metadata":{}},{"cell_type":"code","source":"# Set some variables\nimport os\n\nfocus = 'frame'\nos.environ[\"FOCUS\"] = focus\nos.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:48:33.704263Z","iopub.execute_input":"2023-12-21T21:48:33.704574Z","iopub.status.idle":"2023-12-21T21:48:33.710452Z","shell.execute_reply.started":"2023-12-21T21:48:33.704544Z","shell.execute_reply":"2023-12-21T21:48:33.709388Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Pretraining\n\nWe want to train a ModuleFormer from scratch, so we import example code from IBM's MoLM and configure the settings.","metadata":{}},{"cell_type":"code","source":"# Pretrain configs for ModuleFormer\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n)\n\nfrom moduleformer import (\n    ModuleFormerConfig,\n    ModuleFormerForCausalLM,\n    ModuleFormerForSequenceClassification,\n)\n\nAutoConfig.register(\"moduleformer\", ModuleFormerConfig)\nAutoModelForCausalLM.register(ModuleFormerConfig, ModuleFormerForCausalLM)\nAutoModelForSequenceClassification.register(\n    ModuleFormerConfig, ModuleFormerForSequenceClassification\n)\n\nbase_model = \"ibm/MoLM-350M-4B\"\n\npretrain_config = AutoConfig.from_pretrained(base_model)\noverrides = {\n    \"universal\": True,\n    \"world_size\": 23,\n    \"activation_function\": 'gelu',\n    \"n_layer\": 16,\n    \"n_head\": 2,\n    \"k_att\": 3,\n    \"k_mlp\": 3,\n    \"n_att_experts\": 8,\n    \"n_mlp_experts\": 16,\n    \"n_ctx\": 2048, # history_length * n_layer\n    \"n_embd\": 768,\n    \"att_hidden\": 256,\n    \"ffd_hidden\": 512,\n    \"block_size\": 128,\n    \"gate_type\": 'gmm',\n    \"gating_size\": 64,\n    \"aux_loss_type\": 'mi',\n    \"aux_loss_weight\": 0.1,\n    \"history_length\": 128,\n    \"resid_pdrop\": 0.1,\n    \"embd_pdrop\": 0.1,\n    \"attn_pdrop\": 0.1,\n    \"moe_pdrop\": 0.1,\n    \"sample_topk\": 2,\n    \"tie_word_embeddings\": True,\n}\nsetattr(pretrain_config, \"_name_or_path\", focus)\nfor k, v in overrides.items():\n    setattr(pretrain_config, k, v)\nprint(f\"modified pretrain config:\")\nprint(pretrain_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:48:33.711864Z","iopub.execute_input":"2023-12-21T21:48:33.712179Z","iopub.status.idle":"2023-12-21T21:48:38.290619Z","shell.execute_reply.started":"2023-12-21T21:48:33.712154Z","shell.execute_reply":"2023-12-21T21:48:38.289555Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/jit/annotations.py:310: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n  warnings.warn(\"TorchScript will treat type annotations of Tensor \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/952 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a11a9213f494e69aba18bdc7e055e50"}},"metadata":{}},{"name":"stdout","text":"modified pretrain config:\nModuleFormerConfig {\n  \"_name_or_path\": \"frame\",\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"ModuleFormerForCausalLM\"\n  ],\n  \"att_func\": \"stickbreaking\",\n  \"att_hidden\": 256,\n  \"attn_pdrop\": 0.1,\n  \"aux_loss_type\": \"mi\",\n  \"aux_loss_weight\": 0.1,\n  \"block_size\": 128,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"ffd_hidden\": 512,\n  \"gate_type\": \"gmm\",\n  \"gating_size\": 64,\n  \"history_length\": 128,\n  \"initializer_range\": 0.02,\n  \"k_att\": 3,\n  \"k_mlp\": 3,\n  \"layer_norm_epsilon\": 1e-05,\n  \"local_size\": 1,\n  \"model_type\": \"moduleformer\",\n  \"moe_pdrop\": 0.1,\n  \"moe_type\": \"moe\",\n  \"n_att_experts\": 8,\n  \"n_ctx\": 2048,\n  \"n_embd\": 768,\n  \"n_head\": 2,\n  \"n_layer\": 16,\n  \"n_mlp_experts\": 16,\n  \"resid_pdrop\": 0.1,\n  \"sample_topk\": 2,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.36.0\",\n  \"universal\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 50295,\n  \"world_size\": 23\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load a pretrained tokenizer\n\nThis isn't actually necessary here, but it can be required in some cases.","metadata":{}},{"cell_type":"code","source":"# Tweak the tokenizer\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    cache_dir=\"/kaggle/working/models\",\n    padding=\"max_length\",\n    padding_side=\"left\",\n    use_fast=True,\n    return_overflowing_tokens=True,\n    truncation=True,\n    trust_remote_code=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:48:38.293913Z","iopub.execute_input":"2023-12-21T21:48:38.294910Z","iopub.status.idle":"2023-12-21T21:48:39.058147Z","shell.execute_reply.started":"2023-12-21T21:48:38.294857Z","shell.execute_reply":"2023-12-21T21:48:39.057274Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Load the model\n\nHere we initialize the model with random weights.","metadata":{}},{"cell_type":"code","source":"# Instantiate your model\nimport os\nfrom aigen import aigen\n\nprototype = aigen(\n    model=base_model,\n    tokenizer=tokenizer,\n    cache_dir=\"/kaggle/working/models\",\n    precision=16,\n    gradient_checkpointing=False,\n    config=pretrain_config\n)\n\nprint(prototype)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-21T21:48:39.059318Z","iopub.execute_input":"2023-12-21T21:48:39.059616Z","iopub.status.idle":"2023-12-21T21:48:45.125254Z","shell.execute_reply.started":"2023-12-21T21:48:39.059591Z","shell.execute_reply":"2023-12-21T21:48:45.124115Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"ModuleFormer loaded with 298M parameters.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Parameter-Efficient Fine-Tuning (PEFT)\nHere is a basic example of Low-Rank Adapter training. We remove this because it's not used in pre-training.","metadata":{}},{"cell_type":"code","source":"# # Prepare model for PEFT training\n\n# from peft import (\n#     LoraConfig,\n#     get_peft_model,\n#     prepare_model_for_kbit_training,\n# )\n\n# peft_config = LoraConfig(\n#     task_type=\"CAUSAL_LM\",\n#     r=4,\n#     lora_alpha=16,\n#     lora_dropout=0.01,\n#     bias=\"all\",\n#     target_modules=[\n#       \"embed_in\",\n#       \"query_key_value\",\n#       \"dense\",\n#       \"dense_h_to_4h\",\n#       \"dense_4h_to_h\",\n#       \"embed_out\"\n#     ]\n# )\n\n# prototype.model = prepare_model_for_kbit_training(\n#     prototype.model, use_gradient_checkpointing=True\n# )\n\n# prototype.model = get_peft_model(prototype.model, peft_config)\n\n# prototype.model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:48:45.126780Z","iopub.execute_input":"2023-12-21T21:48:45.127404Z","iopub.status.idle":"2023-12-21T21:48:45.132828Z","shell.execute_reply.started":"2023-12-21T21:48:45.127374Z","shell.execute_reply":"2023-12-21T21:48:45.131794Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Metrics\n\nWe want to log training metrics, so we install Tensorboard and expose it via ngrok. This requires an authtoken from ngrok.com, saved in Kaggle's \"Add-ons>Secrets\".","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"NGROK_SECRET\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nif secret_value:\n\n    !pip install ngrok tensorboard\n\n    import threading\n    import subprocess\n\n    def start_tensorboard():\n        subprocess.Popen(\n            [\"tensorboard\", \"--logdir\", \"/kaggle/working/logs\", \"--bind_all\", \"--samples_per_plugin\", \"scalars=999999999\"], \n        )\n\n    tensorboard_thread = threading.Thread(target=start_tensorboard)\n    tensorboard_thread.start()\n\n    import ngrok\n\n    listener = ngrok.forward(6006, authtoken=secret_value)\n    \n    import time\n    \n    time.sleep(15)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:48:45.134128Z","iopub.execute_input":"2023-12-21T21:48:45.134426Z","iopub.status.idle":"2023-12-21T21:49:15.800115Z","shell.execute_reply.started":"2023-12-21T21:48:45.134400Z","shell.execute_reply":"2023-12-21T21:49:15.798783Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting ngrok\n  Obtaining dependency information for ngrok from https://files.pythonhosted.org/packages/09/e5/1d908d18ba0c532a2d9bb13bbc0020a48e166135aa1ae8d120cf7bf3d3e8/ngrok-0.12.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading ngrok-0.12.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.13.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.57.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.4)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.24.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (4.21.12)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.1.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.41.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\nDownloading ngrok-0.12.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ngrok\nSuccessfully installed ngrok-0.12.1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https://github.com/tensorflow/tensorboard/issues/4784\n\nTensorBoard 2.13.0 at http://c55b8d1ff4c2:6006/ (Press CTRL+C to quit)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training\n\nFinally, we train the model on a dataset streamed from: https://huggingface.co/datasets","metadata":{}},{"cell_type":"code","source":"# Train the model\n\nimport os\nfrom lightning.pytorch import loggers\n\nos.makedirs(f\"/kaggle/working/logs/{focus}\", exist_ok=True)\nlogger = loggers.TensorBoardLogger(\"/kaggle/working/logs\", name=focus, default_hp_metric=True)\n\nprototype.model.training = True\n\nprototype.train(\n    devices=\"auto\",\n    strategy=\"auto\",\n    streaming_data=[\n        {\"dataset\": \"tiiuae/falcon-refinedweb\", \"content_key\": \"content\"}\n    ],\n    output_dir=\"/kaggle/working/trained\",\n    batch_size=4,\n    gradient_accumulation_steps=64,\n    block_size=512,\n    num_steps=10000,\n    warmup_steps=100,\n    optimizer=\"Lion\",\n    learning_rate=0.000333,\n    weight_decay=0.01,\n    gradient_clip_val=1.0,\n    scheduler=\"cosine\",\n    generate_every=500,\n    save_every=1000,\n    loggers=[logger]\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T21:49:15.802146Z","iopub.execute_input":"2023-12-21T21:49:15.802660Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/9.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bdc0c8b84964cf3ae661939c8f3cd61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/5534 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a6419f4c31467bb1171ffbb6257ec6"}},"metadata":{}},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                    | Params\n--------------------------------------------------\n0 | model | ModuleFormerForCausalLM | 298 M \n--------------------------------------------------\n298 M     Trainable params\n0         Non-trainable params\n298 M     Total params\n1,192.930 Total estimated model params size (MB)\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/640000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb9d232fa5341bcac20c60832a6ef60"}},"metadata":{}},{"name":"stdout","text":"\n==>\n\n==>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing\n\nFor testing, we just run an interactive inference session.","metadata":{}},{"cell_type":"code","source":"# Test inference\n\nwhile True:\n    print(\"PROMPT:\\n\")\n    prompt = input()\n    completion = prototype.generate(\n        prompt=prompt,\n        do_sample=True,\n        min_length=23,\n        max_new_tokens=111,\n        temperature=0.9,\n        eta_cutoff=0.0003,\n        penalty_alpha=0.6,\n        top_k=4,\n        repetition_penalty=1.023,\n        no_repeat_ngram_size=13,\n        renormalize_logits=True,\n        remove_invalid_values=True,\n        max_time=60,\n        use_cache=True,\n    )\n    print(\"COMPLETION:\\n\")\n    print(completion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}