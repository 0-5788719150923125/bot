{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VTX\n---\n[AIGen](https://github.com/LuciferianInk/aigen) is a text generation and training library, originally forked from [AITextGen](https://aitextgen.minimaxir.com/) (which is now defunct).\n\nAIGen is also the foundation of [VTX](https://github.com/0-5788719150923125/vtx).\n\nTo use this notebook with Kaggle, one must first enable the \"Internet\". To do so:\n\n1. Find \"Notebook options\" in the sidebar on the right-hand side of this page.\n2. If required, verify your phone number.\n3. Choose \"Internet on\"\n\nAs well, do not forget to connect to an accelerator. The P100's are better for training.","metadata":{}},{"cell_type":"code","source":"# Kaggle uses an old version of CUDA, so we need to install a version of Pytorch that was built for that version.\n!pip install torch>=2.1.0 --no-build-isolation --index-url https://download.pytorch.org/whl/cu110\n\n# We don't even use this, but have to install it because of Kaggle bugs\n!pip install torchaudio\n\n# Now we install AIGen\n!pip install 'git+https://github.com/LuciferianInk/aigen.git'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration\n\nWe could set a bunch of variables here, but we don't. For now, we just hardcode things in the steps below for clarity.","metadata":{}},{"cell_type":"code","source":"# Set some variables\nfocus = 'frame'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretraining\n\nWe want to train a ModuleFormer from scratch, so we import example code from IBM's MoLM and configure the settings.","metadata":{}},{"cell_type":"code","source":"# Install our fork of ModuleFormer\n!pip install 'git+https://github.com/LuciferianInk/ModuleFormer.git@enable-gradient-checkpointing'\n\n# Pretrain configs for ModuleFormer\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n)\n\nfrom moduleformer import (\n    ModuleFormerConfig,\n    ModuleFormerForCausalLM,\n    ModuleFormerForSequenceClassification,\n)\n\nAutoConfig.register(\"moduleformer\", ModuleFormerConfig)\nAutoModelForCausalLM.register(ModuleFormerConfig, ModuleFormerForCausalLM)\nAutoModelForSequenceClassification.register(\n    ModuleFormerConfig, ModuleFormerForSequenceClassification\n)\n\nbase_model = \"ibm/MoLM-350M-4B\"\n\npretrain_config = AutoConfig.from_pretrained(base_model)\noverrides = {\n    \"universal\": True,\n    \"world_size\": 23,\n    \"activation_function\": 'gelu',\n    \"n_layer\": 16,\n    \"n_head\": 2,\n    \"k_att\": 3,\n    \"k_mlp\": 3,\n    \"n_att_experts\": 8,\n    \"n_mlp_experts\": 16,\n    \"n_ctx\": 2048, # history_length * n_layer\n    \"n_embd\": 768,\n    \"att_hidden\": 256,\n    \"ffd_hidden\": 512,\n    \"block_size\": 128,\n    \"gate_type\": 'gmm',\n    \"gating_size\": 64,\n    \"aux_loss_type\": 'mi',\n    \"aux_loss_weight\": 0.1,\n    \"history_length\": 128,\n    \"resid_pdrop\": 0.1,\n    \"embd_pdrop\": 0.1,\n    \"attn_pdrop\": 0.1,\n    \"moe_pdrop\": 0.1,\n    \"sample_topk\": 2,\n    \"tie_word_embeddings\": True,\n}\nsetattr(pretrain_config, \"_name_or_path\", focus)\nfor k, v in overrides.items():\n    setattr(pretrain_config, k, v)\nprint(f\"modified pretrain config:\")\nprint(pretrain_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load a pretrained tokenizer\n\nThis isn't actually necessary here, but it can be required in some cases.","metadata":{}},{"cell_type":"code","source":"# Tweak the tokenizer\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    cache_dir=\"/kaggle/working/models\",\n    padding=\"max_length\",\n    padding_side=\"left\",\n    use_fast=True,\n    return_overflowing_tokens=True,\n    truncation=True,\n    trust_remote_code=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the model\n\nHere we initialize the model with random weights.","metadata":{}},{"cell_type":"code","source":"# Instantiate your model\nimport os\nfrom aigen import aigen\n\n# Use this to continue training.\nresume_training = False\n\nif resume_training:\n    model = None\n    model_folder = \"/kaggle/working/trained\"\n    pretrain_config = None\nelse:\n    model = launch_model\n    model_folder = None\n\nprototype = aigen(\n    model=model,\n    model_folder=model_folder,\n    tokenizer=tokenizer,\n    cache_dir=\"/kaggle/working/models\",\n    precision=16,\n    gradient_checkpointing=False,\n    config=pretrain_config\n)\n\nprint(prototype)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameter-Efficient Fine-Tuning (PEFT)\nHere is a basic example of Low-Rank Adapter training. Currently, we've commented-out this code, because it's not used in pre-training.","metadata":{}},{"cell_type":"code","source":"# # Prepare model for PEFT training\n\n# opts = {\n#     \"r\": 4,\n#     \"alpha\": 16,\n#     \"dropout\": 0.01,\n#     \"bias\": \"all\",\n#     \"target_modules\": [\n#       \"embed_in\",\n#       \"query_key_value\",\n#       \"dense\",\n#       \"dense_h_to_4h\",\n#       \"dense_4h_to_h\",\n#       \"embed_out\"\n#     ]\n# }\n\n# prototype.create_adapter(\"/kaggle/working/trained\", opts)\n\n# prototype.model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics\n\nWe want to log training metrics, so we install Tensorboard and expose it via ngrok. This requires an authtoken from ngrok.com, saved in Kaggle's \"Add-ons>Secrets\".","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"NGROK_SECRET\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nimport os\n\nclean_logs = True\n\nif clean_logs:\n    directory = \"/kaggle/working/logs\"\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        os.remove(file_path)\n\nif secret_value:\n\n    !pip install ngrok tensorboard\n\n    import threading\n    import subprocess\n\n    def start_tensorboard():\n        subprocess.Popen(\n            [\"tensorboard\", \"--logdir\", \"/kaggle/working/logs\", \"--bind_all\", \"--samples_per_plugin\", \"scalars=999999999\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT\n        )\n\n    tensorboard_thread = threading.Thread(target=start_tensorboard)\n    tensorboard_thread.start()\n\n    import ngrok\n\n    listener = await ngrok.forward(6006, authtoken=secret_value)\n    \n    import time\n\n    time.sleep(1)\n    print(listener.url())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nFinally, we train the model on a dataset streamed from: https://huggingface.co/datasets","metadata":{}},{"cell_type":"code","source":"# Train the model\n\nimport os\nfrom lightning.pytorch import loggers\n\nos.makedirs(f\"/kaggle/working/logs/{focus}\", exist_ok=True)\nlogger = loggers.TensorBoardLogger(\"/kaggle/working/logs\", name=focus, default_hp_metric=True)\n\nprototype.model.training = True\n\nprototype.train(\n    devices=\"auto\",\n    strategy=\"auto\",\n    streaming_data=[\n        {\n            \"repo\": \"togethercomputer/RedPajama-Data-V2\", \n            \"content_key\": \"raw_content\", \n            \"sample_size\": 1000,\n            \"snapshots\": [\n                \"2023-14\"\n            ],\n            \"name\": \"default\",\n            \"languages\": [\n                \"en\"\n            ],\n            \"sequential\": True\n        }\n    ],\n    output_dir=\"/kaggle/working/trained\",\n    batch_size=8,\n    gradient_accumulation_steps=128,\n    block_size=512,\n    num_steps=1000,\n    warmup_steps=10,\n    optimizer=\"Lion\",\n    learning_rate=0.000333,\n    weight_decay=0.01,\n    gradient_clip_val=1.0,\n    scheduler=\"cosine\",\n    lookahead=5,\n    generate_every=500,\n    save_every=1000,\n    loggers=[logger],\n    checkpoint=1,\n    resume=resume_training,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing\n\nFor testing, we just run an interactive inference session.","metadata":{}},{"cell_type":"code","source":"# Test inference\n\nwhile True:\n    print(\"PROMPT:\\n\")\n    prompt = input()\n    completion = prototype.generate(\n        prompt=prompt,\n        do_sample=True,\n        min_length=23,\n        max_new_tokens=111,\n        temperature=0.9,\n        eta_cutoff=0.0003,\n        penalty_alpha=0.6,\n        top_k=4,\n        repetition_penalty=1.023,\n        no_repeat_ngram_size=13,\n        renormalize_logits=True,\n        remove_invalid_values=True,\n        max_time=60,\n        use_cache=True,\n    )\n    print(\"COMPLETION:\\n\")\n    print(completion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}