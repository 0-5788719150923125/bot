{"cells":[{"cell_type":"markdown","metadata":{},"source":["# VTX\n","---\n","[AIGen](https://github.com/LuciferianInk/aigen) is a text generation and training library, originally forked from [AITextGen](https://aitextgen.minimaxir.com/) (which is now defunct).\n","\n","AIGen is also the foundation of [VTX](https://github.com/0-5788719150923125/vtx).\n","\n","To use this notebook in Kaggle, one must first enable the \"Internet\". To do so:\n","\n","1. Find \"Notebook options\" in the sidebar on the right-hand side of this page.\n","2. If required, verify your phone number.\n","3. Choose \"Internet on\""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-12-21T14:58:34.423840Z","iopub.status.busy":"2023-12-21T14:58:34.423023Z","iopub.status.idle":"2023-12-21T15:00:57.488917Z","shell.execute_reply":"2023-12-21T15:00:57.487862Z","shell.execute_reply.started":"2023-12-21T14:58:34.423808Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Kaggle uses an old version of CUDA, so we need to install a version of Pytorch that was built for that version.\n","!pip install torch>=2.1.0 --no-build-isolation --index-url https://download.pytorch.org/whl/cu110\n","\n","# We don't even use this, but have to install it because of Kaggle bugs\n","!pip install torchaudio\n","\n","# Now we install AIGen\n","!pip install 'git+https://github.com/LuciferianInk/aigen.git'\n","\n","# Install our fork of ModuleFormer\n","!pip install 'git+https://github.com/LuciferianInk/ModuleFormer.git@enable-gradient-checkpointing'"]},{"cell_type":"markdown","metadata":{},"source":["## Configuration\n","\n","We could set a bunch of variables here, but we don't. For now, we just hardcode things for clarity."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-21T15:01:55.492880Z","iopub.status.busy":"2023-12-21T15:01:55.492485Z","iopub.status.idle":"2023-12-21T15:01:55.498248Z","shell.execute_reply":"2023-12-21T15:01:55.497279Z","shell.execute_reply.started":"2023-12-21T15:01:55.492848Z"},"trusted":true},"outputs":[],"source":["# Set some variables\n","import os\n","\n","focus = 'frame'\n","os.environ[\"FOCUS\"] = focus\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"]},{"cell_type":"markdown","metadata":{},"source":["## ModuleFormer\n","\n","We want to pretrain a ModuleFormer, so we import its code and configure the settings (based upon the defaults from MoLM)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-21T15:01:58.190860Z","iopub.status.busy":"2023-12-21T15:01:58.190477Z","iopub.status.idle":"2023-12-21T15:02:03.774507Z","shell.execute_reply":"2023-12-21T15:02:03.773610Z","shell.execute_reply.started":"2023-12-21T15:01:58.190829Z"},"trusted":true},"outputs":[],"source":["# Pretrain configs for ModuleFormer\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n",")\n","\n","from moduleformer import (\n","    ModuleFormerConfig,\n","    ModuleFormerForCausalLM,\n","    ModuleFormerForSequenceClassification,\n",")\n","\n","AutoConfig.register(\"moduleformer\", ModuleFormerConfig)\n","AutoModelForCausalLM.register(ModuleFormerConfig, ModuleFormerForCausalLM)\n","AutoModelForSequenceClassification.register(\n","    ModuleFormerConfig, ModuleFormerForSequenceClassification\n",")\n","\n","base_model = \"ibm/MoLM-350M-4B\"\n","\n","pretrain_config = AutoConfig.from_pretrained(base_model)\n","overrides = {\n","    \"universal\": True,\n","    \"world_size\": 23,\n","    \"activation_function\": 'gelu',\n","    \"n_layer\": 16,\n","    \"n_head\": 2,\n","    \"k_att\": 3,\n","    \"k_mlp\": 3,\n","    \"n_att_experts\": 8,\n","    \"n_mlp_experts\": 16,\n","    \"n_ctx\": 2048, # history_length * n_layer\n","    \"n_embd\": 768,\n","    \"att_hidden\": 256,\n","    \"ffd_hidden\": 512,\n","    \"block_size\": 128,\n","    \"gate_type\": 'gmm',\n","    \"gating_size\": 64,\n","    \"aux_loss_type\": 'mi',\n","    \"aux_loss_weight\": 0.1,\n","    \"history_length\": 128,\n","    \"resid_pdrop\": 0.1,\n","    \"embd_pdrop\": 0.1,\n","    \"attn_pdrop\": 0.1,\n","    \"moe_pdrop\": 0.1,\n","    \"sample_topk\": 2,\n","    \"tie_word_embeddings\": True,\n","}\n","setattr(pretrain_config, \"_name_or_path\", focus)\n","for k, v in overrides.items():\n","    setattr(pretrain_config, k, v)\n","print(f\"modified pretrain config:\")\n","print(pretrain_config)"]},{"cell_type":"markdown","metadata":{},"source":["## Load the model\n","\n","For training, we tweak the tokenizer a bit, before loading the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-21T15:02:24.540052Z","iopub.status.busy":"2023-12-21T15:02:24.539093Z","iopub.status.idle":"2023-12-21T15:02:25.600992Z","shell.execute_reply":"2023-12-21T15:02:25.600054Z","shell.execute_reply.started":"2023-12-21T15:02:24.540016Z"},"trusted":true},"outputs":[],"source":["# Tweak the tokenizer\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    base_model,\n","    cache_dir=\"/kaggle/working/models\",\n","    padding=\"max_length\",\n","    padding_side=\"left\",\n","    use_fast=True,\n","    return_overflowing_tokens=True,\n","    truncation=True,\n","    trust_remote_code=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-21T15:02:31.373852Z","iopub.status.busy":"2023-12-21T15:02:31.372974Z","iopub.status.idle":"2023-12-21T15:02:37.442030Z","shell.execute_reply":"2023-12-21T15:02:37.441152Z","shell.execute_reply.started":"2023-12-21T15:02:31.373817Z"},"trusted":true},"outputs":[],"source":["# Instantiate your model\n","import os\n","from aigen import aigen\n","\n","prototype = aigen(\n","    model=base_model,\n","    tokenizer=tokenizer,\n","    cache_dir=\"/kaggle/working/models\",\n","    precision=16,\n","    gradient_checkpointing=False,\n","    config=pretrain_config\n",")\n","\n","print(prototype)"]},{"cell_type":"markdown","metadata":{},"source":["## Parameter-Efficient Fine-Tuning (PEFT)\n","Here is a basic example of Low-Rank Adapter training. We remove this because it's not used in pre-training."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Prepare model for PEFT training\n","\n","# from peft import (\n","#     LoraConfig,\n","#     get_peft_model,\n","#     prepare_model_for_kbit_training,\n","# )\n","\n","# peft_config = LoraConfig(\n","#     task_type=\"CAUSAL_LM\",\n","#     r=4,\n","#     lora_alpha=16,\n","#     lora_dropout=0.01,\n","#     bias=\"all\",\n","#     target_modules=[\n","#       \"embed_in\",\n","#       \"query_key_value\",\n","#       \"dense\",\n","#       \"dense_h_to_4h\",\n","#       \"dense_4h_to_h\",\n","#       \"embed_out\"\n","#     ]\n","# )\n","\n","# prototype.model = prepare_model_for_kbit_training(\n","#     prototype.model, use_gradient_checkpointing=True\n","# )\n","\n","# prototype.model = get_peft_model(prototype.model, peft_config)\n","\n","# prototype.model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["## Training\n","\n","Finally, we train the model with settings from above."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-21T15:05:57.514901Z","iopub.status.busy":"2023-12-21T15:05:57.514504Z"},"trusted":true},"outputs":[],"source":["# Train the model\n","\n","prototype.model.training = True\n","\n","prototype.train(\n","    devices=\"auto\",\n","    strategy=\"auto\",\n","    streaming_data=[\n","        {\"dataset\": \"tiiuae/falcon-refinedweb\", \"content_key\": \"content\", \"padding\": \"max_length\"}\n","    ],\n","    output_dir=\"/kaggle/working/trained\",\n","    batch_size=4,\n","    gradient_accumulation_steps=64,\n","    block_size=512,\n","    num_steps=10000,\n","    warmup_steps=100,\n","    optimizer=\"Lion\",\n","    learning_rate=0.000333,\n","    weight_decay=0.01,\n","    gradient_clip_val=1.0,\n","    scheduler=\"cosine\",\n","    generate_every=500,\n","    save_every=1000,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Testing\n","\n","For testing via this notebook, we just run an interactive inference session."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test inference\n","\n","while True:\n","    print(\"PROMPT:\\n\")\n","    prompt = input()\n","    completion = prototype.generate(\n","        prompt=prompt,\n","        do_sample=True,\n","        min_length=23,\n","        max_new_tokens=111,\n","        temperature=0.9,\n","        eta_cutoff=0.0003,\n","        penalty_alpha=0.6,\n","        top_k=4,\n","        repetition_penalty=1.023,\n","        no_repeat_ngram_size=13,\n","        renormalize_logits=True,\n","        remove_invalid_values=True,\n","        max_time=60,\n","        use_cache=True,\n","    )\n","    print(\"COMPLETION:\\n\")\n","    print(completion)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
