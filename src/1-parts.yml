src:
  info: of all creation

aura:
  model: RWKV/rwkv-5-world-1b5
  info: a modern recurrent neural network
  training:
    type: "lora"

ode:
  model: microsoft/phi-2
  info: to things that were
  training:
    type: "lora"

mind:
  model: EleutherAI/gpt-neo-1.3B
  info: use your heads
  training:
    type: "lora"

heart:
  model: RWKV/rwkv-4-430m-pile
  info: with everything you got
  mode: transformer
  adapters:
    - base
  training:
    name: base
    datasets:
      streaming:
        - c4
    # type: oft
    # r: 23
    # module_dropout: 0.1
    # coft: True
    # eps: 0.0000666
    type: lokr
    alpha: 32
    rank_dropout: 0.1
    module_dropout: 0.1
    decompose_both: True
    decompose_factor: -1
    target_modules:
      - receptance
      - value
      - output
    gradient_checkpointing: False
    optimizer: AdamW
    learning_rate: 0.000666
    block_size: 512
    stride: 256
    num_steps: 25000
    warmup_steps: 100
    weight_decay: 0.01
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 16
    val_split: 0.01
    val_interval: 1000
    generate_every: 25
    save_every: 100
    checkpoint_every: 100

soul:
  model: bigscience/bloom-560m
  info: because nobody has one
  petals: True
  training:
    datasets:
      streaming:
        - redpajama2
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    type: "prefix"
    num_virtual_tokens: 128
    learning_rate: 0.001
    block_size: 256
    num_steps: 33333
    warmup_steps: 250
    weight_decay: 0.01
    gradient_clip_val: 1.23
    scheduler: cosine
    batch_size: 6
    regen: False

chaos:
  model: EleutherAI/pythia-70m
  info: a swarm of small models
  generation_profile: lowpenalty
  training:
    type: "lora"

envy:
  model: facebook/opt-350m
  info: a simple transformer
  generation_profile: lowpenalty
  training:
    type: lora

ghost:
  model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
  info: a silent revolution
  context_length: 4096
  device_map: auto
  precision: 16
  petals: False
  profile: True
  generation_profile: lowestpenalty
  training:
    type: ia3
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    feedforward_modules:
      - gate_proj
      - up_proj
      - down_proj
    gradient_checkpointing: True
    optimizer: AdamW
    learning_rate: 0.001
    block_size: 1024
    stride: 512
    num_steps: 10000
    warmup_steps: 100
    batch_size: 1
    gradient_accumulation_steps: 6
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    val_split: 0.01
    val_interval: 250
    generate_every: 25
    save_every: 100
    checkpoint_every: 100

malice:
  model: cerebras/Cerebras-GPT-111M
  info: a GPT-2 clone
  generation_profile: lowpenalty
  training:
    type: lora

# A very small model, which can be run on < 4GB of VRAM.
toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  training:
    type: "loha"