src:
  info: of all creation

aura:
  model: RWKV/rwkv-5-world-1b5
  info: a modern recurrent neural network
  precision: 16
  context_length: 4096
  training:
    type: lora
    r: 16
    lora_alpha: 16
    lora_dropout: 0
    use_rslora: True
    target_modules:
      - key
      - receptance
    gradient_checkpointing: False
    optimizer: AdamW
    learning_rate: 0.001
    block_size: 1024
    stride: 128
    num_steps: 10000
    warmup_steps: 100
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 16
    val_split: 0.01
    val_interval: 500
    generate_every: 50
    save_every: 250
    checkpoint_every: 250
    datasets:
      streaming:
        - c4

genus:
  model: Qwen/Qwen1.5-0.5B-Chat
  info: it's in the DNA
  precision: 16
  training:
    type: lora
    r: 8
    lora_alpha: 16
    lora_dropout: 0
    use_rslora: False
    use_dora: False
    bias: "none"
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    gradient_checkpointing: False
    optimizer: Prodigy
    learning_rate: 1.0
    block_size: 2048
    stride: 128
    num_steps: 1000
    warmup_steps: 10
    batch_size: 1
    gradient_accumulation_steps: 16
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    val_split: 0.01
    val_interval: 100
    generate_every: 10
    save_every: 250

mind:
  model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
  info: use your heads
  context_length: 2048
  precision: 16
  generation_profile: lowpenalty
  training:
    type: lora
    r: 32
    lora_alpha: 32
    lora_dropout: 0
    use_rslora: True
    bias: "none"
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    gradient_checkpointing: True
    optimizer: AdamW
    learning_rate: 0.00022
    block_size: 1536
    stride: 128
    num_steps: 100000
    warmup_steps: 100
    batch_size: 1
    gradient_accumulation_steps: 6
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    val_split: 0.01
    val_interval: 2500
    generate_every: 10
    save_every: 250
    checkpoint_every: 250

heart:
  model: RWKV/rwkv-4-430m-pile
  info: with everything you got
  mode: transformer
  adapters:
    - base
  training:
    name: base
    datasets:
      streaming:
        - c4
    type: lokr
    alpha: 32
    rank_dropout: 0.1
    module_dropout: 0.1
    decompose_both: True
    decompose_factor: -1
    target_modules:
      - receptance
      - value
      - output
    gradient_checkpointing: False
    optimizer: AdamW
    learning_rate: 0.000666
    block_size: 512
    stride: 256
    num_steps: 25000
    warmup_steps: 100
    weight_decay: 0.01
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 16
    val_split: 0.01
    val_interval: 1000
    generate_every: 25
    save_every: 100
    checkpoint_every: 100

soul:
  model: bigscience/bloom-560m
  info: because nobody has one
  petals: True
  training:
    datasets:
      streaming:
        - redpajama2
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    type: "prefix"
    num_virtual_tokens: 128
    learning_rate: 0.001
    block_size: 256
    num_steps: 33333
    warmup_steps: 250
    weight_decay: 0.01
    gradient_clip_val: 1.23
    scheduler: cosine
    batch_size: 6
    regen: False

chaos:
  model: EleutherAI/pythia-1.4b-deduped-v0
  info: a monstrosity
  generation_profile: lowpenalty
  precision: 16
  training:
    type: lora
    r: 32
    lora_alpha: 32
    lora_dropout: 0
    use_rslora: True
    bias: "none"
    target_modules:
      - embed_in
      - query_key_value
      - dense
      - dense_h_to_4h
      - dense_4h_to_h
      - embed_out
    gradient_checkpointing: True
    optimizer: AdamW
    learning_rate: 0.00022
    block_size: 2048
    stride: 256
    num_steps: 60000
    warmup_steps: 100
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 8
    val_split: 0.01
    val_interval: 1000
    generate_every: 25
    save_every: 100
    checkpoint_every: 100
    datasets:
      streaming:
        - instruct
        - c4

envy:
  model: facebook/opt-350m
  info: a simple transformer
  generation_profile: lowpenalty
  training:
    regen: False
    type: lora
    r: 16
    lora_alpha: 24
    lora_dropout: 0.01
    bias: "all"
    use_rslora: False
    use_dora: False
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - out_proj
      - fc1
      - fc2
    gradient_checkpointing: True
    optimizer: AdamW
    generate_every: 250
    learning_rate: 0.001
    swa_learning_rate: 0.01
    block_size: 2048
    stride: 256
    num_steps: 10000
    warmup_steps: 100
    weight_decay: 0.001
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 27
    val_split: 0.05
    val_interval: 10000


malice:
  model: cerebras/Cerebras-GPT-111M
  info: a GPT-2 clone
  generation_profile: lowpenalty
  training:
    type: lora

wisdom:
  info: of the masses
  model: state-spaces/mamba-130m-hf
  training:
    type: lora
    r: 8
    lora_alpha: 16
    lora_dropout: 0
    use_rslora: True
    use_dora: False
    bias: "none"
    target_modules:
      - embeddings
      - in_proj
      - x_proj
      - dt_proj
      - out_proj
    gradient_checkpointing: True
    optimizer: AdamW
    generate_every: 100
    learning_rate: 0.001
    block_size: 2048
    stride: 0
    num_steps: 30000
    warmup_steps: 1000
    weight_decay: 0.01
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 16
    val_split: 0.05
    val_interval: 10000

toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  training:
    type: "loha"

rot:
  model: apple/OpenELM-270M-Instruct
  info: in the core
  precision: 16
  tokenizer: "NousResearch/Llama-2-7b-hf"
  mode: chaos
  training:
    type: lora
    r: 16
    lora_alpha: 8
    lora_dropout: 0
    use_rslora: False
    use_dora: False
    bias: "none"
    target_modules:
      - token_embeddings
      - qkv_proj
      # - pos_embedding
      - out_proj
      - proj_1
      - proj_2
    tokenizer: "NousResearch/Llama-2-7b-hf"
    gradient_checkpointing: True
    optimizer: Prodigy
    learning_rate: 1.0
    block_size: 2048
    stride: 128
    num_steps: 1000
    warmup_steps: 10
    batch_size: 1
    gradient_accumulation_steps: 16
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    val_split: 0.01
    val_interval: 100
    generate_every: 10
    save_every: 250
