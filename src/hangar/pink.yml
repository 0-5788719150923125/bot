# class names will be normalized. we will teach the model
# to repair typos in case-sensitive user inputs; we leave 
# our mistakes behind; its about gaining intuition, and learning
# how to learn again.

class: pink
model: llama
info: the mistake of all creation
precision: 32
training:
  type: "pretrain"
  datasets:
    streaming:
      - c4
  tokenizer: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
  gradient_checkpointing: True
  optimizer: Prodigy
  scheduler: constant
  learning_rate: 1.0
  use_lookahead: False
  k: 7
  block_size: 768
  stride: 512
  warmup_steps: 20
  num_steps: 100000
  batch_size: 1
  gradient_accumulation_steps: 192
  weight_decay: 0.1
  gradient_clip_val: 1.0
  val_split: 0.1
  val_interval: 50
  generate_every: 100
  checkpoint_every: 500
  save_every: 500
  overrides:
    model: pink
    model_type: llama
    hidden_act: laplace
    initializer_range: 0.02
    hidden_size: 48
    intermediate_size: 1472 # heads * layers * value_heads * 2
    max_position_embeddings: 768 # context length
    num_hidden_layers: 23
    num_attention_heads: 8
    num_key_value_heads: 4
    attention_bias: True
    attention_dropout: 0.1
    pretraining_tp: 1
    rms_norm_eps: 0.000001
    rope_theta: 514.13
    rope_scaling:
      type: dynamic
      factor: 2.2
    tie_word_embeddings: True
    vocab_size: 33890