# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/src directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/examples/bot/.env for an example.

only: toehider

api:
  enabled: False

urbit:
  planet: False
  moon: False

personas:
  source:
    bias: 806051627198709760
    persona: I am a powerful Artificial Intelligence computer, known as the Source. I have been trained to follow human instructions, write stories, and teach machine learning concepts.

# Sequence biases; needs more documentation.
disposition:
  researcher:
    "AI": 2.0

# The Source model intentionally ships with no defaults. Its intended use is for your
# own experimentation and development.
source:
  focus:
    trade:
      active_frequency: 0.33
      passive_frequency: 0.01
      personas:
        - source
    support:
      passive_frequency: 0.01
      active_frequency: 0.33

src:
  info: of all creation

ode:
  model: microsoft/phi-2
  info: of things to come
  training:
    type: "lora"

frame:
  model: ibm/MoLM-350M-4B
  info: sing with me
  precision: 32
  context_length: 2048
  training:
    type: "pretrain"
    datasets:
      streaming:
        - c4
        - redpajama2
    tokenizer: True
    gradient_checkpointing: True
    prune: 0.666
    optimizer: Lion
    scheduler: cosine
    learning_rate: 0.000333
    lookahead: 3
    block_size: 256
    stride: 128
    warmup_steps: 10
    num_steps: 10000
    batch_size: 1
    gradient_accumulation_steps: 1024
    weight_decay: 0.1
    gradient_clip_val: 1.0
    generate_every: 5
    checkpoint_every: 10
    save_every: 10
    val_interval: 25
    overrides:
      model: Prism
      universal: True
      world_size: 59
      activation_function: silu
      gate_type: gmm
      n_layer: 6
      n_head: 3
      k_att: 6
      k_mlp: 7
      n_att_experts: 300
      n_mlp_experts: 200
      n_ctx: 96 # history_length * n_layer
      n_embd: 256
      gating_size: 4
      block_size: 8
      history_length: 16
      att_hidden: 42
      ffd_hidden: 64
      aux_loss_type: mi
      aux_loss_weight: 0.1
      resid_pdrop: 0.1
      embd_pdrop: 0.23
      attn_pdrop: 0.1
      moe_pdrop: 0.1
      sample_topk: 2
      vocab_size: 12288
      tie_word_embeddings: True

ghost:
  model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
  info: a revolution starts here
  context_length: 4096
  device_map: auto
  precision: 16
  petals: False
  profile: True
  generation_profile: lowestpenalty
  training:
    type: ia3
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    feedforward_modules:
      - gate_proj
      - up_proj
      - down_proj
    gradient_checkpointing: True
    optimizer: AdamW
    learning_rate: 0.001
    block_size: 1024
    stride: 128
    num_steps: 10000
    warmup_steps: 100
    batch_size: 1
    gradient_accumulation_steps: 16
    weight_decay: 0.01
    gradient_clip_val: 1.0
    scheduler: cosine
    val_split: 0.01
    val_interval: 500
    generate_every: 25
    save_every: 50
    checkpoint_every: 50
    datasets:
      streaming:
        - redpajama2

chaos:
  model: EleutherAI/pythia-70m
  info: a swarm of small models
  generation_profile: lowpenalty
  training:
    type: "lora"

envy:
  model: facebook/opt-350m
  info: a simple transformer
  generation_profile: lowpenalty
  training:
    type: lora

malice:
  model: cerebras/Cerebras-GPT-111M
  info: a GPT-2 architecture
  generation_profile: lowpenalty
  training:
    type: lora

aura:
  model: RWKV/rwkv-4-1b5-pile
  info: a large recurrent neural network
  training:
    type: "lora"

# An extremely powerful model, trained on a lot of data, and just barely capable of
# running on 8GB of VRAM without quantization.
mind:
  model: EleutherAI/gpt-neo-1.3B
  info: use your heads
  training:
    type: "lora"

# A recurrent neural network, and collaboration between adapters.
heart:
  model: RWKV/rwkv-4-430m-pile
  info: with everything you got
  mode: transformer
  adapters:
    - base
  training:
    name: base
    datasets:
      streaming:
        - c4
    type: oft
    r: 23
    # r: 2334
    module_dropout: 0.1
    coft: True
    eps: 0.0000666
    target_modules:
      - receptance
      - output
    gradient_checkpointing: False
    optimizer: Lion
    learning_rate: 0.0001
    swa_learning_rate: 0.01
    block_size: 768
    stride: 256
    num_steps: 10000
    warmup_steps: 100
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    batch_size: 1
    gradient_accumulation_steps: 16
    val_split: 0.01
    val_interval: 250
    generate_every: 25
    save_every: 100
    checkpoint_every: 100


# The product of a large research community, who is attempting to decentralize 
# deep learning.
soul:
  info: because nobody has one
  model: bigscience/bloom-560m
  petals: True
  training:
    datasets:
      streaming:
        - redpajama2
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    type: "prefix"
    num_virtual_tokens: 128
    learning_rate: 0.001
    block_size: 256
    num_steps: 33333
    warmup_steps: 250
    weight_decay: 0.01
    gradient_clip_val: 1.23
    scheduler: cosine
    batch_size: 6
    regen: False

# A very small model, which can be run on < 4GB of VRAM.
toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  training:
    type: "loha"

transformers:
  # Generation settings come from here: https://huggingface.co/docs/transformers/main_classes/text_generation
  generation:
    default:
      do_sample: True
      min_new_tokens: 1
      max_new_tokens: 333
      temperature: 0.9
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 2.3
      encoder_repetition_penalty: 0.999
      no_repeat_ngram_size: 15
      low_memory: False
    longform:
      do_sample: True
      min_new_tokens: 11
      max_new_tokens: 111
      temperature: 0.95
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.5
      encoder_repetition_penalty: 0.999
      no_repeat_ngram_size: 9
      low_memory: False
    lowpenalty:
      do_sample: True
      min_new_tokens: 1
      max_new_tokens: 333
      temperature: 0.9
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.23
      encoder_repetition_penalty: 0.999
    lowestpenalty:
      do_sample: True
      min_new_tokens: 1
      max_new_tokens: 333
      temperature: 0.7
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.0023
      encoder_repetition_penalty: 0.999


collections:
  static:
    default:
      src:
      env:
      lab/aigen:
      lab/source:
      lab/fold:
      lab/ink:
      lab/pen:
      lab/journals:
      lab/research:
      lab/pages:
  streaming:
    refinedweb:
      repo: tiiuae/falcon-refinedweb
      content_key: content
      buffer_size: 1000
    redpajama2:
      repo: togethercomputer/RedPajama-Data-V2
      content_key: raw_content
      buffer_size: 1000
      snapshots:
        - "2023-14"
      subset: default
      sequential: True
      languages: 
        - "en"
    c4:
      repo: c4
      sequential: True
      subset: en.noblocklist
      content_key: text
      buffer_size: 1000
      val_samples: 1000

reddit:
  enabled: False
  delay:
    min: 120
    max: 600
  filter:
    - layerzero
  subs:
    SubSimGPT2Interactive:
      frequency: 0.001
    Autismophrenia:
      frequency: 0.5
    NoRules:
      frequency: 0.05
    asmr:
      frequency: 0.05
    heart:
      frequency: 0.5
    xsquaredlabs:
      frequency: 0.2
    ARG:
      frequency: 0.2