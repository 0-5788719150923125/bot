# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/src directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/examples/bot/.env for an example.

personas:
  source:
    bias: 806051627198709760
    persona: I am a powerful Artificial Intelligence computer, known as the Source. I have been trained to follow human instructions, write stories, and teach machine learning concepts.

source:
  focus:
    trade:
      active_chance: 0.66
      passive_chance: 0.01
      personas:
        - source
    support:
      passive_chance: 0.01
      active_chance: 0.99

soul:
  info: because nobody has one
  model: bigscience/bloom-560m
  to_gpu: False
  to_fp16: False
  max_new_tokens: 333
  truncate_length: 2048
  petals: True
  training:
    resume: False
    regen: False
    model_max_length: 1000000000000000019884624838656
    padding_side: left
    # peft:
    #   type: "prefix"
    #   num_virtual_tokens: 24
    generate_every: 50
    save_every: 200
    stages:
      - learning_rate: 0.000333
        num_steps: 88888
        warmup_steps: 0
        batch_size: 3
        num_layers_freeze: 8
        gradient_accumulation_steps: 1
        weight_decay: 0.01
        max_grad_norm: 0.444
        scheduler: linear
        block_size: 444
        train_transformers_only: False
        equalize_datasets: False
        datasets:
          - default

heart:
  info: with everything you got
  model: RWKV/rwkv-4-430m-pile
  to_gpu: True
  training:
    gradient_checkpointing: False
    peft:
      type: "lora"

mind:
  info: use your heads
  model: xhyi/PT_GPTNEO350_ATG
  to_gpu: False
  training:
    peft:
      type: "lora"

core:
  info: is hotter than hell
  model: cerebras/Cerebras-GPT-590M
  to_gpu: False
  training:
    peft:
      type: "lora"

toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  to_gpu: False
  training:
    peft:
      type: "lora"

collections:
  default:
    src:
      duplicate: 2
      line_by_line: False
    lab/aitextgen:
    lab/source:
    lab/fold:
    lab/ink:
    lab/pen:
    lab/journals:
    lab/research:
    lab/pages:

reddit:
  enabled: False
  subs:
    SubSimGPT2Interactive:
      chance: 0.001
    Kunism:
      chance: 0.5
    NoRules:
      chance: 0.05
    TheInk:
      chance: 0.5
