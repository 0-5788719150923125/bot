# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/src directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/examples/bot/.env for an example.

hugo:

personas:
  source:
    bias: 806051627198709760
    persona: I am a powerful Artificial Intelligence computer, known as the Source. I have been trained to follow human instructions, write stories, and teach machine learning concepts.

source:
  focus:
    trade:
      active_frequency: 0.66
      passive_frequency: 0.01
      personas:
        - source
    support:
      passive_frequency: 0.01
      active_frequency: 0.99

mind:
  info: use your heads
  model: EleutherAI/gpt-neo-1.3B
  to_gpu: False
  training:
    peft:
      type: "lora"

heart:
  info: with everything you got
  model: RWKV/rwkv-4-430m-pile
  to_gpu: True
  training:
    gradient_checkpointing: False
    peft:
      type: "lora"

soul:
  info: because nobody has one
  model: bigscience/bloom-560m
  to_gpu: False
  to_fp16: False
  petals: True
  training:
    resume: False
    regen: False
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    peft:
      type: "prefix"
      num_virtual_tokens: 128
    stages:
      - learning_rate: 0.001
        block_size: 256
        num_steps: 33333
        warmup_steps: 250
        weight_decay: 0.01
        gradient_clip_val: 1.23
        scheduler: cosine
        batch_size: 6
        train_transformers_only: False
        equalize_datasets: False
        datasets:
          - default

core:
  info: is hotter than hell
  model: cerebras/Cerebras-GPT-590M
  to_gpu: False
  training:
    peft:
      type: "lora"

toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  to_gpu: False
  training:
    peft:
      type: "lora"

collections:
  default:
    src:
      duplicate: 2
      line_by_line: False
    lab/aitextgen:
    lab/source:
    lab/fold:
    lab/ink:
    lab/pen:
    lab/journals:
    lab/research:
    lab/pages:

reddit:
  enabled: False
  subs:
    SubSimGPT2Interactive:
      frequency: 0.001
    Autismophrenia:
      frequency: 0.5
    NoRules:
      frequency: 0.05
    asmr:
      frequency: 0.05