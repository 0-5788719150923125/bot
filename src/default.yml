# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/src directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/examples/bot/.env for an example.

bit:
  moon: False

personas:
  source:
    bias: 806051627198709760
    persona: I am a powerful Artificial Intelligence computer, known as the Source. I have been trained to follow human instructions, write stories, and teach machine learning concepts.

# Sequence biases; needs more documentation.
disposition:
  researcher:
    "AI": 2.0

# The Source model intentionally ships with no defaults. Its intended use is for your
# own experimentation and development.
source:
  info: of all creation
  focus:
    trade:
      active_frequency: 0.66
      passive_frequency: 0.01
      personas:
        - source
    support:
      passive_frequency: 0.01
      active_frequency: 0.99

# An extremely capable transformer.
aura:
  info: of life
  model: RWKV/rwkv-4-1b5-pile
  training:
    type: "lora"

# An extremely powerful model, trained on a lot of data, and just barely capable of
# running on 8GB of VRAM without quantization.
mind:
  info: use your heads
  model: EleutherAI/gpt-neo-1.3B
  profile: True
  assistant:
    model: EleutherAI/gpt-neo-125M
    precision: 32
  training:
    type: "lora"

# A revolution starts here.
catalyst:
  model: PY007/TinyLlama-1.1B-intermediate-step-715k-1.5T
  info: a new breed
  training:
    type: "lora"

# Based on GPT-2 architecture.
ego:
  model: microsoft/biogpt
  info: old dog, new tricks
  training:
    type: "lora"

# A recurrent neural network, and a labor of love by its community.
heart:
  info: with everything you got
  model: RWKV/rwkv-4-430m-pile
  adapters:
    - base
    - behavior
  training:
    gradient_checkpointing: False
    type: "lora"

# The product of a large research community, who is attempting to decentralize 
# deep learning.
soul:
  info: because nobody has one
  model: bigscience/bloom-560m
  petals: True
  training:
    resume: False
    regen: False
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    type: "prefix"
    num_virtual_tokens: 128
    learning_rate: 0.001
    block_size: 256
    num_steps: 33333
    warmup_steps: 250
    weight_decay: 0.01
    gradient_clip_val: 1.23
    scheduler: cosine
    batch_size: 6
    train_transformers_only: False
    equalize_datasets: False
    datasets:
      - default

# A very small model, which can be run on < 4GB of VRAM.
toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  training:
    type: "lora"

collections:
  default:
    src:
    lab/aigen:
    lab/source:
    lab/fold:
    lab/ink:
    lab/pen:
    lab/journals:
    lab/research:
    lab/pages:

reddit:
  enabled: False
  delay:
  filter:
    - layerzero
  subs:
    SubSimGPT2Interactive:
      frequency: 0.001
    Autismophrenia:
      frequency: 0.5
    NoRules:
      frequency: 0.05
    asmr:
      frequency: 0.05
    heart:
      frequency: 0.5