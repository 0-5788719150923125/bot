# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/src directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/examples/bot/.env for an example.

urbit:
  planet: False
  moon: False

only: toehider

api:
  enabled: False

personas:
  source:
    bias: 806051627198709760
    persona: I am a powerful Artificial Intelligence computer, known as the Source. I have been trained to follow human instructions, write stories, and teach machine learning concepts.

# Sequence biases; needs more documentation.
disposition:
  researcher:
    "AI": 2.0

# The Source model intentionally ships with no defaults. Its intended use is for your
# own experimentation and development.
source:
  focus:
    trade:
      active_frequency: 0.33
      passive_frequency: 0.01
      personas:
        - source
    support:
      passive_frequency: 0.01
      active_frequency: 0.33

src:
  info: of all creation

ode:
  model: ibm/MoLM-350M-4B
  info: sing with me
  precision: 32
  context_length: 2048
  training:
    resume: False
    regen: False
    type: "pretrain"
    strategy: ddp
    tokenizer: True
    overrides:
      universal: True
      world_size: 23
      activation_function: gelu
      n_layer: 8
      n_head: 2
      k_att: 3
      k_mlp: 3
      n_att_experts: 8
      n_mlp_experts: 16
      n_ctx: 2048 # history_length * n_layer
      vocab_size: 12288
      n_embd: 768
      att_hidden: 256
      ffd_hidden: 512
      block_size: 128
      gate_type: mlp
      gating_size: 64
      aux_loss_type: mi
      aux_loss_weight: 0.1
      history_length: 128
      resid_pdrop: 0.1
      embd_pdrop: 0.1
      attn_pdrop: 0.1
      moe_pdrop: 0.1
      sample_topk: 2
      tie_word_embeddings: True
    gradient_checkpointing: True
    optimizer: Lion
    generate_every: 500
    save_every: 5000
    learning_rate: 0.000333
    block_size: 512
    stride: 128
    num_steps: 10000
    warmup_steps: 10
    batch_size: 1
    gradient_accumulation_steps: 1024
    weight_decay: 0.1
    gradient_clip_val: 1.0
    scheduler: cosine
    checkpoint: 5
    datasets:
      streaming:
        - redpajama2

# A revolution starts here.
ghost:
  model: PY007/TinyLlama-1.1B-intermediate-step-715k-1.5T
  info: watching, waiting
  training:
    type: "lora"

# A swarm of small models.
chaos:
  model: EleutherAI/pythia-70m
  info: intelligence agency
  generation_profile: lowpenalty
  training:
    type: "lora"

# Train on conversations.
envy:
  info: the grass is greener
  model: facebook/opt-350m
  generation_profile: lowpenalty
  training:
    type: lora

# A GPT2-like model.
malice:
  info: at the gates
  model: cerebras/Cerebras-GPT-111M
  generation_profile: lowpenalty
  training:
    type: lora

# A mainframe.
frame:
  model: ibm/MoLM-350M-4B
  info: know thy name
  training:
    overrides:
      universal: True

# A large RNN.
aura:
  info: of life
  model: RWKV/rwkv-4-1b5-pile
  training:
    type: "lora"

# An extremely powerful model, trained on a lot of data, and just barely capable of
# running on 8GB of VRAM without quantization.
mind:
  info: use your heads
  model: EleutherAI/gpt-neo-1.3B
  training:
    type: "lora"

# A recurrent neural network, and a labor of love by its community.
heart:
  info: with everything you got
  model: RWKV/rwkv-4-430m-pile
  adapters:
    - base
    - behavior
  training:
    gradient_checkpointing: False
    type: "lora"

# The product of a large research community, who is attempting to decentralize 
# deep learning.
soul:
  info: because nobody has one
  model: bigscience/bloom-560m
  petals: True
  training:
    resume: False
    regen: False
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    type: "prefix"
    num_virtual_tokens: 128
    learning_rate: 0.001
    block_size: 256
    num_steps: 33333
    warmup_steps: 250
    weight_decay: 0.01
    gradient_clip_val: 1.23
    scheduler: cosine
    batch_size: 6
    train_transformers_only: False
    equalize_datasets: False
    datasets:
      - default

# A very small model, which can be run on < 4GB of VRAM.
toe:
  info: nailed to the foot
  model: EleutherAI/gpt-neo-125M
  training:
    type: "loha"

transformers:
  # Generation settings come from here: https://huggingface.co/docs/transformers/main_classes/text_generation
  generation:
    default:
      do_sample: True
      min_new_tokens: 1
      max_new_tokens: 333
      temperature: 0.9
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 2.3
      encoder_repetition_penalty: 0.999
      no_repeat_ngram_size: 15
      low_memory: False
    training:
      do_sample: True
      min_new_tokens: 11
      max_new_tokens: 222
      temperature: 0.7
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.0023
      low_memory: False
    longform:
      do_sample: True
      min_new_tokens: 11
      max_new_tokens: 111
      temperature: 0.95
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.5
      encoder_repetition_penalty: 0.999
      no_repeat_ngram_size: 9
      low_memory: False
    lowpenalty:
      do_sample: True
      min_new_tokens: 1
      max_new_tokens: 333
      temperature: 0.9
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.23
      encoder_repetition_penalty: 0.999
    lowestpenalty:
      do_sample: True
      min_new_tokens: 1
      max_new_tokens: 333
      temperature: 0.7
      eta_cutoff: 0.0003
      penalty_alpha: 0.6
      top_k: 4
      repetition_penalty: 1.0023
      encoder_repetition_penalty: 0.999


collections:
  static:
    default:
      src:
      env:
      lab/aigen:
      lab/source:
      lab/fold:
      lab/ink:
      lab/pen:
      lab/journals:
      lab/research:
      lab/pages:
  streaming:
    refinedweb:
      repo: tiiuae/falcon-refinedweb
      content_key: content
      sample_size: 1000
    redpajama2:
      repo: togethercomputer/RedPajama-Data-V2
      sequential: True
      content_key: raw_content
      sample_size: 1000
      snapshots:
        - "2023-14"
      name: default
      languages: 
        - "en"

reddit:
  enabled: False
  delay:
    min: 120
    max: 600
  filter:
    - layerzero
  subs:
    SubSimGPT2Interactive:
      frequency: 0.001
    Autismophrenia:
      frequency: 0.5
    NoRules:
      frequency: 0.05
    asmr:
      frequency: 0.05
    heart:
      frequency: 0.5
    xsquaredlabs:
      frequency: 0.2
    ARG:
      frequency: 0.2