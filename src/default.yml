# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/src directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/examples/bot/.env for an example.

personas:
  source:
    bias: 806051627198709760
    persona: I am a powerful Artificial Intelligence computer, known as the Source. I have been trained to follow human instructions, write stories, and teach machine learning concepts.

# The Source model intentionally ships with no defaults. Its intended use is for your
# own experimentation and development.
source:
  info: of all creation
  focus:
    trade:
      active_frequency: 0.66
      passive_frequency: 0.01
      personas:
        - source
    support:
      passive_frequency: 0.01
      active_frequency: 0.99

# An extremely powerful model, trained on a lot of data, and just barely capable of
# running on 8GB of VRAM without quantization.
mind:
  info: use your heads
  model: EleutherAI/gpt-neo-1.3B
  training:
    peft:
      type: "lora"

# A recurrent neural network, and a labor of love by its community.
heart:
  info: with everything you got
  model: RWKV/rwkv-4-169m-pile
  training:
    gradient_checkpointing: False
    peft:
      type: "lora"

# The product of a large research community, who is attempting to decentralize 
# deep learning.
soul:
  info: because nobody has one
  model: bigscience/bloom-560m
  petals: True
  training:
    resume: False
    regen: False
    generate_every: 50
    save_every: 100
    padding_side: left
    model_max_length: 256
    peft:
      type: "prefix"
      num_virtual_tokens: 128
    stages:
      - learning_rate: 0.001
        block_size: 256
        num_steps: 33333
        warmup_steps: 250
        weight_decay: 0.01
        gradient_clip_val: 1.23
        scheduler: cosine
        batch_size: 6
        train_transformers_only: False
        equalize_datasets: False
        datasets:
          - default

# Mostly GPT-2 architecture. Terrible performance.
core:
  info: is hotter than hell
  model: cerebras/Cerebras-GPT-590M
  training:
    peft:
      type: "lora"

# A very small model, which can be run on < 4GB of VRAM.
toe:
  info: nailed to the foot
  model: EleutherAI/pythia-70m
  training:
    peft:
      type: "lora"

collections:
  default:
    src:
    lab/aigen:
    lab/source:
    lab/fold:
    lab/ink:
    lab/pen:
    lab/journals:
    lab/research:
    lab/pages:

reddit:
  enabled: False
  filter:
    - layerzero
  subs:
    SubSimGPT2Interactive:
      frequency: 0.001
    Autismophrenia:
      frequency: 0.5
    NoRules:
      frequency: 0.05
    asmr:
      frequency: 0.05
    heart:
      frequency: 0.5