\section{Background and Related Work}

\subsection{GNNs for Brain Network Analysis}
Recently, emerging attention has been devoted to the generalization of GNN-based models to brain network analysis \citep{DBLP:conf/miccai/LiDZZVD19, ahmedt2021graph}. GroupINN \citep{groupinn2019} utilizes a grouping-based layer to provide explainability and reduce the model size. BrainGNN \citep{li2020braingnn} designs the ROI-aware GNNs to leverage the functional information in brain networks and uses a special pooling operator to select these crucial nodes. IBGNN \citep{cui2022interpretable} proposes an interpretable framework to analyze disorder-specific ROIs and prominent connections. 
In addition, FBNetGen \citep{kan2022fbnetgen} considers the learnable generation of brain networks and explores the explainability of the generated brain networks towards downstream tasks. Another benchmark paper \citep{braingb} systematically studies the effectiveness of various GNN designs over brain network data. Different from other work focusing on static brain networks, STAGIN \citep{NEURIPS2021_22785dd2} utilizes GNNs with spatio-temporal attention to model dynamic brain networks extracted from fMRI data. 

\subsection{Graph Transformer}
Graph Transformer raises many researchers' interest currently due to its outstanding performance in graph representation learning. Graph Transformer \citep{graphtransformer_aaai} firstly injects edge information into the attention mechanism and leverages the eigenvectors as positional embeddings. SAN \citep{san} enhances the positional embeddings and improves the attention mechanism by emphasizing neighbor nodes while incorporating the global information. Graphomer \citep{graphormer} designs unique mechanisms for molecule graphs and achieves the SOTA performance. Besides, a fine-grained attention mechanism is developed for node classification \citep{jiananzhao}. Also, the Transformer is extended to larger-scale heterogeneous graphs with a particular sampling algorithm in HGT \citep{hu2020heterogeneous}. EGT \citep{hussain2021edge} further employs edge augmentation to assist global self-attention. In addition, LSPE \citep{dwivedi2022graph} leverages the learnable structural and positional encoding to improve GNNs' representation power, and GRPE \citep{https://doi.org/10.48550/arxiv.2201.12787} enhances the design of encoding node relative position information in Transformer. 