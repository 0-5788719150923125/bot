
\section*{Parameter ablations for pretrained models}

\begin{table}[H] 
\begin{center}
\begin{tabular}{c|cccc}
\toprule
\textbf{Task} & \multicolumn{1}{c}{\bf output only} & \multicolumn{1}{c}{\bf output + input} & \multicolumn{1}{c}{\bf output + positions} & \multicolumn{1}{c}{\bf output + layernorm} \\
\midrule
Bit Memory & 76\% & \textbf{98\%} & 93\% & 94\% \\
Bit XOR & 56\% & 72\% & 84\% & \textbf{98\%} \\
ListOps & 15\% & 17\% & 35\% & \textbf{36\%} \\
MNIST & 23\% & 85\% & 93\% & \textbf{96\%} \\
CIFAR-10 & 25\% & 53\% & 38\% & \textbf{54\%} \\
CIFAR-10 LRA & 17\% & 22\% & 30\% & \textbf{39\%} \\
Homology & 2\% &  8\% &  8\% & \textbf{9\%} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Ablation by only finetuning individual types of parameters for pretrained frozen transformers. We bold the most important parameter (measured by highest test accuracy) for each task.}\label{table:finetuning_indep}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{c|cccc}
\toprule
\textbf{Task} & \multicolumn{1}{c}{\bf output only} & \multicolumn{1}{c}{\bf + layernorm} & \multicolumn{1}{c}{\bf + input} & \multicolumn{1}{c}{\bf + positions} \\
\midrule
Bit Memory & 76\% & 94\% & 100\% & 100\% \\
Bit XOR & 56\% & 98\% & 98\% & 100\% \\
ListOps & 15\% & 36\% & 36\% & 38\% \\
MNIST & 23\% & 96\% & 98\% & 98\% \\
CIFAR-10 & 25\% & 54\% & 60\% & 68\% \\
CIFAR-10 LRA & 17\% & 39\% & 39\% & 39\% \\
Homology & 2\% & 9\% & 10\% & 13\% \\
\bottomrule
\end{tabular}
\end{center}
\caption{Ablation by successively adding certain parameters to the list of finetuned parameters for pretrained frozen transformers.}\label{table:finetuning_add}
\end{table}

\section*{Parameter ablations for random models}

\begin{table}[H] 
\begin{center}
\begin{tabular}{c|cccc}
\toprule
\textbf{Task} & \multicolumn{1}{c}{\bf output only} & \multicolumn{1}{c}{\bf output + input} & \multicolumn{1}{c}{\bf output + positions} & \multicolumn{1}{c}{\bf output + layernorm} \\
\midrule
Bit Memory & 75\% & 75\% & 75\% & 75\% \\
Bit XOR & 50\% & 51\% & 59\% & \textbf{100\%} \\
ListOps & 17\% & 17\% & 18\% & \textbf{35\%} \\
MNIST & 25\% & 28\% & 34\% & \textbf{83\%} \\
CIFAR-10 & 20\% & 24\% & 21\% & \textbf{46\%} \\
CIFAR-10 LRA & 11\% & 16\% & 12\% & \textbf{34\%} \\
Homology & 2\% &  2\% &  6\% & \textbf{9\%} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Finetuning individual types of parameters for random frozen transformers.}\label{table:finetuning_random_indep}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{c|cccc}
\toprule
\textbf{Task} & \multicolumn{1}{c}{\bf output only} & \multicolumn{1}{c}{\bf + layernorm} & \multicolumn{1}{c}{\bf + input} & \multicolumn{1}{c}{\bf + positions} \\
\midrule
Bit Memory & 75\% & 75\% & 75\% & 76\% \\
Bit XOR & 50\% & 100\% & 100\% & 100\% \\
ListOps & 17\% & 35\% & 36\% & 37\% \\
MNIST & 25\% & 83\% & 92\% & 92\% \\
CIFAR-10 & 20\% & 46\% & 56\% & 62\% \\
CIFAR-10 LRA & 11\% & 34\% & 36\% & 36\% \\
Homology & 2\% & 9\%  & 9\% & 9\% \\
\bottomrule
\end{tabular}
\end{center}
\caption{Ablation by successively adding certain parameters to the list of finetuned parameters for random frozen transformers.}\label{table:finetuning_random_add}
\end{table}
