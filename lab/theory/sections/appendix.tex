
\begin{center}{\LARGE{\textbf{Appendix}}}\end{center}
\addcontentsline{toc}{section}{Appendix}

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\tableofcontents

\clearpage

\section{Summary of arXiv Updates}
\label{app:changelog}

We summarize changes made in updated versions:
\begin{enumerate}[label={v\arabic*}.]
    \item (9 Mar 2021) Original release.
    
    \item (30 June 2021) Updated Section \ref{sec:architecture_results} with more analysis of the frozen LSTM architecture and additional experimental details. Added new Section \ref{sec:model_depth} discussing model depth and token mixing, new results in Section \ref{sec:moreparams} discussing how different freezing strategies can improve performance, and attention mask visualization for random frozen transformer to Section \ref{sec:attention_maps}. Included more details about experiments and hyperparameters, and added some new citations (notably \cite{wu2021lime} for related LIME work and \cite{frankle2020batchnorm} for similar frozen analysis for CNNs). Github was also updated to include LSTM architecture, vision pretraining, and remote homology tasks. Minor writing updates.
\end{enumerate}

\section{Background on Transformers}
\label{app:architecture}

In this section, we give a description of the transformer architecture used in our experiments, namely the GPT-2 architecture~\citep{radford2019gpt2}.

\subsection{Self-Attention}

The main subcomponent of the transformer architecture is the self-attention layer, which takes in $l$ input tokens and outputs $l$ output tokens, both of dimension $n_{dim}$.
Each input token $x_i$ is mapped by linear transformations $Q$, $K$, and $V$ -- denoting query, key, and values, respectively -- into $q_i$, $k_i$, and $v_i$.
Both $q_i$ and $k_i$ have dimension $d_k$, and $v_i$ has dimension $d_v$.
To generate the output token $y_i$, dot products are calculated between query $q_i$ and keys $k_j$, and fed into a softmax operation to generate weights $w_i \in [0, 1]$ (in practice, a scaling temperature factor of $\frac{1}{\sqrt{d_k}}$ is used to reduce the sharpness of the softmax).
Then, the weights are used to generate $y_i$ as a weighted sum of all the values, i.e.:
\begin{equation}\label{eq:attention}
    y_i = \sum_{j=1}^l \frac{\text{exp}(q_i^\top k_j)}{\sum_{k=1}^l \text{exp}(q_i^\top k_k)} v_j
\end{equation}

This is extended to \emph{multi-head} attention over $n_{heads}$ heads by doing the above procedure $n_{heads}$ times, and then concatenating.
To recover the original dimension the concatenated vector (of dimension $d_v n_{heads}$) is multiplied by a projection matrix $W_{proj} \in \mathbb{R}^{d_v n_{heads} \times n_{dim}}$.

GPT-2 applies a causal mask to its inputs, i.e. the output token $i$ is only allowed to attend to the input tokens $j \leq i$, which changes the upper bounds of the sums in Equation \ref{eq:attention} to $i$ instead of $l$.
This allows for unsupervised pretraining methods like language modeling (see Appendix \ref{app:objective}).

A residual connection is used to connect the inputs with the outputs of the attention layer.
Then, in the rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension upwards to $4 \cdot n_{dim}$ for the inner dimension and using the GELU activation function~\citep{hendrycks2016gelu}.
Another residual connection is used to connect the outputs of the MLP with the previous outputs of the attention layer.

This forms the basis of the transformer block.
As it preserves the dimension $n_{dim}$, multiple blocks can be learned and stacked on top of each other $n_{layers}$ times, before feeding the final hidden states to the output layer.
In our work, we only use the output of the last hidden state for classification, although in principle other methods are reasonable.

\subsection{Positional Embeddings}

As the self-attention blocks are permutation-invariant, in order to capture positional information about sequences, positional embeddings are learned.
For each position $i \in (1, \dots, \text{max\_len})$, a vector $p_i$ is learned.
At the front of the transformer, before feeding in the inputs $x_i$ into the self-attention blocks, the positional embeddings are added to the input embeddings as $x_i := x_i + p_i$.

\subsection{Layer Norm}

Layer norm \citep{ba2016layernorm} is frequently used in recurrent and transformer architectures as a means of normalizing the activations.
In particular, for the activations of training example $x$ of dimension $n_{dim}$, it normalizes by the mean and variance over the features:
\begin{equation}
    \tilde{y}_i = \frac{x_i - \text{mean}(\{x_j\}_{j=1}^{n_{dim}})}{\text{std}(\{x_j\}_{j=1}^{n_{dim}})}
\end{equation}

Then, affine scale and shift parameters each of dimension $n_{dim}$ -- $\gamma$ and $\beta$, respectively -- are learned to generate the outputs $y$.
\begin{equation}
    y_i = \gamma_i \tilde{y}_i + \beta_i
\end{equation}

Layer norm is applied twice per self-attention block: once before the attention layer and once before the MLP.
As a result, a total of $4 \cdot n_{layers} \cdot n_{dim}$ layer norm parameters are learned.

\subsection{Pretraining Objective}
\label{app:objective}

GPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters which maximize the log-likelihood of the data: $\text{max}_\theta \mathbb{E}[\log p_\theta(x)]$.
GPT-2 models sequences autoregressively, factorizing the probability distribution $p(x) = p(x_1, \dots, x_l)$ via chain rule as:
\begin{equation}
    p(x) = \prod_{i=1}^l p(x_i | x_{i-1}, \dots, x_1)
\end{equation}

For the language domain, this objective can be interpreted as ``given the previous $i-1$ words of a sentence, predict the next word''.

\subsection{Model Sizes}

The model sizes from Section \ref{sec:size} are as follows:

\begin{table}[H] 
\begin{center}
\begin{tabular}{c|ccc|c}
\toprule
\textbf{Model Size} & $n_{layers}$ & $n_{dim}$ & $n_{heads}$ & \# Parameters \\
\midrule
Small (Base) & 12 & 768  & 12 & 117M \\
Medium       & 24 & 1024 & 16 & 345M \\
Large        & 36 & 1280 & 20 & 774M \\
\bottomrule
\end{tabular}
\end{center}
\caption{Hyperparameters for architectures for larger model sizes.}\label{table:model_sizes}
\end{table}

The hyperparameters for the experiments with other architectures (Vision Transformer, BERT, Longformer, T5) are the same as for the base model size shown above.

\section{Experimental Details}
\label{app:experimental_details}

We use implementations of and pretrained models from the Huggingface Transformers library \citep{wolf2020transformers}.
We train all models using the Adam \citep{kingma2014adam} optimizer following Pytorch \citep{paszke2019pytorch} defaults.
For all transformer models, we use a learning rate of $10^{-3}$ without learning rate scheduling.
For the remote homology task only, we use a learning rate of $10^{-4}$ as we found it to give better performance than $10^{-3}$.
We generally use the largest batch size that fits on an RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation.
Note that except for the remote homology task, we did not tune the FPT hyperparameters.
For all LSTMs, we use a lower learning rate of $3 \times 10^{-4}$ and the same batch sizes as transformers of the same size.
Models are trained to convergence and evaluated on a heldout test set.
\vspace{2em}

\section{Details by Table}

For clarity, we explicitly write out finer details for some experiment sections where numbers can represent different model types.

\subsection{Can pretrained language models transfer to different modalities?}

This section refers to Table \ref{table:main_result} in Section \ref{sec:transfer}.

\textbf{Bit Memory}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params).
    \item Full: 12-layer base size GPT-2 model (training all params).
    \item LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).
\end{enumerate}

\textbf{Bit XOR}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params).
    \item Full: 12-layer base size GPT-2 model (training all params).
    \item LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).
\end{enumerate}

\textbf{ListOps}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params).
    \item Full: number reported from \cite{tay2020lra} (3-layer vanilla transformer).
    \item LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).
\end{enumerate}

\textbf{CIFAR-10}
\begin{enumerate}
    \item FPT: 36-layer large size FPT model (finetuning input, output, position, and layernorm params).
    \item Full: 3-layer, 768 hidden dimension GPT-2 model (training all params).
    \item LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).
\end{enumerate}

\textbf{CIFAR-10 LRA}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params).
    \item Full: number reported from \cite{tay2020lra} (3-layer vanilla transformer).
    \item LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).
\end{enumerate}

\textbf{Remote Homology}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params).
    \item Full: number reported from \cite{rap2019tape} (12-layer, 512 hidden dimension vanilla transformer).
    \item LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).
\end{enumerate}
\vspace{2em}

\subsection{What is the importance of the pretraining modality?}
\label{app:details_pretraining}

This section refers to Table \ref{table:random} in Section \ref{sec:pretraining}.

\textbf{All tasks}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params). This differs from Table \ref{table:main_result}, Section \ref{sec:transfer} only in the CIFAR-10 model size.
    \item Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training input, output, position, and layernorm params).
    \item Bit: 12-layer base size GPT-2 model (finetuning input, output, position, and layernorm params), after first being fully finetuned on Bit Memory following default random initialization.
    \item ViT: 12-layer, 768 hidden dimension base size ViT model (finetuning input, output, position, and layernorm params), pretrained on 224 $\times$ 224 ImageNet-21k with a patch size of 16. (\texttt{vit\_base\_patch16\_224} from the \texttt{timm} Pytorch library \citep{wightman2019timm}).
    We reinitialize the input layer from scratch to match each task, and do not use a CLS token or an MLP as the output network -- instead using a linear layer from the last token -- matching the protocol for the other methods.
\end{enumerate}

\subsection{How important is the transformer architecture compared to LSTM architecture?}

The following refer to Section \ref{sec:architecture_results}.
In Table \ref{table:random_architecture}:

\textbf{All tasks}
\begin{enumerate}
    \item Trans: 12-layer randomly initialized (default scheme) base size GPT-2 model (training input, output, and layernorm params). Note: same as ``Random'' in Table \ref{table:random}, Section \ref{sec:pretraining}.
    \item LSTM: 3-layer, 768 hidden dimension ``standard'' LSTM (training input, output, and layernorm params). Does not have residual connections or positional embeddings.
    \item LSTM$^*$: 12-layer, 768 hidden dimension LSTM (training input, output, position, and layernorm params). 
\end{enumerate}

Table \ref{table:lstm_layers}:

\textbf{All tasks}
\begin{enumerate}
    \item 12: 12-layer, 768 hidden dimension ``standard'' LSTM (training input, output, and layernorm params).
    \item 3: 3-layer, 768 hidden dimension ``standard'' LSTM (training input, output, and layernorm params).
\end{enumerate}

Table \ref{table:lstm_layers_residual}:

\textbf{All tasks}
\begin{enumerate}
    \item 12-layer LSTM: 12-layer, 768 hidden dimension ``standard'' LSTM (training input, output, and layernorm params). Note: same as ``12'' in Table \ref{table:lstm_layers}, Section \ref{sec:architecture_results}.
    \item + Residual Connections: 12-layer, 768 hidden dimension LSTM with residual connections (training input, output, and layernorm params).
    \item + Positional Embeddings: 12-layer, 768 hidden dimension LSTM with residual connections and positional embeddings (training input, output, position, and layernorm params). Note: same as ``LSTM$^*$'' in Table \ref{table:random_architecture}, Section \ref{sec:architecture_results}.
\end{enumerate}
\vspace{4em}

\subsection{Does language pretraining improve compute efficiency over random initialization?}

This section refers to Table \ref{table:convergence} in Section \ref{sec:compute_efficiency}.

\textbf{All tasks}
\begin{enumerate}
    \item FPT: 12-layer base size FPT model (finetuning input, output, position, and layernorm params). Note: same models as ``FPT'' in Table \ref{table:random}, Section \ref{sec:pretraining}.
    \item Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training input, output, position, and layernorm params). Note: same models as ``Random'' in Table \ref{table:random}, Section \ref{sec:pretraining}.
\end{enumerate}
