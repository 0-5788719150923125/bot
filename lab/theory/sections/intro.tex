
\section{Introduction}
\label{sec:intro}

The transformer architecture \citep{vaswani2017attention} has shown broad successes in deep learning, serving as the backbone of large models for tasks such as modeling natural language \citep{brown2020gpt3}, images \citep{dosovitskiy2020vit}, proteins \citep{jumper2021alphafold}, behaviors \citep{abramson2020imitating}, and multimodal tasks comprising of both images and text \citep{lu2019vilbert, radford2021clip}.
Inspired by these successes, we seek to explore the generalization capabilities of a transformer in transferring from one modality to another.

Classical approaches to sequence processing used recurrent neural network (RNN) approaches  \citep{rumelhart1985rnn, hochreiter1997lstm}.
In contrast, transformers utilize self-attention layers to extract features across tokens of a sequence, such as words \citep{vaswani2017attention} or image patches \citep{dosovitskiy2020vit}.
Furthermore, it has become common practice to train large models on unsupervised or weakly supervised objectives before finetuning or evaluating zero-shot generalization on a downstream task.
However, the downstream tasks that have been studied are generally restricted to the same modality as the original training set: for example, train GPT \citep{radford2018gpt} on a large language corpus, and finetune on a small task-specific dataset.
Our goal in this work is to investigate finetuning on modalities distinct from the training modality.

We hypothesize that transformers, namely the self-attention layers, can be pretrained on a data-rich modality (i.e. where data is plentiful, such as a natural language corpus) and identify feature representations that are useful for \emph{arbitrary} data sequences, enabling downstream transfer to different modalities.
In particular, we seek to investigate what pretrained language models (LMs) are capable of in terms of generalizing to other modalities with sequential structure.

To investigate this hypothesis, we take a transformer model pretrained on natural language data, GPT-2 \citep{radford2019gpt2}, and finetune only the linear input and output layers, as well as the positional embeddings and layer norm parameters.
We call this model a Frozen Pretrained Transformer (FPT).
On a range of tasks across a variety of modalities -- including numerical computation, image classification, and protein fold prediction -- FPT displays comparable performance to training the entire transformer or LSTM models from scratch, matching reported benchmarks for these tasks (Figure \ref{fig:main_result}).
Additionally, we find FPT models also converge faster during training.
Our results suggest the self-attention layers learned by a language model may have properties amenable to efficient universal computation.
Through a series of experiments, we seek to investigate what contributes to the performance of FPTs by isolating various sub-components of these models.
