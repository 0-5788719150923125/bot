\section{Training Curves of Different Models with or without StratifiedSampling}
\label{app:setting}
In Figure \ref{fig:setting}, we demonstrate the training curves of different models with or without stratified sampling based on site information from ABIDE. The curves of different variants display similar patterns across three model architectures in a single run. We remove Graphormer since its performance is much worse than others. Specifically, it is shown that (a) with stratified sampling, the performance gap between validation and test on ABIDE is much smaller than the one without stratified sampling; (b) stratified sampling can stabilize the training process on ABIDE, especially for VanillaTF and \methodtable.
\begin{figure*}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/settingcompare.png}
	\caption{Training Curves of Different Models with or without StratifiedSampling.}
	\label{fig:setting}
\end{figure*}

\section{Transformer Performance with Different Node Features}
\label{app:node_feature}
We compare the performance of Transformer model equipped with different node features. The results are shown in Table 
\ref{tab:node_feature}, where connection profile represents the corresponding row for each node in the adjacency matrix, identity feature initializes a unique one-hot vector for each node, and eigen feature generates a $k$-dimensional feature vector for each node from the $k$ eigenvectors based on the eigendecomposition on the adjacency matrix. Empirical observations demonstrate that adding identity or eigen node features to connection profiles cannot improve the model's performance. 
\begin{table*}[htbp]
	\centering
	\small
	\resizebox{0.8\linewidth}{!}{
		\begin{tabular}{ccccc}
			\toprule
			\multirow{2.5}{*}{\bf Model} &\multirow{2.5}{*}{\bf Node Feature} & \multirow{2.5}{*}{} & \multicolumn{2}{c}{\bf Dataset}\\
			\cmidrule(lr){4-5} 
			& & & {ABIDE} & {ABCD}  \\
			\midrule
			\multirow{3}{*}{VanillaTF}&Connection Profile & & 76.4±1.2 & 94.3±0.7 \\
			&Connection Profile w/ Identity Feature & & 75.4±1.9 & 94.5±0.6 \\
			&Connection Profile w/ Eigen Feature & & 75.9±2.1 & 94.0±0.8 \\
			\bottomrule
		\end{tabular}
	}
	\caption{The Performance (AUROC\%) of Transformer with Different Node Features.}
	\label{tab:node_feature}
\end{table*}

\section{Statistical Proof of the Goodness with Orthonormal Cluster Centers} \label{app:prove}

% It is difficult to compare the performance of the orthonormal and non-orthonormal initializations directly, so we propose two methods from the statistics to complete the task.
We propose two statistical methods to prove the goodness in orthonormal case since it is impractical to directly compare the performance of the orthonormal and non-orthonormal initializations.

\subsection{Proof of Theorem \ref{Thm3.1}}
We state Theorem \ref{Thm3.1} here and show the proof details.
\begin{theorem}
	For arbitrary $r > 0$, let $B_r = \{\mathcal{\bm Z} \in \mathbb{R}^V; \Vert \mathcal{\bm Z} \Vert \leq r\}$ denote the round ball centered at origin of radius $r$ with $\mathcal{\bm Z}$ being fracture vectors. Let $V_r$ be the volume of $B_r$. The variance of Softmax projection averaged over $B_r$
	\begin{align}
		\frac{1}{V_r} \int_{B_r} \sum_k^K \Big( \frac{e^{\langle \mathcal{\bm Z}, \bm E_{k\cdot}\rangle} }{\sum_{k^{\prime}}^K e^{\langle \mathcal{\bm Z}, \bm E_{k^{\prime}\cdot}\rangle}} - \frac{1}{K} \Big)^2 d \mathcal{\bm Z},
	\end{align}
	attains maximum when $\bm E$ is orthonormal.
\end{theorem}
\begin{proof}
	For simplicity, we first consider the two-dimensional case with two cluster centers $\bm E_1, \bm E_2$. Since we integrate over the round ball $B_r$, spherical symmetry allows us to set $\bm E_1 = (1,0)$ and $\bm E_1 = (\cos(\phi), \sin(\phi))$ with $\phi \in [0,\frac{\pi}{21}]$ being the angle between $\bm E_1$ and $\bm E_2$ under polar coordinates. Then the Softmax readout Eq.~\eqref{equ:project} can be rewritten as:
	\begin{align}
		& \bm P_1 = \frac{ e^{\rho \cos(\theta)} }{ e^{\rho \cos(\theta)} + e^{\rho \cos(\theta-\phi)} }, \quad \bm P_2 = \frac{ e^{\rho \cos(\theta-\phi)}  }{ e^{\rho \cos(\theta)} + e^{\rho \cos(\theta-\phi)} },
	\end{align}
	where $\theta$ is the angle between $\mathcal{\bm Z}$ and $\bm E_1$ and $\rho$ is the norm of $\mathcal{\bm Z}$. Hence, the integral is 
	\begin{align}
		F(\phi) \vcentcolon = \frac{1}{V_r} \int_{B_r} \sum_{k = 1}^2 (\bm P_k - \frac{1}{2})^2 d\mathcal{\bm Z} = \frac{1}{\pi r^2} \int_0^r \int_0^{2\pi} \Big( \frac{ e^{2\rho\cos(\theta)} + e^{2\rho\cos(\theta-\phi)}  }{ (e^{\rho\cos(\theta)} + e^{\rho\cos(\theta-\phi)} )^2 }  + \frac{1}{2} \Big) d\theta d\rho.
	\end{align}
	Our aim is to show that the integral $F(\phi)$ attains its maximum when $\bm E_1, \bm E_2$ are orthogonal. It is unclear whether the above integral has an elementary antiderivative. Thus, instead of evaluating the integral directly, we firstly prove two symmetric properties of the integrand $f(\rho,\theta,\phi)$: (a) It is straightforward to show that $f(\rho,\theta + k\pi, \phi) = f(\rho,\theta, \phi)$ for $k \in \mathbb{N}$. That is, $f$ is periodic for $\pi$ on the first argument $\theta$. (b) We have
	\begin{equation}
	\begin{aligned}
		f(\frac{\phi}{2} + \frac{\pi}{2} - \theta) & = \frac{e^{2\rho\sin(\frac{\phi}{2}+\theta)}+e^{-2\rho\sin(\frac{\phi}{2}-\theta)}}{(e^{\rho\sin(\frac{\phi}{2}+\theta)}+e^{-\rho\sin(\frac{\phi}{2}-\theta)})^2} \\
		& = \frac{e^{2\rho\sin(\frac{\phi}{2}+\theta)}+e^{-2\rho\sin(\frac{\phi}{2}-\theta)}}{e^{2\rho\sin(\frac{\phi}{2}+\theta)}+e^{-2\rho\sin(\frac{\phi}{2}-\theta)} + 2e^{\rho\sin(\frac{\phi}{2}+\theta)-\rho\sin(\frac{\phi}{2}-\theta)}} \\
		& =\frac{e^{2\rho\sin(\frac{\phi}{2}-\theta)}+e^{-2\rho\sin(\frac{\phi}{2}+\theta)}}{(e^{\rho\sin(\frac{\phi}{2}-\theta)}+e^{-\rho\sin(\frac{\phi}{2}+\theta)})^2} = f(\frac{\phi}{2}+\frac{\pi}{2}+\theta),
	\end{aligned}
    \end{equation}
	which means $f$ is symmetric with respect to $\theta = \frac{\phi}{2} + \frac{\pi}{2} + k\pi$. As the integrand $f(\rho,\theta,\phi)$ is periodic, we are allowed to compare $F(\phi_1), F(\phi_2)$ via 
	\begin{equation}
	\begin{aligned}
		\int_{\frac{\phi_1}{2}}^{\frac{\phi_1}{2} + 2\pi} f(\rho,\theta,\phi_1) d\theta = \int_0^{2\pi} f(\rho,\theta,\phi_1) d\theta ,\\ \quad \int_{\frac{\phi_1}{2}}^{\frac{\phi_2}{2} + 2\pi} f(\rho,\theta,\phi_2) d\theta = \int_0^{2\pi} f(\rho,\theta,\phi_2) d\theta.
	\end{aligned}
    \end{equation}
	The integral domain $[\frac{\phi}{2}, \frac{\phi}{2} + 2\pi]$ is taken according to the second symmetry property of $f$ and can be significant for the following trick: we take the directional derivative of $f$ along $\bm v = (1,2)$ tangent to the straight line $\theta = \frac{\phi}{2}$:
	\begin{equation}
	\begin{aligned}
		Df(\bm v) &= \frac{\partial f}{\partial \theta} + 2\frac{\partial f}{\partial \phi} \\&= \frac{2\rho e^{\rho\cos(\theta-\phi) + \rho\cos(\theta)}  (e^{\rho\cos(\theta-\phi)}-e^{\rho\cos(\theta)}) (\sin(\theta) + \sin(\theta-\phi)) }{(e^{\rho\cos(\theta-\phi)} + e^{\rho\cos(\phi)})^3}.
	\end{aligned}
    \end{equation}
	It is easy to check that in the above integral domain and for any $\rho > 0$, $Df(\bm v)$ is always non-negative. Hence, 
	\begin{align}
		f(\rho,\theta -\frac{\phi_1}{2},\phi_1) \leq f(\rho,\theta - \frac{\phi_2}{2},\phi_2) 
	\end{align}
	when $\phi_1 \leq \phi_2$. After taking integral, $F(\phi_1) \leq F(\phi_2)$ and thus it attains maximum in the orthonormal case ($\phi = \frac{\pi}{2}$). Comparing $F(\phi_1), F(\phi_2)$ without adjusting the integral domain as above cannot give a clear result because the simple partial derivative $\partial f/\partial \phi$ oscillates around zero. Higher dimensional cases follow similarly by employing spherical and hyperspherical coordinates.   
\end{proof}	

%------------------------------------------------------------------------------------------------------------------------------------
\subsection{Proof of Theorem \ref{Thm3.2}}

Theorem \ref{Thm3.2} deals with a more general case: comparing the performance of an arbitrary readout $\bm P$ defined by orthonormal cluster centers with non-orthonormal ones. 
We regard $\bm P$ as an estimated similarity probability between nodes and clusters and solve this problem from the perspective of statistics. 
The estimation is considered as a regression of samples $(\hat{\bm Z}^{(s)}, \hat{\bm E}^{(t)}, \hat{\bm P}^{(st)})$ from node features, cluster centers and similarity probabilities. 
We then judge the estimation relative to true similarity probability $\bm P_T$. 
Although it is almost impossible to find an analytic formula for $\bm P_T$, we can indirectly judge the quality of estimation. To clarify the idea, we introduce some basic concepts from statistics and prove our results on a statistical basis. 

%------------------------------------------------------------------------------------------------------------------------------------

\subsubsection{Background Knowledge of Regression Analysis}\label{C2.1}

We first consider process samples by logistic regression with cluster centers as \emph{categorical variables}.
Intuitively, non-orthonormal centers correlate with each other, which means there is an \emph{overlap} among categorical variables and makes it hard to identify the \emph{decision boundary} that leads to a failed classification. However, as far as we know, it is \emph{unclear} how to compare overlaps between orthonormal and non-orthonormal variables rigorously. Thus, we simply process samples by a general nonlinear regression. 
% To facilitate the analysis, we linearize the regression process by Gauss-Newton algorithm. 
The regression process is linearized by the Gauss-Newton algorithm to facilitate the analysis.
We judge the \emph{goodness-of-fit} describing the degree to which the regression function fits its observed value, and then conduct a hypothesis test. The \emph{goodness-of-fit} is measured by \emph{coefficient of determinate} $R^2$ \cite{Kutner1985}:

\begin{definition}\label{def:R}
	We consider a regression with $r$ independent main variables:
	\begin{align}\label{RegressionModel}
		Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_r X_r + \epsilon.
	\end{align}
	Let $\hat{x}_p = (\hat{x}_{p1},...,\hat{x}_{ps})^{\top}$ and $\hat{y} = (\hat{y}_1,...,\hat{y}_s)^{\top}$ be data sets (samples) associated with \emph{fitted values} $\check{y} = (\check{y}_1,...,\check{y}_s)$. Each difference $e_q = \hat{y}_q - \check{y}_q$ is called a \emph{residue}. We denote the mean of $\hat{x}_p$ and $\hat{y}$ by $\bar{x}_p, \bar{y}$. The variability of data set can be measured by the \emph{total sum of squares} (SST), the \emph{sum of squares of residuals} (SSR) and the \emph{explained sum of squares} (SSE) defined as (where $p=1,2,...,r$ $q=1,2,...,s$):
	\begin{align}
		\text{SST} = \sum_q (\hat{y}_q - \bar{y})^2, \quad
		\text{SSR} = \sum_q e_q^2 = \sum_q (\hat{y}_q - \check{y}_q)^2, \quad
		\text{SSE} = \sum_{q,p} (\hat{x}_{qp} - \bar{x}_p)^2.
	\end{align}
	In linear regression, SSR $+$ SSE $=$ SST and the coefficient of determination $R^2$ is defined as:
	\begin{align}
		R^2 = \frac{\text{SSE}}{\text{SST}} = 1 - \frac{\text{SSR}}{\text{SST}}.
	\end{align}
\end{definition}

Conceptually, SSE is the error cost by regression of main variables. Thus by definition, $R^2$ reveals the percentage of errors that main variables can explain in the total error SST. The value of $R^2$ is bounded by $1$. A large value of $R^2$ indicates a better fitting. However, it should be noted that an extremely-large $R^2$ could indicate overfitting.

In our problem, since our regression is nonlinear, the sum of SSR and SSE is less than SST \cite{Seber1989}. Therefore, measuring \emph{goodness-of-fit} by $R^2$ in nonlinear regression is inaccurate. A common strategy to remedy this problem is approximating nonlinear functions by polynomials via \emph{Gauss-Newton algorithm}. We provide a brief introduction here, and more details can be found in \cite{Seber1989}: for a nonlinear model $f_k$ with parameter $\delta$, in a small neighborhood of $\delta_T$-the true value of $\delta$, we have the linear expansion:
\begin{align}\label{expansion1}
	f_k(\delta) \approx f_k(\delta_T) + \sum_{m=1}^M \frac{\partial f_k}{\partial \delta_m} \Big \vert_{\delta_T} (\delta_m-\delta_{Tm}).
\end{align}
Or briefly, we write it by \emph{vector notation}:
\begin{align}
	\bm f(\delta) \approx \bm f(\delta_T) + \bm F (\delta-\delta_T),
\end{align}
where $\bm F (\delta-\delta_T)$ stands for the dot product of derivatives and differences of parameters from Eq.~\eqref{expansion1}. Suppose $\delta^{(\gamma)}$ is an approximation to the least-squares estimation $\delta$ of our model, for $\delta$ close to $\delta^{(\gamma)}$, we rewrite the expansion as:
\begin{align}
	\check{\bm P}=\bm f(\delta) \approx \bm f(\delta^{(\gamma)}) + \bm F^{(\gamma)}(\delta-\delta^{(\gamma)}),
\end{align}
where $\check{\bm P}$ denotes a fitted value of $\bm P$ and $F^{(\gamma)}(\delta-\delta^{(\gamma)})$ again means a dot product. Applying this to the residual vector $\bm e(\delta)$, we have:
\begin{align} \label{residualvector}
	\bm e(\delta) = \bm P - \bm f(\delta) \approx \bm e(\delta^{(\gamma)}) - \bm F^{(\gamma)}(\delta-\delta^{(\gamma)}).
\end{align}
Thus, the norm
\begin{align}
	S(\delta) & \vcentcolon = \Vert \bm P - f(\delta) \Vert^2 = \bm e^{\top}(\delta)\bm e(\delta) \notag \\
	& \approx \bm e^{\top}(\delta^{(\gamma)})\bm e(\delta^{(\gamma)}) -2\bm e^{\top}(\delta^{(\gamma)})\bm F^{(\gamma)}(\delta-\delta^{(\gamma)})+(\delta-\delta^{(\gamma)})^{\top}\bm F^{(\gamma)\top}\bm F^{(\gamma)}(\delta-\delta^{(\gamma)}).
\end{align}
The right-hand side is minimized with respect to $\delta$ when
\begin{align}
	\delta-\delta^{(\gamma)} = (\bm F^{(\gamma)\top}\bm F^{(\gamma)})^{-1}\bm F^{(\gamma)\top}\bm e(\delta^{(\gamma)}) = \zeta^{(\gamma)}.
\end{align}
This suggests that given a current approximation $\delta^{(\gamma)}$, the next approximation should be: 
\begin{align}
	\delta^{(\gamma+1)} = \delta^{(\gamma)} + \zeta^{(\gamma)}.
\end{align}
Expanding the nonlinear function $\bm f$ as polynomials and modifying the parameter $\delta$ as above, we can use $R^2$ to measure the \emph{goodness-of-fit}. To acquire higher accuracy in a general nonlinear regression, one can make a elaborated \emph{goodness-of-fit test} for specific fitting functions e.g., \cite{GeneralPurpose1995,Common1997}. We do not discuss this sophisticated method as it is out of the scope of this paper. 

%------------------------------------------------------------------------------------------------------------------------------------

\subsubsection{Comparing $R^2$ by Variance Inflation Factor}
\label{C2.2}

The proof of Theorem \ref{Thm3.2} consists of two steps: (a) we first prove that the \emph{regression accuracy}, the accuracy when regressing $\bm P$ is higher when sampling from orthonormal cluster centers (Theorem \ref{Thm:VIF}), and consequently (b) higher regression accuracy increases \emph{appraisal accuracy}, the accuracy when appraising an estimated value in \emph{hypothesis testing} (Theorem \ref{Thm:MSE}). 

In this subsection, we compare regression accuracy. we fix $\bm Z_{i}$ when regressing $\bm P$ via the fitted value $\check{\bm P}(\bm E_{k})$. Statistically, the expectation $\operatorname{E}(\bm P)$ of all readouts is identified as the true similarly probability $\bm P_T$. In regression analysis, the Ordinary Least Squares (OLS) guarantees asymptotically unbiased estimations. That is, when the sample size $s$ is large enough, it can be regarded as an \emph{unbiased estimation} \cite{Kutner1985}: 
\begin{align}
	\text{E}(\check{\bm P}) = \bm P_T = \text{E}(\bm P).
\end{align}
Therefore, the better the \emph{goodness-of-fit} reflected by $R^2$, the smaller the variance of estimation. To compare this, we use the concept of \emph{variance inflation factor} which reflects the inflation of weights of variables in regression:
\begin{definition} \label{def:vif}
	The variance inflation factor (VIF)$_p$ is defined as:
	\begin{align}
		(\text{VIF})_p = \frac{1}{(1 - R_p^2)},
	\end{align}
	where $R_p^2$ is the coefficient of multiple determination when $X_p$ is regressed by the r-1 other variables in the model from Eq.~\eqref{RegressionModel}. 
\end{definition}

\begin{remark}
	We discuss more details about VIF in the following context \cite{Kutner1985}. For simplicity, we denote the following collection of samples and regression coefficients:
	\begin{align*}
		\hat{X} = (\hat{x}_1,...,\hat{x}_r) = (\hat{x}_{qp}), \quad \hat{y} = (\hat{y}_1,...,\hat{y}_s)^{\top}, \quad \beta = (\beta_1,...,\beta_r).
	\end{align*}
	In the regression model Eq.~\eqref{RegressionModel}, the estimation $\check{\beta}_p$ of regression coefficients $\beta_p$ are obtained by Ordinary Least Squares (OLS):
	\begin{align} \label{betaOLS}
		\check{\beta} = (\hat{X}^{\top} \hat{X})^{-1} \hat{X}^{\top} \hat{y}.
	\end{align}
	We standardize the regression equation by covariance matrices $\sigma_y$ of $\check{y}$ and the variance $\sigma_q$ of $\hat{x}_{p}$ as
	\begin{align}
		\check{y}_q^\ast = \frac{\check{y}_q - \bar{y}}{\sigma_y}, \quad \hat{x}_{qp}^\ast = \sigma_q^{-1}(\hat{x}_{pq}-\bar{x}_p),
	\end{align}
	and 
	\begin{align}
		\label{xy}
		\check{\beta}_q^\ast = \check{\beta}_q \frac{\sigma_q}{\sigma_y} , \quad
		\check{y}^\ast = \check{\beta}_0^\ast + \check{\beta}_1^\ast X_1^\ast + \check{\beta}_2^\ast X_2^\ast + \cdots + \check{\beta}_r^\ast X_r^\ast.
	\end{align}
	Similarly to Eq.~\eqref{betaOLS}, standardized estimation of regression coefficients are equal to
	\begin{align} \label{beta}
		\check{\beta}^\ast = (\check{X}^{\ast \top} \check{X}^\ast)^{-1} \check{X}^{\ast \top}\check{y}^\ast.
	\end{align}
	
	On the other hand, the covariance matrix of the estimated regression coefficients is
	\begin{align}\label{SigmaSquare}
		\sigma^2_{\check{\beta}} = \sigma^2( X^{\top}  X)^{-1}, \quad
		\sigma^2=\sum_{q=1}^s(\check{y}_q-\bar{y})^2,
	\end{align}
	where $\sigma^2$ is the \emph{error term variance} for $X$ (cf. Definition \ref{def:R}). After standardization, it is noted that $X^{*\top}X^*$ is just the correlation matrix $r_{XX}$ of $X^*$. Hence, by Eq.~\eqref{SigmaSquare} we obtain:	
	\begin{align}
		\sigma^2_{\check{\beta}^\ast} = (\sigma^\ast)^2 r_{XX}^{-1}.
	\end{align}
	Let (VIF)$_p$ be the $p$-th diagonal element of the matrix $r_{XX}^{-1}$. The variance of $\beta_p^\ast$ is equal to: 
	\begin{align}
		\sigma^2_{\check{\beta}_p^*} = (\sigma^*)^2(\text{VIF})_{p}.
	\end{align}
	The diagonal element (VIF)$_p$ is just the variance inflation factor for $\check{\beta}_p^*$. The variance of $\beta_p^*$ can also be written as \cite{Kutner1985}
	\begin{align}
		\sigma^2_{\check{\beta}_p^*}=\frac{1}{1-R_p^2}\Big[ \frac{\sigma^2}{\sum_q(x_{qp}-\bar{x}_p)^2} \Big].
	\end{align}
	With the previous discussion, we conclude that
	\begin{align}
		(\text{VIF})_p = \frac{1}{(1-R_p^2)},
	\end{align}
	where $R_p^2$ is defined in \ref{def:vif}. 
\end{remark}

\begin{theorem}\label{Thm:VIF}
	Let
	\begin{align}
		\text{VIF} = \frac{\sum_{p=1}^{r}(\text{VIF})_p}{r-1},
	\end{align}
	where $r$ denotes the number of variables in Eq.~\eqref{RegressionModel}. Then \emph{VIF} $\geq$ 1 with equality holds if and only if the variables are orthogonal.
\end{theorem}
\begin{proof}
	To prove this, we need to generalize the definition of $R^2$. By definition,
	\begin{align}
		R^2 = \frac{\text{SSE}}{\text{SST}} = \frac{\sum_{q=1}^s (\check{y_q} - \bar{y})^2}{\sum_{q=1}^s (y_q - \bar{y})^2}=\sum_{q=1}^s(\check{y}_q^*)^2.
	\end{align}
	Substituting Eq.~\eqref{xy} into the above identity, we have
	\begin{align}
		\sum_{q=1}^s(\check{y}_q^*)^2=\sum_{q=1}^s(\check{X}_{q}^* \check{\beta}^*)^2=(X_{q}^* \check{\beta}^*)^{\top}X_{q}^* \check{\beta}^*,
	\end{align}
	and by Eq.~\eqref{beta}, we conclude that
	\begin{align}\label{rewriteR}
		R^2 = (r_{XY})^{\top}(r_{XX})^{-1}r_{XY}.
	\end{align}
    
    As the finial step, we compute $R_p^2$ from Definition \ref{def:vif} by Eq.~\eqref{rewriteR}. It should be noted that according to Definition \ref{def:vif}, $R_p^2$ is the \emph{goodness-of-fit} when $X_p$ is regressed by the r-1 other variables. These variables are uncorrelated in orthonormal case. Hence $r_{XY} = 0, R_p^2 = 0$ and \text{VIF} $= 1$.
\end{proof}

\begin{remark}
	In statistics, when a variable's VIF is greater than 1, or equivalently $R_p^2 \neq 0$, the influence of this variable on the whole estimation is inflated. It breaks the so-called \emph{absence of multicollinearity}, a fundamental principle in multiple regression analysis, and hence causes more error. Since SSE is a constant value, the error generated by the inflation would be counted into SSR, which leads to a decrease in $R^2$ by Definition \ref{def:R} (see \cite{Kutner1985, Seber1989} for more details). 
\end{remark}

%----------------------------------------------------------------------------------------------------------------------

\subsubsection{Statistical Hypothesis Testing}\label{C2.3}

The previous discussion verifies that regressing with orthonormal samples attains a higher \emph{goodness-of-fit}. In other words, it achieves a higher regression accuracy. Tools from \emph{hypothesis testing} are borrowed here to determine the appraisal accuracy mentioned at the beginning of Section \ref{C2.2}. We first introduce \emph{mean squared error} (MSE) commonly used in statistics \cite{DeGroot2012}:

\begin{definition}
\label{remarkMSE}
     Recall that the residue $e_{q} = (\hat{y}_q-\check{y}_q)$ from Definition \ref{def:R}. Then,
	\begin{align}
		\text{MSE} = \frac{1}{s}\sum_{q=1}^s(\hat{y}_q-\check{y}_q)^2 = \frac{1}{s}\sum_{q=1}^s(e_q)^2=\frac{1}{s}\bm e^{\top}\bm e.
	\end{align}
	As mentioned in \ref{C2.1}, a small coefficient of determination $R^2$ indicates a large SSR and hence leads to a large MSE. As a result of Theorem \ref{Thm:VIF}, MSE is minimized in the orthonormal case.
\end{definition}

We now assume a domain centered at the true value $\bm P_T$ of radius $d$, and treat the outside space $W$ as the \emph{rejection region}. Statistically, if the distance between $\check{\bm P}$ and $\bm P_T$ is less than a small enough $d$, we can regard them as the same. Intuitively, if fitted values $\check{\bm P}$ are largely scattered from the true value $\bm P_T$, that is, when MSE is large, it 
can interfere with our judgment of whether $\bm P$ can be identified with $\bm P_T$. Rigorously, we make a \emph{hypothesis testing} and analyze the probability of rejecting a well-estimated readout function. We prove in the following that when sampling from orthonormal cluster centers, a higher regression accuracy (Theorem \ref{Thm:VIF}) guarantees a lower MSE and therefore increases the appraisal accuracy.

\begin{theorem}\label{Thm:MSE}
	The significance level $\alpha_{E_{k\cdot}}$ reveals that the probability of rejecting a well-estimated readout is lower when sampling from orthonormal centers than sampling from non-orthonormal centers.
\end{theorem}
\begin{proof}
	Let $\bm P$ be a readout function such that $\Vert \bm P_T- \bm P \Vert \le d$ for small enough $d$. Statistically, we can treat them as the same and simply write $\check{\bm P} = \bm P_T$. In \emph{hypothesis testing}, we define \emph{null hypothesis} $H_0$ and \emph{alternative hypothesis} $H_1$ by
	\begin{align}
		H_0: \check{\bm P} = \bm P_T, \quad H_1: \check{\bm P} \neq \bm P_T,
	\end{align}
    in which $H_1$ means that we reject a well-estimated readout with $H_0$ having the opposite meaning. The rejection region for this test is thus given as $W = \{ \check{\bm P} \neq \bm P_T\}$. As a conventional procedure in \emph{hypothesis testing}, we take a suitable test statistic $T_{\bm E_k}(\bm Z_i)$ whose distribution $f$ is known \cite{DeGroot2012}. It is used to compute the probability that $\check{\bm P}$ is in the rejection region. The corresponding probability distribution is called potential function $g(\theta)$ for $W$ in this setting:
	\begin{align}
		g(\theta) = P_{\theta}(\check{\bm P} \in W) = \int_W f(T_{\bm E_k}(\bm Z_i)) d\bm Z_i \leq \alpha_{\bm E_k},\quad \theta = H_0 \cup H_1,
	\end{align}
	where the significance level $\alpha_{\bm E_k}$ is the upper bound of the probability of making mistakes (formally called \emph{type I error}) \cite{DeGroot2012}.
	
    By Theorem \ref{Thm:VIF} and Remark \ref{remarkMSE}, MSE is minimized in the orthonormal case. It can be treated as a variance of distribution $f$. Then by \emph{Vysochanskij–Petunin inequality}, a refinement of Chebyshev inequality, the integration over $W$ with orthonormal cluster centers $\bm E_k$ is smaller than that with non-orthonormal cluster centers $\bm E'_k$:
	\begin{align}
		\int_W f(T_{\bm E_k}(\bm Z_i)) d\bm Z_i \leq \int_W f(T_{\bm E'_k}(\bm Z_i)) d\bm Z_i. 
	\end{align}
    As the result holds true for any well-chosen $T_{\bm E_k}(\bm Z_i)$, $\alpha_{\bm E_k} \leq \alpha_{\bm E'_K}$, this finishes the proof.
\end{proof}


\section{Running Time}
\label{app:time}
Table \ref{tab:runningtime} shows that state-of-the-art models of Graphormer and SAN are much slower than our \methodtable and VanillaTF, mainly because their implementations are not optimized toward the unique properties of brain networks. Specifically, let $e$ be the number of edges and $v$ be the number of nodes. The calculation of Graphormer and SAN optimizes the case where $e \ll v^2$. However, brain networks usually have a small number of nodes but dense connections, i.e., $e \simeq v^2$. Therefore the optimized sparse graph operations in PyTorch Geometric~\citep{Fey/Lenssen/2019} do not work properly. On the other hand, since the number of nodes in brain networks is usually relatively small (less than 500), we can directly speed up the calculation using matrix multiplication, which is what we did in \methodtable and VanillaTF. Besides, the edge feature generation operator in Graphormer further increases the burden on its computing time.


\begin{table*}[htbp]
\centering
\small
\caption{Running time with different graph transformer methods.}
\label{tab:runningtime}
\resizebox{0.85\linewidth}{!}{
% add graphomer
\begin{tabular}{cccc}
\toprule
Method & Running Time on ABIDE (min) & & Running Time on ABCD (min)\\
\midrule
SAN  & 93.01±0.96&   & 908.05±3.6\\
Graphormer &133.52±0.54&   & 4089.86±5.7\\
VanillaTF  & 2.32±0.10 &   & 36.26±2.12\\
\methodtable  & \textbf{1.98±0.04} & & \textbf{30.31±1.16} \\
\bottomrule
\end{tabular}
}
\end{table*}

\section{Number of Parameters}
\label{app:para}

\begin{table*}[htbp]
\centering
\small
\caption{The number of parameters in different models.}
\label{tab:paras}
\resizebox{0.5\linewidth}{!}{
% add graphomer
\begin{tabular}{cccc}
\toprule
Method & \#Para on ABIDE & & \#Para on ABCD\\
\midrule
BrainNetCNN  & 0.93M &   & 0.93M\\
BrainGB & 1.08M&& 1.49M \\
FBNetGen &0.55M&& 1.18M\\
SAN  & 57.7M6&   & 186.7M\\
Graphormer &1.23M&   & 1.66M\\
VanillaTF &15.6M && 32.7M\\
\methodtable  & 4.0M & &11.2M \\
\bottomrule
\end{tabular}
}
\end{table*}


% \section{Brain Network Generation}
% \label{app:generation}

% \xkan{Take functional brain networks as an example. To generate a functional brain network $X$, a brain atlas or a set of Regions of Interest (ROI) is first selected to define the nodes. Then, the representative fMRI BOLD series from each node are obtained by either averaging or performing SVD on the time series from all the voxels within the node. Various measures have been proposed for assessing brain connectivity between pairs of nodes. We adopted the simplest and most frequently used method in the neuroimaging community, where connections are calculated as the pairwise correlations between BOLD time courses from two ROIs. After selecting the Functional Connectivity (FC) measure, the connectivity strength between each pair of nodes in the brain network is evaluated. Fisher’s transformation is further performed to transform the original FC measures and improve their distribution properties. The transformed FC measures can then be utilized to analyze functional brain networks.}


\section{Parameter Tuning}
\label{app:turing}
For \href{https://github.com/HennyJie/BrainGB}{BrainGB}, \href{https://github.com/xxlya/BrainGNN_Pytorch}{BrainGNN}, \href{https://github.com/Wayfear/FBNETGEN}{FBNetGen}, we use the authors' open-source codes. For \href{https://github.com/DevinKreuzer/SAN}{SAN} and \href{https://github.com/microsoft/Graphormer}{Graphormer}, we folk their repositories and modified them for the brain network dataset. For BrainNetCNN and VanillaTF, we implement them by ourselves. 
We use the grid search for some important hyper-parameters for these baselines based on the provided best setting. To be specific, for BrainGB, we search different readout functions \{mean, max, concat\} with different message-passing functions \{Edge weighted, Node edge concat, Node concat\}. For BrainGNN, we search different learning rates \{0.01, 0.005, 0.001\} with different feature dimensions \{100, 200\}. For FBNetGen, we search different encoders \{1D-CNN, GRU\} with different hidden dimensions \{8, 12, 16\}. For BrainNetCNN, we search different dropout rates \{0.3, 0.5, 0.7\}. For VanillaTF, we search the number of transformer layers \{1, 2, 3\} with the number of headers \{2, 4, 6\}. For SAN, we test LPE hidden dimensions \{4, 8, 16\}, the number of LPE and GT transformer layers  \{1, 2\} and the number of headers \{2, 4\} with 50 epochs training. For Graphormer, we test encoder layers \{1, 2\} and embed dimensions \{256, 512\}. Furthermore, since the rebuttal time is pretty short, we do not have enough time to dig two new baselines, BrainnetGNN and DGM, which may be why their performance is worse than others. 


\section{Software Version}
\label{app:software}
\begin{table*}[htbp]
\centering
\small
\caption{The dependency of \methodtable.}
\label{tab:software}
\resizebox{0.3\linewidth}{!}{
% add graphomer
\begin{tabular}{cc}
\toprule
Dependency & Version\\
\midrule
python  & 3.9\\
cudatoolkit & 11.3 \\
torchvision & 0.13.1\\
pytorch  & 1.12.1\\
torchaudio &0.12.1\\
wandb &0.13.1\\
scikit-learn &1.1.1\\
pandas &1.4.3\\
hydra-core &1.2.0\\
\bottomrule
\end{tabular}
}
\end{table*}


\section{The Difference between Various Initialization Methods}
\label{app:difference}
To show orthonormal initialization can produce more discriminative $\bm P$ between classes than random initialization, we calculate the difference score $d$ based on the formula 
\begin{equation}
 d = \sum_{i}^{K}\sum_{j}^{V}\frac{\lvert P_{ij}^{female}-P_{ij}^{male}\rvert}{KV},   
\end{equation}
where $V$ is the number of nodes and $K$ is the number of clusters. After running the t-test, we found the margins between random and orthonormal on both ABIDE and ABCD are significant, which is consistent with our conclusion.

\begin{table*}[h]
\centering
\small
\caption{The difference score between different initialization methods.}
\label{tab:diff}
\resizebox{0.8\linewidth}{!}{
% add graphomer
\begin{tabular}{cccc}
\toprule
Method & Difference score on ABIDE & & Difference score on ABCD\\
\midrule
Random  & 0.067±0.016 &   & 0.125±0.010\\
Orthonormal &0.085±0.015&& 0.142±0.014 \\
\bottomrule
\end{tabular}
}
\end{table*}




