
\section{Related Work and Discussion}
\label{sec:relatedwork}

\subsection{Transformers in multimodal settings}

Transformers \citep{vaswani2017attention} were first used successfully for natural language processing \citep{radford2018gpt, devlin2019bert, radford2019gpt2, brown2020gpt3}.
In recent years, they have also been shown to be effective architectures for other modalities.
One particular modality of interest is computer vision \citep{chen2020imagegpt, touvron2020deit}; in particular, \cite{dosovitskiy2020vit} showed that transformers can outperform CNNs in the high-data regime on standard object recognition benchmarks such as ImageNet and CIFAR.
Furthermore, transformers have also been used for prediction tasks over protein sequences~\citep{jumper2021alphafold, rao2021msa}, reinforcement learning \citep{parisotto2020stabilizing}, and imitation learning \citep{abramson2020imitating}.

Work specifically tackling multimodal tasks include \cite{kaiser2017multitask}, who showed a single model could learn a variety of multimodal tasks with an attention architecture.
Recent work has utilized transformers for multimodal predictive tasks, such as images and text in ViLBERT \citep{lu2019vilbert} and CLIP \citep{radford2021clip}; these approaches generally use two distinct transformers to embed images and text.
\cite{lu2020vilbertmulti} applies ViLBERT to train a single model for a variety of combined vision and language tasks.
Recent work from OpenAI \citep{goh2021multimodal} finds that some neurons learned by CLIP are activated by a particular semantic concept, regardless of if the concept is presented in language or picture form.
Our work is most similar to DALL-E \citep{ramesh2021dalle}, which uses a single transformer to embed both the image and text modalities, which we consider to be generating a ``universal latent space'' that projects any type of input into a single latent space.
Such a latent space would be useful for a model that could learn from many sources of supervision.

\subsection{Transformers in transfer settings}

There are also many works looking at transformers specifically in the context of in-modality transfer, such as ViT for vision \citep{dosovitskiy2020vit}, T5 for language \citep{raffel2019t5}, and UDSMProt for protein sequences \citep{strodthoff2020udsmprot}.
CLIP~\citep{radford2021clip} showed that training on text in addition to images could allow for zero-shot classification via providing downstream labels as text.
\cite{hernandez2021scaling} do a thorough investigation of transfer with language pretraining, notably showing transfer from English to Python, which they consider to be reasonably distanced from English; many works have also looked at transferring from one langauge to another \citep{artetxe2019cross, ponti2019towards}.
Similar to our work, \cite{papadimitriou2020music} investigate transfer for LSTMs between modalities including code, different languages, and music, finding that pretraining on ``non-linguistic data with latent structure'' can transfer to language, finding grammatical structure in a modality to be important, although we generally investigate the other direction and explore more distanced modalities. 
\cite{kiela2019supervised} make similar observations for aligning representation spaces of language and vision.
\cite{li2020communication} pretrain on a referential communication game where an emergent learned language is used to transfer to NLP tasks.
\cite{wu2021lime} found explicitly pretraining computational primitives to transfer to mathematics tasks.

\subsection{Pretraining and finetuning of transformer models}

A common trend in deep learning models is to first train a large model on an unsupervised objective on a large dataset \citep{dai2015semi, radford2018gpt} and then finetune on a small downstream dataset (e.g., by freezing the model and only finetuing the output layer).
A common method used to finetune transformers are adapter networks \citep{rebuffi2017adapter, houlsby2019adapter}, which add a fully connected residual block for each unique downstream task and also finetune the layer norm parameters.
For simplicity, we do not add the full adapter block but only train the layer norm parameters, reducing the number of parameters we consider.
These techniques used are similar to prior approaches such as FiLM \citep{perez2018film} and self-modulation \citep{chen2018selfmodulation}.
A recent direction of research has explored learning prompt templates for large models \citep{shin2020autoprompt} that simply require forward passes over the transformer.
Unlike these works, we consider finetuning on one modality (language) and finetuning on others, whereas prior work investigates finetuning on the same modality as the pretraining task.
Another interesting related work, although not investigating transformers, by \cite{frankle2020batchnorm} find randomly initialized CNNs, which only train the batchnorm affine parameters, to work well on CIFAR-10.
Their numbers are stronger than ours on CIFAR-10, but include significantly more inductive bias via a convolutional architecture, so the main takeaway is slightly more relevant towards image tasks rather than arbitrary sequences.

\subsection{Self-attention layers as optimization steps}

The nature of computation performed by self-attention layers has also been explored by other related works.
\cite{bai2019deq} show that a single transformer self-attention block can be trained to perform an optimization step towards finding a stationary point, representing the solution to the task.
\cite{ramsauer2020hopfield} show that the self-attention layer is a gradient step in a Hopfield network with a learning rate of 1, hinting that transformers are capable of storing and retrieving a large amount of patterns with an implicit energy function.
An interesting discussion from \cite{goyal2020inductive} points out a connection in viewing the key-value queries used in attention as similar to function signatures in computer programming: the key maps the input to a type (e.g., float) and the value maps the input to its value (e.g., $3.14$), and if the type matches the function signature, the function can be applied to the value -- this may be particularly relevant when we consider using a single self-attention layer applied to different modalities, as the modality may be embedded in the type.

\subsection{Global workspace theory} \label{sec:gwt}

A common technique for evaluating the embeddings learned by an unsupervised model is to train a linear layer on top of the embeddings for a downstream task \citep{donahue2016bigan, oord2018cpc, chen2020simclr}, which is reasonable when you finetune on the same modality as the pretrained one.
However, when finetuning on a different modality, as in our setting, we have to reframe this notion of generalizable embedding quality -- instead of only finetuning the output layer, we also want to finetune the input layer, and instead evaluate the ability of the frozen intermediate model to perform generalizable \emph{computation}.
This is reminiscent of Global Workspace Theory \citep{baars1993gwt}, which revolves around the notion that there is a ``blackboard'' that different parts of the brain send data to; we might consider the frozen language model as being a blackboard in this setting.
Language might also be a natural choice of model for this blackboard, as there are hypotheses that language may serve as a good multipurpose high-level representation for cognitive behavior and conscious planning \citep{andreas2017l3, goyal2020inductive}.

\subsection{Reservoir computing} \label{sec:resevoir}

Similarly to the FPT setup and Global Workspace Theory, in reservoir computing \citep{tanaka2019reservoir} and echo state networks \citep{jaeger2001echo, jaeger2004harnessing}, a random recurrent network is frozen and only the output readout layer is trained.
These models are very fast to train, using a similar setup as in Section \ref{sec:reservoir}, because the activations of the recurrent network can be cached and it is unnecessary to backpropagate over time.
Somewhat differently to the FPT architecture, echo state networks are recurrent and thus feed back into themselves, which allows the outputs of the random frozen network to modulate future inputs.
Unlike echo state networks, we also notably finetune the input and positional embeddings, which allow the inputs to the frozen network to adapt to a particular modality/for a query to the frozen network to be learned.
Echo state networks are also similar to the perspective of self-attention applying a data-dependent filter to the inputs, as opposed to 1D convolutions, which are fixed filters regardless of the input modality.
