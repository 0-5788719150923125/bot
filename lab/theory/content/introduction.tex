Recent years have seen the advent of large language models 
characterized by emergent capabilities (e.g., zero-shot generalization) arising from sheer scale alone~\cite{radford2019language,brown2020gpt3}.
Scaling LLMs results in a predictable increase in performance: simple scaling laws connect the number of parameters, pretraining dataset size, and compute budget~\cite{kaplan2020scaling,ganguli2022predictability,hoffmann2022training}, providing a clear path towards more capable models. This paradigm shift has been fueled by the wide~adoption of the Transformer~\cite{vaswani2017attention}, providing a scalable basis for practitioners to build upon. 

In this paper, we design an architecture and training setup for a multilingual 100B+ parameters model (BLOOM, \citet{bigscience_workshop_2022}), seeking to best use a fixed 1,000,000 A100-hours budget. Because of the costs involved with training large language models, we cannot exhaustively explore the landscape of possible models. Instead, we position ourselves as practitioners exploring "off-the-shelf" solutions. We thus test promising additions to the Transformer to attempt to reproduce their findings in a controlled, large-scale setting.

Although our main goal was to prepare the architecture and training setup of BLOOM, our findings are also valuable for practitioners building models in the 1-10B range, as they equally improve the performance of such smaller models. At variance with major works on large language models, we also make a significant effort towards reproducibility and openness: all of our pretrained models, code, and notes from our weekly meetings are made available. See Appendix \ref{sec:artefacts} for the relevant links.

\paragraph{Contributions.} We first study the impact of pretraining corpora, positional embeddings, activation functions, and embedding norm on zero-shot generalization. We base our study on the popular GPT-2 architecture \cite{radford2019language}, with experiments at the 1.3B parameters scale. We then consider the impact of massive multilinguality, showing language-specific scaling laws in a multilingual setting for the first time. Finally, we describe our approach to drafting an architecture for the final 176B parameters BLOOM model.

