@inproceedings{vsurveilance,
  author={K. K. {Htike} and O. O. {Khalifa} and H. A. {Mohd Ramli} and M. A. M. {Abushariah}},
  booktitle={The Third International Conference on e-Technologies and Networks for Development (ICeND2014)}, 
  title={Human activity recognition for video surveillance using sequences of postures}, 
  year={2014},
  volume={},
  number={},
  pages={79-82},
  doi={10.1109/ICeND.2014.6991357}}

@InProceedings{hcinter/978-3-540-78566-8_10,
author="Choi, Jin
and Cho, Yong-il
and Han, Taewoo
and Yang, Hyun S.",
editor="Wyeld, Theodor G.
and Kenderdine, Sarah
and Docherty, Michael",
title="A View-Based Real-Time Human Action Recognition System as an Interface for Human Computer Interaction",
booktitle="Virtual Systems and Multimedia",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="112--120",
isbn="978-3-540-78566-8"
}
@inproceedings{yan2018spatial,
  title={Spatial temporal graph convolutional networks for skeleton-based action recognition},
  author={Yan, Sijie and Xiong, Yuanjun and Lin, Dahua},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@inproceedings{2sagcn2019cvpr,  
      title     = {Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition},  
      author    = {Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu},  
      booktitle = {CVPR},  
      year      = {2019},  
}

@inproceedings{attn2017all,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, undefinedukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

 @InProceedings{katharopoulos20a, 
 title = {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention}, 
 author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois}, 
 booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
 pages = {5156--5165}, 
 year = {2020}, 
 editor = {Hal Daumé III and Aarti Singh}, 
 volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, 
 publisher = {PMLR}, 
 pdf = {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf}, 
 url = { http://proceedings.mlr.press/v119/katharopoulos20a.html }, 
 abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our \emph{Linear Transformers} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.} 
 } 
 
 @inproceedings{choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@inproceedings{tsai2019TransformerDissection,
  title={Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={EMNLP},
  publisher={ACL},
  year={2019},
}

@inproceedings{shen2021efficient,
    author = {Zhuoran Shen and Mingyuan Zhang and Haiyu Zhao and Shuai Yi and Hongsheng Li},
    title = {Efficient Attention: Attention with Linear Complexities},
    booktitle = {WACV},
    publisher={IEEE},
    year = {2021},
}

@misc{tay2020efficient,
      title={Efficient Transformers: A Survey}, 
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2020},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@online{torch_scatter,
  author = {Matthias Fey},
  title = {{PyTorch Scatter}},
  year = 2021,
  url = {https://github.com/rusty1s/pytorch_scatter},
  urldate = {2020-09-30}
}

@online{torch_sparse,
  author = {Matthias Fey},
  title = {{PyTorch Sparse}},
  year = 2021,
  url = {https://github.com/rusty1s/pytorch_sparse},
  urldate = {2020-09-30}
}

@inproceedings{neurips2019treepositional,
 author = {Shiv, Vighnesh and Quirk, Chris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Novel positional encodings to enable tree-based transformers},
 url = {https://proceedings.neurips.cc/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{omote-etal-2019-dependency,
    title = "Dependency-Based Relative Positional Encoding for Transformer {NMT}",
    author = "Omote, Yutaro  and
      Tamura, Akihiro  and
      Ninomiya, Takashi",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://www.aclweb.org/anthology/R19-1099",
    doi = "10.26615/978-954-452-056-4_099",
    pages = "854--861",
    abstract = "This paper proposes a new Transformer neural machine translation model that incorporates syntactic distances between two source words into the relative position representations of the self-attention mechanism. In particular, the proposed model encodes pair-wise relative depths on a source dependency tree, which are differences between the depths of the two source words, in the encoder{'}s self-attention. The experiments show that our proposed model achieves 0.5 point gain in BLEU on the Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.",
}

@inproceedings{lie2014,
  author={R. {Vemulapalli} and F. {Arrate} and R. {Chellappa}},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group}, 
  year={2014},
  volume={},
  number={},
  pages={588-595},
  doi={10.1109/CVPR.2014.82}}
  
  @inproceedings{actionlet2012,
  author={J. {Wang} and Z. {Liu} and Y. {Wu} and J. {Yuan}},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Mining actionlet ensemble for action recognition with depth cameras}, 
  year={2012},
  volume={},
  number={},
  pages={1290-1297},
  doi={10.1109/CVPR.2012.6247813}}

@inproceedings{rnndu,
  author={ {Yong Du} and W. {Wang} and L. {Wang}},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Hierarchical recurrent neural network for skeleton based action recognition}, 
  year={2015},
  volume={},
  number={},
  pages={1110-1118},
  doi={10.1109/CVPR.2015.7298714}}

@inproceedings{xiememory,
author = {Xie, Chunyu and Li, Ce and Zhang, Baochang and Chen, Chen and Han, Jungong and Liu, Jianzhuang},
year = {2018},
month = {07},
pages = {1639-1645},
title = {Memory Attention Networks for Skeleton-based Action Recognition},
doi = {10.24963/ijcai.2018/227}
}

@inproceedings{zhang2017view,
  title={View adaptive recurrent neural networks for high performance human action recognition from skeleton data},
  author={Zhang, Pengfei and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Xue, Jianru and Zheng, Nanning},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2117--2126},
  year={2017}
}

@article{ts3dliu,
author = {Liu, Hong and Tu, Juanhui and Liu, Mengyuan},
year = {2017},
month = {05},
pages = {},
title = {Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition}
}

@article{tdskrepke,
  author={Q. {Ke} and M. {Bennamoun} and S. {An} and F. {Sohel} and F. {Boussaid}},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A New Representation of Skeleton Sequences for 3D Action Recognition}, 
  year={2017},
  volume={},
  number={},
  pages={4570-4579},
  doi={10.1109/CVPR.2017.486}}
  
@inproceedings{directed2019,
  author={L. {Shi} and Y. {Zhang} and J. {Cheng} and H. {Lu}},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Skeleton-Based Action Recognition With Directed Graph Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={7904-7913},
  doi={10.1109/CVPR.2019.00810}}
  
@InProceedings{Li_2019_CVPR,
author = {Li, Maosen and Chen, Siheng and Chen, Xu and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
title = {Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{shi2019skeleton,
  title={Skeleton-based action recognition with directed graph neural networks},
  author={Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7912--7921},
  year={2019}
}

@inproceedings{liu2020disentangling,
  title={Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition},
  author={Liu, Ziyu and Zhang, Hongwen and Chen, Zhenghao and Wang, Zhiyong and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={143--152},
  year={2020}
}

@inproceedings{cheng2020shiftgcn,  
  title     = {Skeleton-Based Action Recognition with Shift Graph Convolutional Network},  
  author    = {Ke Cheng and Yifan Zhang and Xiangyu He and Weihan Chen and Jian Cheng and Hanqing Lu},  
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year      = {2020},  
}

@inproceedings{song2020stronger,
  author    = {Song, Yi-Fan and Zhang, Zhang and Shan, Caifeng and Wang, Liang},
  title     = {Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-Based Action Recognition},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia (ACMMM)},
  pages     = {1625--1633},
  year      = {2020},
  isbn      = {9781450379885},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3394171.3413802},
  doi       = {10.1145/3394171.3413802},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186"
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{touvron2020deit,
  title={Training data-efficient image transformers & distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\'e J\'egou},
  journal={arXiv preprint arXiv:2012.12877},
  year={2020}
}

@inproceedings{huang2019ccnet,
  title={Ccnet: Criss-cross attention for semantic segmentation},
  author={Huang, Zilong and Wang, Xinggang and Huang, Lichao and Huang, Chang and Wei, Yunchao and Liu, Wenyu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={603--612},
  year={2019}
}

@article{plizzari2020spatial,
  title={Spatial temporal transformer network for skeleton-based action recognition},
  author={Plizzari, Chiara and Cannici, Marco and Matteucci, Matteo},
  journal={arXiv preprint arXiv:2008.07404},
  year={2020}
}

@inproceedings{shahroudy2016ntu,
  title={Ntu rgb+ d: A large scale dataset for 3d human activity analysis},
  author={Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1010--1019},
  year={2016}
}

@article{liu2019ntu,
  title={Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding},
  author={Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={10},
  pages={2684--2701},
  year={2019},
  publisher={IEEE}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{swish2017,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{deepspeed2020kdd,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {distributed deep learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{pytorch_geometric_Fey2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}

@inproceedings{simgnn2019,
author = {Bai, Yunsheng and Ding, Hao and Bian, Song and Chen, Ting and Sun, Yizhou and Wang, Wei},
title = {SimGNN: A Neural Network Approach to Fast Graph Similarity Computation},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290967},
doi = {10.1145/3289600.3290967},
pages = {384–392},
numpages = {9},
keywords = {network embedding, graph edit distance, neural networks, graph similarity computation},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

