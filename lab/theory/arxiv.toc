\@ifundefined {etoctocstyle}{\let \etoc@startlocaltoc \@gobble \let \etoc@settocdepth \@gobble \let \etoc@depthtag \@gobble \let \etoc@setlocaltop \@gobble }{}
\etoc@depthtag {mtchapter}
\contentsline {section}{\numberline {1}Introduction}{3}{section.1}%
\contentsline {section}{\numberline {2}Methodology}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Tasks}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Architecture}{4}{subsection.2.2}%
\contentsline {section}{\numberline {3}Empirical Evaluations}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Can pretrained language models transfer to different modalities?}{5}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}What is the importance of the pretraining modality?}{6}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}How important is the transformer architecture compared to LSTM architecture?}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Does language pretraining improve compute efficiency over random initialization?}{8}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Do the frozen attention layers attend to modality-specific tokens?}{8}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Does freezing the transformer prevent overfitting or underfitting?}{9}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Does performance scale with model size?}{9}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Can performance be attributed simply to better statistics for initialization?}{10}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Can we train a transformer by only finetuning the output layer?}{10}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}What is the role of model depth in token mixing?}{11}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Can training more parameters improve performance?}{11}{subsection.3.11}%
\contentsline {subsection}{\numberline {3.12}Which parameters of the model are important to finetune?}{12}{subsection.3.12}%
\contentsline {subsection}{\numberline {3.13}Is finetuning layer norm necessary for FPT to perform well?}{12}{subsection.3.13}%
\contentsline {subsection}{\numberline {3.14}How well do the trends hold across other transformer models?}{13}{subsection.3.14}%
\contentsline {section}{\numberline {4}Related Work and Discussion}{13}{section.4}%
\contentsline {subsection}{\numberline {4.1}Transformers in multimodal settings}{13}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Transformers in transfer settings}{13}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Pretraining and finetuning of transformer models}{14}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Self-attention layers as optimization steps}{14}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Global workspace theory}{14}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Reservoir computing}{15}{subsection.4.6}%
\contentsline {section}{\numberline {5}Conclusion}{15}{section.5}%
\contentsline {section}{Acknowledgements}{15}{section*.25}%
\contentsline {section}{References}{20}{section*.32}%
\contentsline {section}{Appendix}{21}{section*.32}%
\etoc@depthtag {mtappendix}
\contentsline {section}{\numberline {A}Summary of arXiv Updates}{22}{appendix.A}%
\contentsline {section}{\numberline {B}Background on Transformers}{22}{appendix.B}%
\contentsline {subsection}{\numberline {B.1}Self-Attention}{22}{subsection.B.1}%
\contentsline {subsection}{\numberline {B.2}Positional Embeddings}{22}{subsection.B.2}%
\contentsline {subsection}{\numberline {B.3}Layer Norm}{23}{subsection.B.3}%
\contentsline {subsection}{\numberline {B.4}Pretraining Objective}{23}{subsection.B.4}%
\contentsline {subsection}{\numberline {B.5}Model Sizes}{23}{subsection.B.5}%
\contentsline {section}{\numberline {C}Experimental Details}{23}{appendix.C}%
\contentsline {section}{\numberline {D}Details by Table}{24}{appendix.D}%
\contentsline {subsection}{\numberline {D.1}Can pretrained language models transfer to different modalities?}{24}{subsection.D.1}%
\contentsline {subsection}{\numberline {D.2}What is the importance of the pretraining modality?}{25}{subsection.D.2}%
\contentsline {subsection}{\numberline {D.3}How important is the transformer architecture compared to LSTM architecture?}{25}{subsection.D.3}%
\contentsline {subsection}{\numberline {D.4}Does language pretraining improve compute efficiency over random initialization?}{26}{subsection.D.4}%
