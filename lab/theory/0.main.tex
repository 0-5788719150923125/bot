\documentclass[a4paper]{article}
\usepackage{INTERSPEECH2021}
\input{packages}

\title{Grapheme-to-Phoneme Transformer Model for Transfer Learning Dialects}
\name{Eric Engelhart, Mahsa Elyasi, Gaurav Bharaj}
\address{
  AI Foundation, USA
}
\email{{\{ericengelhart, mahsa, gaurav}\}@aifoundation.com}

\begin{document}
\maketitle
% Submission Number: 1919
% Submission Passcode: 1919X-P8A7D6F7B3

\begin{abstract}
Grapheme-to-Phoneme (G2P) models convert words to their phonetic pronunciations. Classic G2P methods include rule-based systems and pronunciation dictionaries, while modern G2P systems incorporate learning, such as, LSTM and Transformer-based attention models. Usually, dictionary-based methods require significant manual effort to build, and have limited adaptivity on unseen words. And transformer-based models require significant training data, and do not generalize well, especially for dialects with limited data.

% Method, what is novel?
We propose a novel use of transformer-based attention model that can adapt to unseen dialects of English language, while using a small dictionary. We show that our method has potential applications for accent transfer for text-to-speech, and for building robust G2P models for dialects with limited pronunciation dictionary size.

% Results
We experiment with two English dialects: Indian and British. A model trained from scratch using 1000 words from British English dictionary, with 14211 words held out, leads to phoneme error rate (PER) of 26.877\%, on a test set generated using the full dictionary. The same model pretrained on CMUDict American English dictionary~\cite{weide2005carnegie}, and fine-tuned on the same dataset leads to PER of 2.469\% on the test set.
\end{abstract}

\noindent\textbf{Index Terms}: Grapheme-to-Phoneme (G2P), end-to-end models, dialects, transfer learning, pronunciation generation

\input{1.intro}
\input{2.rw}
\input{3.model}
\input{4.experiments}
\input{5.conclusions}

\bibliographystyle{IEEEtran}
\bibliography{0.main.bib}
\end{document}


% 
% Grapheme-to-Phoneme (G2P) models convert words to their phonetic pronunciations. \me{make the following sentence more bold, right now is too general}They are a key component of speech technologies, for examples, Text-To-Speech (TTS). There have been different approaches to G2P as new methods have been applied.  Classical methods include rule-based systems, and pronunciation dictionaries.  Recently, modern G2P systems have begun to incorporate neural network methods, including LSTM models, and most recently, Transformer self-attention based models. Classical dictionary-based approaches have limitations in adapting to new words and require significant manual effort to build.  However, transformer-based models can require significant amounts of data to train, making it difficult to build robust G2P models, especially for language dialects with extremely limited data.

% In this work, we propose a transformer-based attention model that can adapt to unseen accents of English language while using only a small size of dictionary. We show that our proposed model has potential application in accent transfer for TTS, and building robust G2P models for dialects with limited pronunciation dictionary size.
% We examine potential of our proposed model on two different English dialects: Indian English pronunciation and British English pronunciation (Received Pronunciation).    

% Words were held-out from the Indian English and British English dictionaries for testing.  The top 1000 most common English words according the the 1 Trillion Word Corpus were retained in the training set to ensure extremely common words, such as "the", were not removed from the training set, which would significantly reduce the amount of training data available for the models, and such words would be a priority to add to any pronunciation dictionary, so even extremely small dictionaries are likely contain these words.

% When training a model from scratch on 20\% of the British English dictionary, holding out 80\% of the dictionary's word list, it had a phoneme error rate (PER) of 17.71\% on a test set where the entire dictionary was used.  A model pretrained on the CMUDict American English dictionary and finetuned on the same data had a PER of 3.1\%.

% testing the result with held-out words.
% G2P uses a dictionary, but fail for nouns, etc. based on cultural references, accents. 
% Ex: supercalifragilisticexpialidocious
% Conventional G2P fails on ...? and can't handle edge cases.