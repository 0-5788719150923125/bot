\documentclass{article}
\pdfoutput=1 

% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips2022}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amsmath,amsthm,amssymb,amsfonts,amscd,keyval}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{float}
\usepackage{xspace}
\usepackage{subfig}
\usepackage{bbding}
\usepackage{makecell}


\newcommand{\methodfull}{\textsc{Brain Network Transformer}\xspace}
% \newcommand{\methodshort}{\textsc{BrainTransformer}\xspace}
\newcommand{\methodtable}{\textsc{BrainNetTF}\xspace}
\newcommand{\pooling}{\textsc{Orthonormal Clustering Readout}\xspace}
\newcommand{\poolingshort}{\textsc{OCRead}\xspace}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\xkan}[1]{{\color{blue}{#1}}}
\newcommand{\dvd}[1]{{\color{red}{#1}}}
\newcommand{\hejie}[1]{{\color{blue}{#1}}}
\newcommand{\zzl}[1]{{\color{brown}{#1}}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{example}{Example}
\newtheorem{proposition}[definition]{Proposition}

\theoremstyle{plain}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{\methodfull}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
Xuan Kan$^1$ \quad Wei Dai$^2$ \quad Hejie Cui$^1$ \quad Zilong Zhang$^3$ \quad Ying Guo$^1$ \quad Carl Yang$^1$\\
$^1$Emory University \quad $^2$Stanford University \quad $^3$University of International Business and Economics\\
\texttt{\{xuan.kan,hejie.cui,yguo2,j.carlyang\}@emory.edu}\\
\texttt{dvd.ai@stanford.edu} \quad \texttt{201957020@uibe.edu.cn}
}


\begin{document}
	\maketitle
	\begin{abstract}
	Human brains are commonly modeled as networks of Regions of Interest (ROIs) and their connections for the understanding of brain functions and mental disorders. Recently, Transformer-based models have been studied over different types of data, including graphs, shown to bring performance gains widely. In this work, we study Transformer-based models for brain network analysis. Driven by the unique properties of data, we model brain networks as graphs with nodes of fixed size and order, which allows us to (1) use connection profiles as node features to provide natural and low-cost positional information and (2) learn pair-wise connection strengths among ROIs with efficient attention weights across individuals that are predictive towards downstream analysis tasks. Moreover, we propose an \textsc{Orthonormal Clustering Readout} operation based on self-supervised soft clustering and orthonormal projection. This design accounts for the underlying functional modules that determine similar behaviors among groups of ROIs, leading to distinguishable cluster-aware node embeddings and informative graph embeddings. Finally, we re-standardize the evaluation pipeline on the only one publicly available large-scale brain network dataset of ABIDE, to enable meaningful comparison of different models. Experiment results show clear improvements of our proposed \textsc{Brain Network Transformer} on both the public ABIDE and our restricted ABCD datasets. The implementation is available at \url{https://github.com/Wayfear/BrainNetworkTransformer}.
	\end{abstract}
	
	
	
	\input{sections/intro.tex}
	\input{sections/related.tex}
	\input{sections/method.tex}
	\input{sections/experiments.tex} 
	\input{sections/conclusion.tex}
	\input{sections/acknowledgment.tex}
	
	\bibliographystyle{plain}
	\bibliography{reference}
	\clearpage
	\input{sections/checklist.tex}
	\clearpage
	\appendix
	\input{sections/appendix.tex}
	
	
	
	
	% Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
	% This section will often be part of the supplemental material.
	
	
\end{document}
