\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
%\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{tablefootnote}
\usepackage{hyperref}       % hyperlinks
\usepackage{float}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%-----------------------------------
% Additionally installed packages ...
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{array, booktabs}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{caption}
\usepackage{comment}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[subrefformat=parens]{subcaption}
%-----------------------------------
% Customized Setup ...
\captionsetup[table]{skip=5pt}
\captionsetup[figure]{skip=5pt}

\definecolor{myblack}{rgb}{0.5, 0.5, 0.5}
%-----------------------------------
% User defined functions ...
\newcommand{\kojima}[1]{{\color{red}[{#1} --TK]}}
\newcommand{\mr}[1]{{\color{teal}[{#1} --MR]}}
\newcommand{\sg}[1]{{\color{orange}[{#1} --SG]}}
\newcommand{\yi}[1]{{\color{magenta}[{#1} --YI]}}
\newcommand{\lighttext}[1]{{\color{myblack}{#1}}}
\newcommand{\algcomment}[1]{$\quad$\lighttext{{#1}}}
%\newcommand{\kojima}[1]{{\color{black}{#1}}}

\newcommand{\CoT}{chain of thought\xspace}
\newcommand{\ours}{Zero-shot-CoT\xspace}
\newcommand{\oursvtwo}{Zero-shot-Ex-CoT\xspace}
\newcommand{\theirs}{Few-shot-CoT\xspace}
\newcommand{\theirsz}{Zero-shot\xspace}
\newcommand{\theirsf}{Few-shot\xspace}
%\newcommand{\okmark}{{\textbf{\color{green}{$\checkmark$}}}}
\newcommand{\okmark}{{\textbf{\textcolor[rgb]{0.1, 0.5, 0.1}{$\checkmark$}}}}
\newcommand{\ngmark}{{\textbf{\color{red}{\ding{55}}}}}
%\newcommand{\davinci}{Text-davinci-002 (175B)\xspace}
\newcommand{\davinci}{text-davinci-002\xspace}
\newcommand{\bblue}[1]{{\textbf{\color{blue}{#1}}}}
\newcommand{\bred}[1]{{\textbf{\color{red}{#1}}}}
\newcommand{\bblack}[1]{{\textbf{\color{black}{#1}}}}

\newcommand{\mysection}{\S\xspace}

\newcommand{\myspace}{$\:$}
%-----------------------------------

%\title{Formatting Instructions For NeurIPS 2022}
\title{Large Language Models are Zero-Shot Reasoners}

%\title{Let's Think: Zero-Shot Chain-of-Thought Prompting as Logical Dark Knowledge}%SG: among keywords, i think "large language model" is the most redundant".

%\title{Let's Think Step By Step : Eliciting Chain of Thought From Large Language Models By Prompt-Based Template}
% \title{Let's Think: Zero-Shot Prompt for Distilling Logical Knowledge from Large Language Models}
%\title{Zero-Shot Chain-of-Thought Prompting Distill Logical Dark Knowledge from Large Language Models} % this sounds too long
%\title{Logical Dark Knowledge: Are Pretrained Language Models Few-shot or Zero-shot Learners?}
% \title{Let's Think Step by Step: Prompt for Distilling the Logical Knowledge from Large Language Models}
% \title{Let's Think Step By Step : Prefix Prompt for Extracting Logical Dark Knowledge from Large Language Models}
% \title{Let's Think Step By Step : Meta Prompt for Extracting Logical Dark Knowledge from Large Language Models}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  %David S.~Hippocampus\thanks{Use footnote for %providing further information
  %  about author (webpage, alternative %address)---\emph{not} for acknowledging
  %  funding agencies.} \\
  %Department of Computer Science\\
  %Cranberry-Lemon University\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  Takeshi Kojima%\thanks{First two authors have equal contribution.} 
  \\
  The University of Tokyo \\
  \texttt{t.kojima@weblab.t.u-tokyo.ac.jp} \\
  % examples of more authors
  \And
  Shixiang Shane Gu
  %\footnotemark[1] 
  \\
  Google Research, Brain Team \\
  % Address \\
  %\texttt{email} \\
  \AND
  Machel Reid \\
  Google Research\thanks{Work done while at The University of Tokyo.} \\
  %Address \\
  %\texttt{machelreid@weblab.t.u-tokyo.ac.jp} \\
  \And
  Yutaka Matsuo \\
  The University of Tokyo \\
  % Address \\
  %\texttt{matsuo@weblab.t.u-tokyo.ac.jp} \\
  \And
  Yusuke Iwasawa \\
  The University of Tokyo \\
  % Address \\
  %\texttt{iwasawa@weblab.t.u-tokyo.ac.jp} \\
}

%\author[1]{Takeshi Kojima}
%\author[2]{Shixiang Shane Gu}
%\author[1]{Machel Reid}
%\author[1]{Yutaka Matsuo}
%\author[1]{Yusuke Iwasawa}
%\affil[1]{The University of Tokyo}
%\affil[2]{Google Research, Brain Team}

\begin{document}


\maketitle

% notes:
% requests to machel:
% (1) finish Table 1. we are likely missing a lot of references for Zero-Shot Prompts that are Task-Dependent (known as instruction prompts?). also, any other task-independent examples like ours? e.g. similar things to "unreal engine trick". or "i am a mathematician." "i am a senior programmer." tricks for task-independent prompting.
% (2) 


\begin{abstract}
% How can we understand and enhance the logical ability of pre-trained Large Language Models (LLMs)? 
% This paper proposes a meta prompt, ``Let's think step by step``, and empirically shows that it can surprisingly well extract the chain-of-though from LLMs. 
% Recent studies show that example-based prompt can improve the performance of logical reasoning, but it requires manual examples of step-by-step reasoning process for each tasks in hand. 
% This paper shows that simple template-based prompt, such as ``Let's think step by step``, can surprisingly well extract the chain-of-though from LLMs without changing it depend on the target tasks. 
% Pretrained large language models (LLMs) are widely deployed in many sub-fields of natural language processing (NLP).
% \CoT(CoT), which mimics step-by-step reasoning process of a person, a recent approach which has been validated in LLMs. 
% This study shows that the existing in-context based \CoT approach can be reconstructed as zero-shot approach. 
% Specifically, we insert prompt-based template, such as "Let's think step by step" before answering questions, in order to elicit intrinsic reasoning ability from LLMs without inserting any example-based template into context. 
% This finding leads us to two paradigm change: First, we can immediately adapt our LLMs to unknown tasks without preparing hand-crafted examples in advance. 
% Second, we would be able to observe what LLMs are thinking, or elicit logical dark knowledge from LLMs, because we do not need to inject human biases as examples into context. 
% Experiment results demonstrate that our method improves the performance on several benchmark reasoning tasks including Arithmetics (SingleEq, MultiArith, GSM8K, AQUA-RAT, SVAMP) and logical reasoning (Date Understanding, Object Tracking). 
% Our code and experiment logs are available at \url{https://github.com/kojima-takeshi188/zero_cot}. \sg{remember to deanonymize}
% 
% SG general sketch for abstract: (1) LLMs very good and few-shot learners, (2) CoT really impactful results, (3) we can get CoT zero-shot in fact, (4) practical implications (advantages of zero-shot vs few-shot), (5) philosophical implications (dark knowledge in pretrained models), (6) we show that LLMs are *surprisingly* great zero-shot reasoners. 
% SG: it's a nontrivial paper to sell. the complexity/novelty of the method itself is limited, so we need to iterate more on the core stories. I'll continue tomorrow.

% SG: why is CoT special? among all prompt engineering work. the more special CoT is, the easier it is to sell our work.


Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent \textit{few-shot} learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult \textit{system-2} tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent \textit{zero-shot} reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects),  without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental \textit{zero-shot} capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars. 
%\footnote{Our code is available at \url{https://github.com/kojima-takeshi188/zero_shot_cot}}

% While such result may be obvious in hindsight, we discuss practical implications of this simple finding. 
% We categorize and analyze few-shot versus zero-shot prompting, and discuss shortcomings of the former approach, such as requirement for per-task engineering and sensitivity to the choices of exemplars. 

% Drawing analogy to hidden class probability ratios -- \textit{dark knowledge} -- in distillation literature, we term this hidden zero-shot reasoning capability as \textit{logical dark knowledge} and suggests high-level, task-agnostic core knowledge may be extracted through careful prompting. 

\end{abstract}

%\kojima{We also validate by quantitative analysis that unlocking human biases causes LLMs to think more flexibly.}

\section{Introduction}
%\label{}
%\subsection{}

\begin{figure}[t]
  \begin{center}
   %\includegraphics[width=0.9\columnwidth]{fig_overview_1}
   \includegraphics[width=\columnwidth]{conceptual_differences.pdf}
  \end{center}
  \caption{Example inputs and outputs of GPT-3 with (a) standard \theirsf (\citep{brown2020language}), (b) \theirs (\citep{cot_wei}), (c) standard \theirsz, and (d) ours (\ours). 
  Similar to \theirs, \ours facilitates multi-step reasoning (blue text) and reach correct answer where standard prompting fails. 
  Unlike \theirs using step-by-step reasoning examples \textbf{per task}, ours does not need any examples and just uses the same prompt ``Let's think step by step'' \textit{across all tasks} (arithmetic, symbolic, commonsense, and other logical reasoning tasks).
  %While \theirs needs step-by-step reasoning examples \textbf{per task},  \yi{todo: finalize it}
  }
  \label{fig_overview_1}
\end{figure}

% Recent Large Language Models (LLMs) \citep{transformer, bert, t5, gpt3, lamda, gopher, palm} demonstrate remarkable results in many fields of Natural Language Processing. 
Scaling up the size of language models has been key ingredients of recent revolutions in natural language processing (NLP) \citep{transformer, bert, t5, brown2020language, lamda, gopher, palm}. 
The success of large language models (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various tasks by simply conditioning the models on a few examples (few-shot) or instructions describing the task (zero-shot). 
The method of conditioning the language model is called ``prompting'' \citep{liu2021pre}, and designing prompts either manually \citep{schick2020s,prompt1} or automatically \citep{gao2021making,shin2020autoprompt} has become a hot topic in NLP. 

In contrast to the excellent performance of LLMs in intuitive and single-step \textit{system-1}~\citep{stanovich2000individual} tasks with task-specific few-shot or zero-shot prompting~\citep{liu2021pre}, even language models at the scale of 100B or more parameters had struggled on \textit{system-2} tasks requiring slow and multi-step reasoning \citep{gopher}. 
To address this shortcoming, \citet{cot_wei, cot_wei_sc} have proposed \textit{\CoT} prompting (CoT), which feed LLMs with the step-by-step reasoning examples rather than standard question and answer examples (see Fig. \ref{fig_overview_1}-a). 
Such \CoT demonstrations facilitate models to generate a reasoning path that decomposes the complex reasoning into multiple easier steps. 
Notably with CoT, the reasoning performance then satisfies the scaling laws better and jumps up with the size of the language models. For example, when combined with the 540B parameter PaLM model~\citep{palm}, \CoT prompting significantly increases the performance over standard few-shot prompting across several benchmark reasoning tasks, e.g., GSM8K (17.9\% $\rightarrow$ 58.1\%). 

While the successes of CoT prompting~\citep{cot_wei}, along those of many other task-specific prompting work~\citep{gao2021making,schick2020s,liu2021pre}, are often attributed to LLMs' ability for few-shot learning~\citep{brown2020language}, we show that LLMs are decent \textit{zero-shot} reasoners by adding a simple prompt, \textit{Let's think step by step}, to facilitate step-by-step thinking before answering each question (see~\autoref{fig_overview_1}). 
% In other words, our method purely evaluates the zero-shot ability of LLMs. 
% \yi{should describe the notation here. Few-shot CoT vs. Zero-shot CoT or chain-of-though demonstrations or chain-of-though instruction. }
Despite the simplicity, our \ours successfully generates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a problem where the standard zero-shot approach fails. 
Importantly, our \ours is versatile and \textit{task-agnostic}, unlike most prior task-specific prompt engineering in the forms of examples (few-shot) or templates (zero-shot)~\citep{liu2021pre}: 
it can facilitate step-by-step answers across various reasoning tasks, including arithmetic (MultiArith~\citep{multiarith}, GSM8K~\citep{gsm8k}, AQUA-RAT~\citep{aqua}, and SVAMP~\citep{svamp}), symbolic reasoning (Last letter and Coin flip), commonsense reasoning (CommonSenseQA~\citep{commonsenseqa} and Strategy QA~\citep{strategyqa}), and other logical reasoning tasks (Date understanding and Tracking Shuffled Objects from BIG-bench~\citep{bigbench}) without modifying the prompt per task.

%\sg{discuss experiment results and end with takeaway message} 
%\sg{todo: complete this} 
We empirically evaluate \ours against other prompting baselines in~\autoref{tab:few_shot}. While our \ours underperforms \theirs with carefully-crafted and task-specific step-by-step examples, \ours achieves enormous score gains compared to the zero-shot baseline, e.g. from 17.7\% to 78.7\% on MultiArith and from 10.4\% to 40.7\% on GSM8K with large-scale InstructGPT model (text-davinci-002). 
We also evaluate \ours with another off-the-shelf large model, 540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K. 
Importantly, with our single fixed prompt, zero-shot LLMs have a significantly better scaling curve comparable to that of the few-shot CoT baseline. 
We also show that besides \theirs requiring human engineering of multi-step reasoning prompts, their performance deteriorates if prompt example question types and task question type are unmatched, suggesting high sensitivity to per-task prompt designs.   
In contrast, the versatility of this single prompt across diverse reasoning tasks hints at untapped and understudied \textit{zero-shot} fundamental capabilities of LLMs, such as higher-level broad cognitive capabilities like generic logical reasoning~\citep{chollet2019measure}. 
While the vibrant field of LLMs started out from the premise of excellent few-shot learners~\citep{brown2020language}, we hope our work encourages more research into uncovering \textit{high-level} and \textit{multi-task} zero-shot capabilities hidden inside those models. %\sg{somewhere in this paragraph needs to state how our prompt is significant discovery compared to many other prompting works. e.g. from task-agnostic perspective, from high-level core knowledge perspective. if you agree with high-level story and define logical dark knowledge or something like that, make a figure like Figure 1 https://arxiv.org/pdf/1911.01547.pdf}

% In short, our works show that LLMs have a decent zero-shot ability to reason about the tasks, and single instruction elicits such knowledge across diverse reasoning tasks. 

% Experimental results show that \ours facilitates step-by-step reasoning across various reasoning tasks, and outperforms standard zero-shot prompting on many reasoning tasks, e.g., MultiArith (17.7\% $\rightarrow$ 78.7\%), GSM8K (10.4\% $\rightarrow$ 40.7\%). 


% Experimental results demonstrate that our \ours increase the performance to standard zero-shot prompting across diverse benchmark reasoning tasks, including MultiArith (17.7\% $\rightarrow$ 78.7\%), GSM8K (10.4\% $\rightarrow$ 40.7\%), AQUA (22.4\% $\rightarrow$ 33.5\%), SVAMP \yi{add}. 

% However, it was shown that the performance improvements in tasks requiring multi-step reasoning, e.g. system-2 problems, are limited even when the model size is scaled \citep{gopher}.
% \yi{should explain prompting (or in-context learning)}

% To elicit the logical ability from LLMs, \cite{cot_wei, cot_wei_sc} propose \CoT prompting (CoT). 
% % Unlike the standard prompting that 
% \CoT(CoT) \cite{cot_wei, cot_wei_sc}, which mimics step-by-step reasoning process of a person, is an recently emerged approach to solve reasoning tasks. Given a difficult task that requires multi-step reasoning, CoT explicitly makes the task decomposed into multiple easy tasks auto-regressively and solve them step-by-step via text generation process by LLMs. The approach is proven to be effective in LLMs. Previous researches have prompted LLMs to output \CoT reasoning by giving some examples into in-context, i.e., few-shot learning. While proven to be effective, this approach has two drawbacks. First, we need to create or prepare \CoT question-reasoning-answer paired examples for every task in advance. This will be a much more time-consuming task than one might imagine, as it is quite different from the usual few-shot setup which needs only question-answer pairs. This becomes a critical property when applying LLMs to practical setting, where LLMs needs immediate adaptation to infinite variety of tasks. Second, hand-crafted \CoT examples intentionally or unintentionally includes human biases about how to think step-by-step, which may restrict intrinsic ability of \CoT within LLMs.

% In order to overcome the aforementioned shortcomings, we propose a novel approach, \ours, which is a template-based prompting for \CoT reasoning. \ours is categorized as zero-shot learning approach. Therefore, we do not need to prepare \CoT reasoning examples for each task.
% Instead, we just need to  prepare only one fixed template that encourages the model to start step-by-step reasoning.
% Specifically, we add a fixed text "Let's think step by step" just after the question and feed the model as input across all the tasks. We empirically have found that this prompt triggers the model to start \CoT reasoning before directly answering the question. Contrary to few-shot-cot approach, our approach does not need to prepare examples and is task-independent. Furthermore, by releasing the model from human 
% biases, it is expected that we can elicit logical "dark knowledge" \cite{darkknowledge2, darkknowledge} from LLMs.

% We validated the effectiveness of our approach by using several scales of GPT-3 models.
% Experiment results demonstrate that our method improves the performance on several benchmark reasoning tasks including Arithmetics (SingleEq, MultiArith, GSM8K, AQUA-RAT, SVAMP) and logical reasoning (Date Understanding, Object Tracking). Another finding is that our approach is not effective on Commonsense reasoning tasks (CommonsenseQA, StrategyQA). %\kojima{Further ananlysis indicates that \CoT is too flexible to think about human commonsense even its reasoning process makes sense, i.e., multi-step reasoning by LLMs sometimes arrives at counter-intuitive conclusions.}

% In summary, our main contributions are as follows.
% \begin{itemize}
% \item We propose a task-independent prompt for zero-shot chain-of-thought reasoning and Evaluate our method on multiple variety of reasoning tasks and validated the effectiveness with several findings.
% \begin{enumerate}
% \item Our method stably outperforms hte  existing zero-shot method on Arithmetics, and relational Reasoning, but not on commomsense task.(Table \ref{tab:main_results})
% \item Our method is effective on larger model but not on smaller ones, which is aligned with the few-shot-cot results \cite{cot_wei} (table \ref{tab:model_size})
% \item The performance of our method is competitive with few-shot-cot with the example size of between 2 - 8, depending on tasks. (table \ref{tab:few_shot})
% \item The perfomance of Few-shot-Cot significantly deteriorates when we use examples from entirely different tasks, but our method \kojima{does not care about it}. (table \ref{tab:robustness_against_examples})
% \item  Changing prompt texts significantly affects the perfomance, but still outperforms normal zero-shot setting. (table \ref{tab:template_study})
% \end{enumerate}
% \end{itemize}

%few-shotで作成するexampleには思考の連鎖を含める必要がある.
%①手間
%②人間のバイアス


\section{Background}   
% \sg{goal of backrgound is not related work coverage. its to clarify all terminologies/minimal prior works so readers can understand method sections}
We briefly review the two core preliminary concepts that form the basis of this work: the advent of large language models (LLMs) and prompting, and \CoT (CoT) prompting for multi-step reasoning.

\paragraph{Large language models and prompting}
A language model (LM), is a model that looks to estimate the probability distribution over text. Recently, scaling improvements through larger model sizes (from a few million~\citep{merity2016pointer} to hundreds of millions~\citep{bert} to hundreds of billions~\citep{brown2020language} parameters) and larger data (e.g. webtext corpora \citep{gao2020pile}) have enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream NLP tasks. Besides the classic ``pre-train and fine-tune'' paradigm~\citep{liu2021pre}, models scaled to 100B+ parameters exhibit properties conducive to few-shot learning~\citep{brown2020language}, by way of in context learning, where one can use a text or template known as a \emph{prompt} to strongly guide the generation to output answers for desired tasks, thus beginning an era of ``pre-train and prompt''~\citep{liu2021makes}. In work, we call such prompts with explicit conditioning on few task examples as \textit{few-shot} prompts, and other template-only prompts as \textit{zero-shot} prompts. 

\paragraph{Chain of thought prompting}
Multi-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models~\citep{gopher}. Chain of thought (CoT) prompting~\citep{cot_wei}, an instance of few-shot prompting, proposed a simple solution by modifying the answers in few-shot examples to step-by-step answers, and achieved significant boosts in performance across these difficult benchmarks, especially when combined with very large language models like PaLM~\citep{palm}. The top row of \autoref{fig_overview_1} shows standard few-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a given for tackling such difficult tasks, and the zero-shot baseline performances were not even reported in the original work~\citep{cot_wei}. To differentiate it from our method, we call \citet{cot_wei} as \textit{\theirs} in this work.   

% \paragraph{Prompting methods and in-context learning with Large Language Models}
% Given the prevalence of the effective few shot capabilities of large langauge models, there has been a flurry of research looking at more effective prompting methods \citep{min2021noisy,khashabi2021prompt,cot_wei}. 

% Pertinent to this line of work is the prompting method known as \textit{chain-of-thought} \citep{cot_wei}. Chain-of-thought constructs prompts for the few-shot in-context learning paradigm in which the prompts not only contain setup/answer pairs, but also an added explanation (i.e. a \emph{chain-of-thought}) describing a natural thought process that would lead one to arrive at the correct answer. Using this prompting methodology, \citet{cot_wei} were able to \mr{Takeshi: can you explain how CoT improves over instruction based prompting?}.



\section{Zero-shot Chain of Thought}
\label{sec:proposal}

% \sg{moved from introduction. insert somewhere in this section}
% Fig. \ref{fig_overview_1} shows conceptual differences between prior chain-of-though prompting and our chain-of-though instruction, as well as text generated by GPT-3 (\texttt{davinci} v2) \cite{gpt3,instructgpt} with each prompting strategy. 
% Unlike \cite{cot_wei, cot_wei_sc}, our method does not require reasoning examples to elicit logical reasoning knowledge from LLMs. 

%We propose a novel 
%\sg{in terms of wording, i recommend using "simple", "under-emphasized", "understudied" etc over "novel" throughout the texts. if we emphasize novelty too much, that may gather criticisms. } \yi{kojima-san, please read the shane's comment and select appropriate oen}
We propose \ours, a zero-shot template-based prompting for \CoT reasoning. 
%\ours is categorized as zero-shot learning approach, although \ours differs from the previous zero-shot learning in that it elicits step-by-step reasoning process before reaching the final answer. 
It differs from the original \CoT prompting~\citep{cot_wei} as it does not require step-by-step few-shot examples, and it differs from most of the prior template prompting~\citep{liu2021pre} as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks with a single template. The core idea of our method is simple, as described in~\autoref{fig_overview_1}: add \textit{Let's think step by step}, or a a similar text (see~\autoref{tab:template_study}), to extract step-by-step reasoning.

%An overview of our proposal is described at Figure \ref{fig_overview_2}

\subsection{Two-stage prompting} 

While \ours is conceptually simple, it uses prompting twice to extract both reasoning and answer, as explained in~\autoref{fig_overview_2}. In contrast, the zero-shot baseline (see the bottom-left in~\autoref{fig_overview_1}) already uses prompting in the form of ``The answer is'', to extract the answers in correct formats. Few-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly designing the few-shot example answers to end in such formats (see the top-right and top-left in~\autoref{fig_overview_1}). In summary, \theirs~\citep{cot_wei} requires careful human engineering of a few prompt examples with specific answer formats per task, while \ours requires less engineering but requires prompting LLMs twice.

\paragraph{1st prompt: reasoning extraction} In this step we first modify the input question $\mathbf{x}$ into a \textit{prompt} $\mathbf{x}^{\prime}$ using a simple template ``Q: \texttt{[X]}. A: \texttt{[T]}'', where \texttt{[X]} is an input slot for $\mathbf{x}$ and \texttt{[T]} is an slot for hand-crafted trigger sentence $\mathbf{t}$ that would extract chain of though to answer the question $\mathbf{x}$. 
For example, if we use ``Let's think step by step'' as a trigger sentence, the prompt $\mathbf{x}^{\prime}$ would be ``Q: \texttt{[X]}. A: Let's think step by step.''. 
See~\autoref{tab:template_study} for more trigger examples.
Prompted text $\mathbf{x}^{\prime}$ is then fed into a language model and generate subsequent sentence $\mathbf{z}$. 
We can use any decoding strategy, but we used greedy decoding throughout the paper for the simplicity. 
% While it is not clear whether the language model would generates reasonable chain of thought given such a simple trigger, we empirically show that it can consistently generate a reasonable reasoning path across diverse types of questions. \sg{commented out optional sentences}

% First, ... \sg{iwasawa-san or machel, could you write the rest more concisely and fix minor grammars? thanks!} we setup a template-based prompting for encouraging the model to start \CoT reasoning, such as "Let's think step by step." before making the model answer the given question. 
% Let $x \in X$ be a question from a certain task. The template including this prompt is defined as $x' = \tau_1(x)$. The model is fed the template $x'$ as input and generate step-by-step reasoning text $z$ as output. 

\paragraph{2nd prompt: answer extraction} 
In the second step, we use generated sentence $\mathbf{z}$ along with prompted sentence $\mathbf{x}^{\prime}$ to extract the final answer from the language model. 
To be concrete, we simply concatenate three elements as with ``\texttt{[X$^{\prime}$]} \texttt{[Z]} \texttt{[A]}'': \texttt{[X$^{\prime}$]} for 1st prompt $\mathbf{x}^{\prime}$, \texttt{[Z]} for sentence $\mathbf{z}$ generated at the first step, and \texttt{[A]} for a trigger sentence to extract answer. 
The prompt for this step is \textit{self-augmented}, since the prompt contains the sentence $\mathbf{z}$ generated by the same language model. In experiment, we use slightly different answer trigger depending on the answer format. 
For example, we use ``Therefore, among A through E, the answer is'' for multi-choice QA, and ``Therefore, the answer (arabic numerals) is'' for math problem requiring numerical answer. 
See Appendix \ref{appx:answer_prompts} for the lists of answer trigger sentences.
Finally, the language model is fed the prompted text as input to generate sentences $\mathbf{\hat{y}}$ and parse the final answer.
See ``Answer Cleansing'' at \S \ref{sec:experiment} for the parser details.

% The second stage is finding the answer from the reasoning text $z$. 
% This procedure is not trivial in the case of zero-shot \CoT learning because the model does not promise to output the final answer at the very end of \CoT reasoning. In contrast, Few-Shot-Cot can explicitly identify where the answer exists because it mimics the examples to output the format "The answer is" after reasonings.
% Therefore, we create a second template $z' = \tau_2(x', z)$ that encourages the model to directly produce the final answer based on the question $x'$ and reasoning output $z$. 
% This template depends on the answer format. 
% If the task requires us to choose one from multiple answer choice, the prompt in the template should be like "Therefore, among A through E, the answer is".
% If answering arithmetic task, the prompt should be like "Therefore, the answer (arabic numerals) is". See Appendix \ref{appx:answer_prompts} for the details.
% The model is fed the template $z'$ as input and generate the answer $\hat{y}$ as output.

%The procedure is summarized in Algorithm \ref{alg:algorithm}.

\begin{figure}[t]
  \begin{center}
   \includegraphics[width=0.99\columnwidth]{fig_overview_2}
  \end{center}
  \caption{Full pipeline of \ours as described in \mysection ~\ref{sec:proposal}: we first use the first ``reasoning'' prompt  to extract a full reasoning path from a language model, and then use the second ``answer'' prompt to extract the answer in the correct format from the reasoning text.}
  \label{fig_overview_2}
\end{figure}

%\input{table_algorithm}

\section{Experiment}
\label{sec:experiment}

% \sg{one way people can make experiment section more "scientific" is to begin with a list of research questions to answer } \mr{+1}

% %In order to validate the effectiveness of our proposal,
% In order to validate that the LLMs are strong zero-shot reasoners, we conducted a series of experiments to compare the proposed \ours method with existing approaches including \theirsz, \theirsf, \theirs on a wide range of tasks.
% We find that \ours stably improves the accuracy across a variety of tasks which requires multi-step reasoning.

% \subsection{Experimental Setup}

\paragraph{Tasks and datasets}

We evaluate our proposal on 12 datasets from four categories of reasoning tasks: arithmetic, commonsense, symbolic, and other logical reasoning tasks. 
See Appendix \ref{appx:dataset_description} for the detailed description of each datasets. 

For arithmetic reasoning, we consider the following six datasets: (1) SingleEq \citep{singleeq}, (2) AddSub \citep{addsub}, (3) MultiArith \citep{multiarith}, (4) AQUA-RAT \citep{aqua}, (5) GSM8K \citep{gsm8k}, and (6) SVAMP \citep{svamp}. 
The first three are from the classic Math World Problem Repository \citep{mawps}, and the last three are from more recent benchmarks. 
SingleEq and AddSub contain easier problems, which do not require multi-step calculation to solve the tasks. 
MultiArith, AQUA-RAT, GSM8k, and SVAMP are more challenging datasets that require multi-step reasoning to solve.
% Also See Table \autoref{tab:example_table_dataset1}, \autoref{tab:example_table_dataset2}, and \autoref{tab:example_table_dataset3} for some examples from each datasets.


For commonsense reasoning, we use CommonsenseQA \citep{commonsenseqa} and StrategyQA \citep{strategyqa}.
CommonsenseQA asks questions with complex semantics that often require reasoning based on prior knowledge \citep{commonsenseqa}. 
StrategyQA requires models to infer an implicit multi-hop reasoning to answer questions \citep{strategyqa}. 

For symbolic reasoning, we use Last Letter Concatenation and Coin Flip \citep{cot_wei}. Last letter Concatenation asks the model to concatenate the last letters of each word. We used randomly selected four names for each sample. Coin Flip asks the model to answer whether a coin is still heads up after people either flip or do not flip the coin.
We created samples of four times flip or not flip trials. 
Although these tasks are easy for humans, LMs typically exhibit a flat scaling curve. 

For other logical reasoning tasks, we choose two evaluation sets from the BIG-bench effort \citep{bigbench}: Date Understanding \footnote{While prior work \citep{cot_wei} categorized Date Understanding task into Common Sense reasoning, our study categorized this task into logical reasoning because this task requires less prior knowledge and more logical reasoning between dates.} and Tracking Shuffled Objects. 
Date Understanding asks models to infer the date from a context. 
Tracking Shuffled Objects tests a model's ability to infer the final state of objects given its initial state and a sequence of object shuffling. We used a dataset of tracking three shuffled objects for our experiment.

\paragraph{Models}
%\mr{please put in a footnote which month these experiments were ran (for version control issues)} 
We experiment with 17 models in total. Main experiments are conducted with Instruct-GPT3 \citep{instructgpt} (text-ada/babbage/curie/davinci-001 and text-davinci-002)\footnote{Our experiment for Instruct GPT-3 models includes both text-****-001 and text-davinci-002. Text-davinci-002 differs from text-****-001 in that they use different fine-tuning data depending on the date range collected from the APIs. Specifically, text-davinci-002 uses data up to Jun 2021, while text-****-001 uses data up to Oct 2019. (See \url{https://beta.openai.com/docs/engines/gpt-3})}, original GPT3 \citep{brown2020language} (ada, babbage, curie, and davinci)\footnote{Our experiments with GPT3 series are conducted by using OpenAI API between April-2022 and May-2022, except for No.10-16 in \autoref{tab:template_study} in Aug-2022.}, and PaLM \citep{palm} (8B, 62B, and 540B). 
In addition, we used GPT-2\citep{Radford2019LanguageMA}, GPT-Neo\citep{gpt-neo}, GPT-J\citep{gpt-j}, T0 \citep{sanh2022multitask}, and OPT \citep{zhang2022opt} for model scaling study. 
The size of LMs ranges from 0.3B to 540B. 
We include both standard (e.g. GPT-3 and OPT), and instruction following variants (e.g. Instruct-GPT3 and T0). 
% We run experiments on various scales of language models. 
%Specifically, we used the following four models from original GPT3 \cite{brown2020language}: ada, babbage, curie, and davinci.
% we also used the following four models from Instruct-GPT3 \cite{instructgpt}: Text-ada-001 (2.7B), Text-babbage-001 (6.7B), Text-curie-001 (13B), and Text-davinci-002 (175B). . 
% These models are optimized to follow the input instructions more faithfully than the original GPT-3 models \citep{brown2020language}
% In addition, we experiment with language models of various model size, including GPT-2, GPT-Neo, GPT-J, T0, and OPT. 
See Appendix \ref{appx:model_description} for model description details.
%Without mentioning otherwise, 
Unless otherwise stated, we use text-davinci-002 throughout the experiments.

\paragraph{Baselines}
We compare our \ours mainly to standard \theirsz prompting to verify the effectiveness of its \CoT reasoning. For \theirsz experiments, similar answer prompts as \ours are used as default. See Appendix \ref{appx:answer_prompts} for detail.
To better evaluate the zero-shot ability of LLMs on reasoning tasks, we also compare our method to \theirsf and \theirs baselines from~\citep{cot_wei}, using the same in-context examples. 
%For \theirsf prompting, we followed the same setting and used the same in-context examples as \citep{cot_wei}.
%\kojima{Due to budget constraints,} few-shot evaluation results are cited from prior works, although the model size is slightly different from out settings.
Throughout the experiments, we use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore deterministic. 
For the few-shot approaches, since the order of in-context examples could affect the results \citep{lu2021fantastically}, we run each experiment only once with a fixed seed across all methods and datasets, for fair comparisons with the zero-shot methods. 
\citet{cot_wei} showed that the order of examples did not cause large variance in CoT experiments. 

% As for few-shot based approaches, the order of in-context examples usually affects the performance \citep{fantastic}. 
% However, \citep{cot_wei} insisted that the order did not show large variance among different seeds in the few-shot CoT case.
% Therefore, our experiments run only one time with fixed seed across all the methods and datasets. 
% Evaluation metric is accuracy across all the experiments.

\paragraph{Answer cleansing} After the model outputs a text by answer extraction (see \mysection \ref{sec:proposal} and~\autoref{fig_overview_2}), our method picks up only the part of the answer text that first satisfies the answer format. 
For example, if the answer prompting outputs ``probably 375 and 376'' on arithmetic tasks, we extract the first number ``375'' and set it as the model prediction. 
In the case of multiple-choice, the first large letter we encounter is set as the prediction.
See Appendix \ref{appx:answer_cleansing} for more detail.
Standard \theirsz method follows the same idea. 
For \theirsf and \theirs methods, we follow~\citep{cot_wei_sc} and first extract the answer text after "The answer is " from the model output, and apply the same answer cleansing to parse the answer text. 
If ``The answer is'' is not found in the model output, we search from the back of the text and set the first text that satisfies the answer format as the prediction.


\subsection{Results}

\input{table_main_results}

\paragraph{\ours vs. \theirsz}

~\autoref{tab:main_results} summarize accuracy of our method (\ours) and standard zero-shot prompting (\theirsz) for each dataset. 
\ours substantially outperforms four out of six arithmetic reasoning tasks (MultiArith, GSM8K, AQUA, SVAMP), all symbolic reasoning, and all other logical reasoning tasks (from BIG-bench~\citep{bigbench}).
For example, \ours achieves score gains from 17.7\% to 78.7\% on MultiArith and from 10.4\% to 40.7\% on GSM8K. 
Our method gives on-par performances for the remaining two arithmetic reasoning tasks (SingleEq and AddSub), which is expected since they do not require multi-step reasoning. 

In commonsense reasoning tasks, \ours does not provide performance gains. 
It is expected as \citet{cot_wei} also reports that even \theirs does not provide performance gains on Lambda (135B), but does improve StrategyQA when combined with substantially larger PaLM (540B) model, which may also apply for ours. 
More importantly, we observe that many generated chain of thought themselves are surprisingly \textit{logically} correct or only contains human-understandable mistakes (See \autoref{tab:example_main}), suggesting that \ours does elicit for better commonsense reasoning even when the task metrics do not directly reflect it. 
We provide samples generated by \ours for each dataset in \autoref{appx:further_experiment}. 


% \begin{itemize}
% \item{\textbf{Arithmetic Reasoning}}
% The results show that our method stably outperforms the baseline on MultiArith, GSM8K, AQUA-RAT, and SVAMP datasets, which requires multi-step reasoning (calculation). Especially, on MultiArith dataset, our method drastically improves the accuracy from 17.5 \% to 78.7 \%. 
% In addition, on GSM8K dataset, our method improves the performance from 10.4\% to 40.7\%, which is a competitive result to Finetuned GPT-3 models (See Table \ref{tab:few_shot}).
% As for SingleEq and AddSub dataset, which requires only single step reasoning (calculation), our method does not see obvious improvement.
% %\kojima{To investigate the cause of the deterioration, we extracted samples that were correct in baseline and incorrect in our method, after which we manually classified the causes. Of the corresponding XX samples, it is found that XX\% were due to \CoT reasoning error caused by distraction (unnecessary information for arriving at the final answer) in questions. The typical example is described in Table \ref{tab:example_table_dataset1} at AddSub row. The other XX \% had errors related to ... } 
% 
% \item{\textbf{Symbolic Reasoning Tasks}}
% It is also shown that our method significantly outperforms baseline. Especially, as for Last Letter Concatenation (4 words), the baseline is improved from almost 0\% (The model does NOT output 4 letters, but 5 or 6 letters in most of the cases for \theirsz setting) to 57.6 \%. Coin Flip taks also see drastic performance improvement from almost random (53.8\%) to 91.4 \%.
% 
% \item{\textbf{Relational Reasoning Tasks}}
% The results also show that our method outperforms the baseline on both of the new tasks. This indicates that our method is widely expanded to multiple tasks that requires logical step-by-step reasoning. This is a favorable property in practical wild setting because we do not need to manually create few-shot examples for unknown, suddenly arrived tasks unlike \theirs settings.
% 
% \item{\textbf{Commonsense reasoning}}
% The results show that our method does not see performance gain on both datasets. These are opposite results to those of previous studies \cite{cot_wei, cot_wei_sc}, in which \CoT improves the performance of Commonsense reasoning tasks under few-shot-cot setting. 
% %\kojima{To investigate the cause of the deterioration, following the same procedure as described above, we extracted samples that were correct in baseline and incorrect in our method, after which we manually classified the causes. Of the corresponding XX samples, it is found that XX\% were ....}
% \kojima{It is speculated that without any in-context examples, zero-shot based step-by-step reasoning are so flexible as to cause the model to reach counter-intuitive conclusions that are sometimes against commonsense.}
% We further analyze the error pattern of \CoT at later paragraph. 
% %This result is related to a recent reseach \cite{unreliable}}
% 
% \end{itemize}

%\input{table_detail_results}
\input{table_fewshot_results}
\input{table_model_scale_results_graph}
%\input{table_model_scale_results}

\paragraph{Comparison with other baselines}

\autoref{tab:few_shot} compares the performances on two arithmetic reasoning benchmarks (MultiArith and GSM8K) across \ours and baselines. 
%For GSM8K, we list up the performances of Finetuned GPT-3 (175B) and PaLM (540B) with \theirsf and \theirs prompting (bottom block), which are reported in \citet{cot_wei}. 
The large gap between standard prompting (1st block) and chain of thought prompting (2nd block) suggests that these tasks are difficult without eliciting multi-step reasoning. 
Major improvements are confirmed on both Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). 
While \ours naturally underperforms \theirs, it substantially outperforms standard \theirsf prompting with even 8 examples per task. 
For GSM8K, \ours with Instruct GPT-3 (text-davinci-002) also outperforms finetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B), reported in \citet{cot_wei} (3rd and 4th block). 
%Further experiment results with PaLM are found at \autoref{appx:further_experiment_on_palm}.
See App. \ref{appx:further_experiment_on_palm} for more experiment results with PaLM.

\paragraph{Does model size matter for zero-shot reasoning?}

%\autoref{tab:model_size} 
\autoref{fig:model_size} compares performance of various language models on MultiArith / GSM8K. 
Without \CoT reasoning, the performance does not increase or increases slowly as the model scale is increased, i.e., the curve is mostly flat. 
In contrast, the performance drastically increases with \CoT reasoning, as the model size gets bigger, for Original/Instruct GPT-3 and PaLM. %\footnote{The experiment results demonstrate that \theirs performance is better than \ours on davinci and text-davinci-002, while worse than \ours on text-davinci-001. Sample-base analysis for the text-davinci-001 result is summarized at \autoref{appx:text-davinci-001}}.
When the model size is smaller, \CoT reasoning is not effective. 
This result aligns with the few-shot experiment results in \cite{cot_wei}.
Appendix \ref{appx:detail_model_scale} shows extensive experiment results using wider variety of language models, including GPT-2, GPT-Neo, GPT-J, T0, and OPT.
We also manually investigated the quality of generated \CoT, and large-scale models clearly demonstrate better reasoning (See Appendix \ref{appx:further_experiment} for the sampled outputs for each model).

\paragraph{Error Analysis}

\input{example_within_main}

To better understand the behavior of \ours, we manually investigated randomly selected examples generated by Instruct-GPT3 with \ours prompting. 
See~\autoref{appx:error_analysis} for examples, where some of the observations include: (1) In commonsense reasoning (CommonsenseQA), \ours often produces flexible and reasonable \CoT even when the final prediction is not correct. 
\ours often output multiple answer choices when the model find it is difficult to narrow it down to one (see~\autoref{tab:example_main} for examples). (2) In arithmetic reasoning (MultiArith), \ours and \theirs show substantial differences regarding the error patterns.
First, \ours tends to output unnecessary steps of reasoning after getting the correct prediction, which results in changing the prediction to incorrect one. 
\ours also sometimes does not start reasoning, just rephrasing the input question. 
In contrast, \theirs tend to fail when generated \CoT include ternary operation, e.g. $(3+2)*4$. 

% Finally, it is sometimes forced to stop \CoT due to the text length limit by producing over-length-reasoning. (See ~\autoref{appx:error_analysis} for some examples.) 
% \yi{need more epxlanation}

% \begin{itemize}
% \item{(CommonsenseQA) \ours can often produce flexible and reasonable \CoT even when the prediction is not correct. \ours often output multiple answer choices when the model find it difficult to narrow it down to one. (See Table \ref{tab:example_commonsenseqa} in Appendix \ref{appx:error_analysis} for some examples.)}
% 
% \item{(MultiArith) \ours tends to produce some patterns of \CoT errors more often than \theirs: (1) It tends to output unnecessary steps of reasoning after getting the correct prediction, which results in changing the prediction to incorrect one. (2) It sometimes does not start reasoning, just rephrasing the input question. (3) It is sometimes forced to stop \CoT due to the text length limit by producing over-length-reasoning. (See Table \ref{tab:example_multiarith_zsc} in Appendix \ref{appx:error_analysis} for some examples.)}
% 
% \item{(MultiArith) The frequency of calculator error and commonsense mistake made by \ours is fewer than those by \theirs. (See Table \ref{tab:example_multiarith_comparison} in Appendix \ref{appx:error_analysis} for some examples.) \kojima{Perhaps it depends on datasets.}}
% \end{itemize}

\input{table_template_results}
\input{table_robustness_against_examples}

\paragraph{How does prompt selection affect \ours?}
We validate the robustness of \ours against input prompts. \autoref{tab:template_study} summarizes performance using 16 different templates with three categories. Specifically, following \citet{websonpavlick2022prompt}, the categories include instructive (encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong way), and irrelevant (nothing to do with reasoning). The results indicate that the performance is improved if the text is written in a way that encourages \CoT reasoning, i.e., the templates are within "instructive" category. However, the difference in accuracy is significant depending on the sentence. In this experiment, "Let’s think step by step." achieves the best results. Interestingly, it is found that different templates encourage the model to express reasoning quite differently (see \autoref{appx:further_experiment} for sample outputs by each template). In contrast, when we use misleading or irrelevant templates, the performance does not improve. It remains an open question how to automatically create better templates for \ours.


\paragraph{How does prompt selection affect \theirs?}
~\autoref{tab:robustness_against_examples} shows the performance of \theirs when using examples from different datasets: CommonsenseQA to AQUA-RAT and CommonsenseQA to MultiArith. 
The domains are different in both cases, but the answer format is the same in the former. 
Surprisingly, the \CoT examples from different domains (common sense to arithmetic) but with the same answer (multiple-choice) format provide substantial performance gain over \theirsz (to AQUA-RAT), measured relative to the possible improvements from \ours or \theirs. In contrast, the performance gain becomes much less when using examples with different answer types (to MultiArith), confirming prior work \citep{min2022rethinking} that suggests LLMs mostly leverage the few-shot examples to infer the repeated format rather than the task itself in-context. 
Nevertheless, for both cases the results are worse than \ours, affirming the importance of task-specific sample engineering in \theirs.

% % In contrast, 
% In addition, we have experimented using in-context examples from entirely different tasks (i.e., CommonsenseQA examplars are used for MultiArith.). The results on~\autoref{tab:robustness_against_examples} shows that the the performance of \theirs significantly degrades below \ours when in-context examples come from entirely different distribution, while \ours are quite robust across all the tasks because we do not need to prepare any examples.

%------------------------------------------------------------

\input{section_related_work}

\section{Conclusion}
We have proposed \ours,  a single zero-shot prompt that elicits \CoT from large language models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach in previous work that requires hand-crafting few-shot examples per task. 
%This zero-shot reformulation allows us to reap the benefits of CoT prompting without needing task/example specific few shot prompts designed specially to elicit chain-of-thought. 
%Empirical results show that our single fixed prompt facilitates effective multi-step reasoning across a variety of tasks, and substantially increases the zero-shot reasoning ability of LLMs without any hand-crafted few-shot examples. 
Our simple method not only is the minimalist and strongest zero-shot baseline for difficult multi-step \textit{system-2} reasoning tasks that long evaded the scaling laws of LLMs, but also encourages the community to further discover similar \textit{multi-task} prompts that elicit broad cognitive abilities instead of narrow task-specific skills.
%like logical reasoning 
%Expanding the settings in which various types of reasoning can be extracted from LLMs in a zero-shot setting will hopefully increase the applicability and understanding of the capabilities of these models. %\mr{forward looking statement}
%\mr{brief summary}
%
%This study shows that the existing in-context based \CoT approach is reconstructed as zero-shot one by inserting prompt based template. 
%Experiment results demonstrate that our method improves the performance on several benchmark including Arithmetics, symbolic, and other logical reasoning tasks.
%The limitation is that zero-shot based \CoT method is not effective on some of the reasoning tasks, such as Commonsense reasoning. Future work includes improving our approach to achieve better performance on these tasks.
%We hope that this study sheds more light on the importance and availability of \CoT reasoning approach.

% Gopherの知見
% Some language tasks on Paraphrasing, Summarization, or Negation also appear to be hard regardless of the models’ scale.

\section*{Acknowledgements}
This work has been supported by the Mohammed bin Salman Center for Future Science and Technology for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030).
Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used for experiments other than PaLM. We also thank Jason Wei and Denny Zhou for discussions and support on running PaLM experiments, and Sharan Narang and Aakanksha Chowdhery for generic support on PaLM infrastructures.

% https://github.com/kzhai/Papers/blob/master/style/nips.bst
\bibliographystyle{plainnat}
\bibliography{neurips_2022}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
        \item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{Our paper mainly used GPT-3 API with greedy decoding, and there are no randomness for the experiments. }
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerYes{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerYes{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerYes{}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix

\section{Details of Experimental Setup}
\label{appx:experiment_setup}

\subsection{Code}

Code is available at
%\kojima{\url{TBD}}.
\url{https://github.com/kojima-takeshi188/zero_shot_cot}.
%\url{https://anonymous.4open.science/r/zero_shot_cot-1B2D/}

\subsection{Datasets}
\label{appx:dataset_description}

\subsubsection{Dataset Description}

\autoref{tab:dataset_description} summarizes the description of each dataset used in our experiment.
%\kojima{(d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes]}

\input{table_dataset_description}

\subsubsection{Dataset creation}
\label{appx:dataset_creation}

Regarding ``Last Letter Concatenation'' and ``Coin Flip'', datasets are not publicly available so we created the datasets following \cite{cot_wei} with a minor rephrasing of the question template. Specifically, as for Last Letter Concatenation, we use the following template. We randomly select human names from names-dataset library (\url{https://pypi.org/project/names-dataset/}) and insert them into \{Name1\} through \{Name4\}.

\begin{itemize}
    \item 'Take the last letters of each words in "\{Name1\} \{Name2\} \{Name3\} \{Name4\}" and concatenate them.'
\end{itemize}

As for Coin Flip, we use the following template. We randomly select human names from names-dataset library and insert them into \{Name1\} through \{Name4\}. We also randomly pick up ``flips'' or ``does not flip'' and insert the phrase into each \{flips | does not flip\} part, respectively.

\begin{itemize}
    \item 'A coin is heads up. \{Name1\} \{flips | does not flip\} the coin. \{Name2\} \{flips | does not flip\} the coin. \{Name3\} \{flips | does not flip\} the coin. \{Name4\} \{flips | does not flip\} the coin. Is the coin still heads up? Note that "flip" here means "reverse".'
\end{itemize}

%\clearpage
\subsection{Language Models}
\label{appx:model_description}

Our experiment uses multiple language models as described at \autoref{tab:model_description}

\input{table_model_description}


\subsection{Implementation details}
For Original GPT-3 and Instruct-GPT3, we used OpenAI API. 
For OPT, T0, GPT-J, GPT-Neo, and GPT-2, we used Hugging Face Transformer Library \citep{huggingface}.
We set max\_tokens = 128 and used greedy decoding (temperature = 0 in the case of OpenAI API) across all the methods and models except PaLM.
For PaLM, we used `TopK=1' for greedy deterministic decoding and max\_tokens = 256.
``Q:'' is set as a customized stop sequence for all the models except for Instruct-GPT3 to stop the models from repeating questions and answers by themselves.
We run our experiments on cloud V100 instances without GPU for GPT-3 models, on cloud A100x8 GPU(60GB) instances for T0 and OTP, and on cloud A100x1 GPU(60GB) instances for GPT-J, GPT-Neo, and GPT-2. Our implementation is in PyTorch~\citep{paszke2019pytorch}.

%\clearpage
\subsection{Prompts For Answer Extraction}
\label{appx:answer_prompts}

\autoref{tab:answer_prompts_1} and \autoref{tab:answer_prompts_2} summarizes a list of answer extraction prompts used for the experiments at \autoref{tab:main_results}. 
We used \theirsz (left) and \ours (left) as default prompts for answer extraction across all the experiments.

\input{table_answer_prompts}

%\clearpage
\subsection{Answer Cleansing}
\label{appx:answer_cleansing}

\autoref{tab:answer_cleansing} summarizes a list of answer cleansing approaches used across all the experiments.

\input{table_answer_cleansing}

%\clearpage
\section{Additional Experiment Results}
\label{appx:further_experiment}

This section summarizes more example texts generated by models in our experiments.
Note that for readability all texts are modified from the original ones by omitting or inserting some linebreaks. Without mentioning otherwise, we use Instruct-GPT3 (text-davinci-002) model.

\begin{itemize}
    \item \autoref{tab:example_table_dataset} lists example texts generated by \ours for each dataset (See \autoref{tab:main_results}).
    \item \autoref{tab:example_table_templates} lists example texts generated by \ours for each reasoning extraction template (See \autoref{tab:template_study}).
    \item \autoref{tab:example_table_model_size_1} and \autoref{tab:example_table_model_size_2} lists example texts generated by \ours for each langugage model (See \autoref{tab:model_size}).
    \item \autoref{tab:example_table_fewshot} has an example text generated by \theirsf.
    \item \autoref{tab:example_table_fewshot_cot} has an example text generated by \theirs.
    \item \autoref{tab:example_table_fewshot_cot_diff_task} has an example text generated by \theirs with exemplars from a different task (Exemplars from CommonsenseQA, and a task is from MultiArith).
    \item \autoref{tab:example_table_zeroplusfewshot_cot} has an example text generated by Zero-Plus-Few-Shot-CoT.
    \item \autoref{tab:example_table_palm} compares different outcome scenarios on results generated by \theirsz and \ours using PaLM (540B) model.
\end{itemize}

%\subsection{Generated chain of thought with \ours and \theirs}
%\label{appx:more_examples}

\input{example_table_dataset}
%\clearpage
\input{example_table_templates}

\input{example_table_model_size}

\input{example_table_fewshot}
\input{example_table_fewshot_cot}

\clearpage
\input{example_table_palm}

\clearpage
\section{Sample Study}
\label{appx:error_analysis}

To validate the correctness of \CoT, we analyze texts generated by \ours for CommonsenseQA and MultiArith datasets. Instruct-GPT3 (text-davinci-002) model is used for the analysis.

\subsection{CommonsenseQA}

\input{table_analysis_commonsenseqa}

Table \ref{tab:analysis_commonsenseqa} summarizes the categorization results of texts generated by \ours for CommonsenseQA. We randomly picked up 50 samples whose prediction results were correct and 50 samples whose prediction results were incorrect. We categorized those samples by CoT types. Some picked-up samples from each category are found \autoref{tab:example_commonsenseqa}. 

First, it is found that the correct samples contain a certain amount of incorrect \CoT. The main tendency is that \ours cannot narrow down the prediction to one from multiple answer choices, and produce multiple predictions as answers but fortunately the first output answer was correct. See ``Correct - CoT is \textbf{INCORRECT}'' rows in \autoref{tab:example_commonsenseqa}

Second, as for incorrect samples, commonsense mistake is the most frequent error type. By observing the produced \CoT texts, it is found that \ours often produces a flexible and reasonable chain of thought (logically correct but lacks common sense) even when the final prediction is not correct. See ``CommonSense Mistake'' rows in \autoref{tab:example_commonsenseqa}

\input{example_commonsenseqa}

\subsection{MultiArith}

\input{table_analysis_multiarith}

\autoref{tab:analysis_multiarith} summarizes the categorization results of texts generated by \ours and \theirs for MultiArith. We compared \ours and \theirs to contrast the difference of \CoT produced by these two methods. Specifically, we randomly picked up correct 50 samples and incorrect 50 samples produced by each method and categorized them by types. As an exception, the maximum number of incorrect samples from \theirs for MultiArith was 42. 

%Some of the picked up samples from each categories are found at Table \ref{tab:example_multiarith_zsc} and \ref{tab:example_multiarith_fsc}. 

As for correct samples, we examined if the produced \CoT is logical and consistent with the correct prediction. The result shows that almost all the \CoT is correct, with slightly more reasoning mistakes found in Zero-shot-CoT than Few-shot-CoT.
%There are a few samples in which the answer is correct but reasoning is wrong or insufficient in both \ours and \theirs.
%Some examples are found at \autoref{tab:example_multiarith_comparison}.

As for incorrect samples, it is found that \ours tends to output unnecessary steps of reasoning after getting the correct prediction, which results in changing the prediction to incorrect one. 
\ours also sometimes does not start reasoning, just rephrasing the input question.
In contrast, \theirs tends to fail when generated \CoT include ternary operation, e.g. $(3+2)*4$. Another finding is that \ours and \theirs have a certain amount of common sense mistakes to interpret a question. Some examples are found at \autoref{tab:example_multiarith_comparison}.

%\clearpage

%\input{example_multiarith_zero-shot-cot}
%\input{example_multiarith_few-shot-cot}
\input{example_multiarith_comparison_ours_and_theirs}

\clearpage

\section{Further Zero-shot Experiments with PaLM 540B}
\label{appx:further_experiment_on_palm}


We additionally evaluated \ours on PaLM 540B, without and with self-consistency~\citep{cot_wei_sc}. Self-consistency~\citep{cot_wei_sc} generates reasoning paths by random sampling strategy N times and decides the final prediction by majority voting.

% We have conducted further experiments based on PaLM 540B models with two sophisticated method improvements: \ours-v2 and self-consistency.
% Specifically, \ours-v2 (1) samples few questions from training dataset, (2) uses \ours to generate the reasoning paths and answers, (3) insert the Q/A pairs (including the generated reasoning texts) as few-shot exemplars.
% Self-consistency is an existing approach \citep{cot_wei_sc} which executes stochastic decoding multiple times when reasoning and decide the answer by majority voting. 
% However, Our self-consistency uses multiple \ours prompts as in \autoref{tab:template_study}. Therefore, the total number of reasoning paths for our self-consistency is (\# of \ours prompts used) $\times$ (\# of stochastic decoding executed for each prompt).
% See Algorithm \ref{alg:algorithm_palm} for those details.

% The experiment results in \autoref{tab:palm_results} demonstrate that these method improvements drastically boost the performance on a wide variety of tasks including challenging math word problem task: GSM8K. Specifically, \ours-v2 + self consistency improves the accuracy from 12.5\% to 70.5\% on GSM8K, that is 58.0\% performance gain from standard \theirsz method. One more notable point is that this result is much closer to that of existing \theirs + self consistency method 74.4\% \citep{cot_wei_sc}. It indicates that \ours has great potential for further pushing up the baselines of zero-shot based approach by adding some better ideas on top of our methods.

% \input{table_palm_algorithm}
\input{table_palm_results}

%\pagebreak
%\subsection{Additional Error Analysis for text-davinci-001}
%\label{appx:text-davinci-001}

%The results of \autoref{tab:few_shot} demonstrate that \theirs achieves worse performance than \ours in the case of text-davinci-001 on MultiArith task. 
%Sample-based analysis at \autoref{tab:example_multiarith_comparison_davinci001} indicate that \theirs overfits the format of exemplars (such as sentence length or calculation method), which may result in trivial reasoning mistakes, such as calculation error or one-step reasoning missing.

%\input{example_multiarith_comparison_textdavinci001}

% \section{More Examples of Generated Texts by \ours and \theirs}
% \label{appx:more_examples}
% 
% \input{example_table_templates}
% 
% \input{example_table_model_size}
% 
% \input{example_table_dataset}
% 
% \input{example_table_fewshot}
% \input{example_table_fewshot_cot}

\section{Detailed experiment results of model scale study}
\label{appx:detail_model_scale}
This section describes the detailed experiment results of model scale study. The curve within \autoref{fig:model_size} uses the values of \autoref{tab:model_size} and \autoref{tab:model_size_palm}.
\input{table_model_scale_results}
\input{table_model_scale_results_palm}

%\clearpage

\end{document}