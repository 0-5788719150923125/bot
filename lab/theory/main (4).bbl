\begin{thebibliography}{79}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and
  Hajishirzi}]{amini2019mathqa}
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi,
  and Hannaneh Hajishirzi. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1245} {{M}ath{QA}: Towards
  interpretable math word problem solving with operation-based formalisms}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 2357--2367,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Aroca-Ouellette et~al.(2021)Aroca-Ouellette, Paik, Roncone, and
  Kann}]{aroca-ouellette2021prost}
St{\'e}phane Aroca-Ouellette, Cory Paik, Alessandro Roncone, and Katharina
  Kann. 2021.
\newblock \href {https://aclanthology.org/2021.findings-acl.404} {{PROST}:
  {P}hysical reasoning about objects through space and time}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 4597--4608, Online. Association for Computational
  Linguistics.

\bibitem[{Ba et~al.(2016)Ba, Kiros, and Hinton}]{layernorm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton. 2016.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}.

\bibitem[{Ben~Zhou and Roth(2019)}]{zhou2019mctaco}
Qiang~Ning Ben~Zhou, Daniel~Khashabi and Dan Roth. 2019.
\newblock “going on a vacation” takes longer than “going for a walk”: A
  study of temporal commonsense understanding.
\newblock In \emph{EMNLP}.

\bibitem[{Berant et~al.(2013)Berant, Chou, Frostig, and
  Liang}]{berant2013semantic}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1533--1544.

\bibitem[{Biderman(2021)}]{biderman2021nopos}
[@BlancheMinerva]~Stella Biderman. 2021.
\newblock \href
  {https://mobile.twitter.com/BlancheMinerva/status/1394089508723900422} {You:
  Gee stella, \#eleutherai sure hypes rotary embeddings a lot. are you sure
  that they're that good? me:}.
\newblock Twitter.

\bibitem[{{BigScience Workshop}(2022)}]{bigscience_workshop_2022}
{BigScience Workshop}. 2022.
\newblock \href {https://doi.org/10.57967/hf/0003} {Bloom (revision 4ab0472)}.

\bibitem[{Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi}]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi. 2020.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence}.

\bibitem[{Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman}]{gpt-neo}
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021.
\newblock \href {https://doi.org/10.5281/zenodo.5297715} {{GPT-Neo: Large Scale
  Autoregressive Language Modeling with Mesh-Tensorflow}}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei. 2020.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901.

\bibitem[{Child et~al.(2019)Child, Gray, Radford, and Sutskever}]{sparse}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{URL https://openai.com/blog/sparse-transformers}.

\bibitem[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova}]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova. 2019.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In \emph{NAACL}.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord}]{clark2018arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord. 2018.
\newblock \href {http://arxiv.org/abs/1803.05457} {Think you have solved
  question answering? try arc, the {AI2} reasoning challenge}.
\newblock \emph{CoRR}, abs/1803.05457.

\bibitem[{Conneau et~al.(2018)Conneau, Rinott, Lample, Williams, Bowman,
  Schwenk, and Stoyanov}]{conneau2018xnli}
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel~R.
  Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.
\newblock Xnli: Evaluating cross-lingual sentence representations.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics.

\bibitem[{Dagan et~al.(2005)Dagan, Glickman, and Magnini}]{dagan2005rte}
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine Learning Challenges Workshop}, pages 177--190.
  Springer.

\bibitem[{Dauphin et~al.(2016)Dauphin, Fan, Auli, and
  Grangier}]{dauphin2016glu}
Yann~N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2016.
\newblock \href {http://arxiv.org/abs/1612.08083} {Language modeling with gated
  convolutional networks}.
\newblock \emph{CoRR}, abs/1612.08083.

\bibitem[{Dettmers et~al.(2021)Dettmers, Lewis, Shleifer, and
  Zettlemoyer}]{bitsandbytes}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2021.
\newblock \href {http://arxiv.org/abs/2110.02861} {8-bit optimizers via
  block-wise quantization}.

\bibitem[{Dolan and Brockett(2005)}]{dolan2016mrpc}
William~B Dolan and Chris Brockett. 2005.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}.

\bibitem[{Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and
  Hon}]{dong2019unified}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon. 2019.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 32.

\bibitem[{Ganguli et~al.(2022)Ganguli, Hernandez, Lovitt, DasSarma, Henighan,
  Jones, Joseph, Kernion, Mann, Askell et~al.}]{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy
  Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et~al.
  2022.
\newblock Predictability and surprise in large generative models.
\newblock \emph{arXiv preprint arXiv:2202.07785}.

\bibitem[{Gao(2021)}]{gaosize}
Leo Gao. 2021.
\newblock \href {https://blog.eleuther.ai/gpt3-model-sizes/} {On the sizes of
  openai api models}.

\bibitem[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy}]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy. 2020.
\newblock \href {https://arxiv.org/abs/2101.00027} {{The Pile:} an {800GB}
  dataset of diverse text for language modeling}.
\newblock \emph{arXiv preprint arXiv:2101.00027}.

\bibitem[{Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou}]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou. 2021.
\newblock \href {https://doi.org/10.5281/zenodo.5371628} {A framework for
  few-shot language model evaluation}.

\bibitem[{Gordon et~al.(2012)Gordon, Kozareva, and Roemmele}]{gordon2012copa}
Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012.
\newblock \href {https://aclanthology.org/S12-1052} {{S}em{E}val-2012 task 7:
  Choice of plausible alternatives: An evaluation of commonsense causal
  reasoning}.
\newblock In \emph{*{SEM} 2012: The First Joint Conference on Lexical and
  Computational Semantics {--} Volume 1: Proceedings of the main conference and
  the shared task, and Volume 2: Proceedings of the Sixth International
  Workshop on Semantic Evaluation ({S}em{E}val 2012)}, pages 394--398,
  Montr{\'e}al, Canada. Association for Computational Linguistics.

\bibitem[{Hendrycks and Gimpel(2016)}]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel. 2016.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}.

\bibitem[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al. 2022.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}.

\bibitem[{Hu et~al.(2020)Hu, Ruder, Siddhant, Neubig, Firat, and
  Johnson}]{Hu2020XTREMEAM}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
  Melvin Johnson. 2020.
\newblock Xtreme: A massively multilingual multi-task benchmark for evaluating
  cross-lingual generalization.
\newblock \emph{ArXiv}, abs/2003.11080.

\bibitem[{Iyer et~al.(2017)Iyer, Dandekar, and Csernai}]{iyer2019qqp}
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. 2017.
\newblock \href
  {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs} {First
  quora dataset release: Question pairs}.

\bibitem[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.
\newblock Pubmedqa: A dataset for biomedical research question answering.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2567--2577.

\bibitem[{Johannes~Welbl(2017)}]{welbl2017sciq}
Matt~Gardner Johannes~Welbl, Nelson F.~Liu. 2017.
\newblock Crowdsourcing multiple choice science questions.

\bibitem[{Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer}]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer. 2017.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics}, Vancouver, Canada. Association for
  Computational Linguistics.

\bibitem[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei}]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}.

\bibitem[{Khashabi et~al.(2018)Khashabi, Chaturvedi, Roth, Upadhyay, and
  Roth}]{kashabi2018multirc}
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan
  Roth. 2018.
\newblock Looking beyond the surface:a challenge set for reading comprehension
  over multiple sentences.
\newblock In \emph{Proceedings of North American Chapter of the Association for
  Computational Linguistics (NAACL)}.

\bibitem[{Kim et~al.(2021)Kim, Kim, Lee, Lee, Kwak, Jeon, Park, Kim, Kim, Seo,
  Lee, Jeong, Lee, Kim, Ko, Kim, Park, Kim, Kang, Ryu, Yoo, Chang, Suh, In,
  Park, Kim, Kim, Jeong, Yeo, hyun Ham, Park, Lee, Kang, Kang, Ha, Park, and
  Sung}]{Kim2021WhatCC}
Boseop Kim, Hyoungseok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak,
  Dong~Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dong~Hyung Seo,
  Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, SukHyun Ko, Seokhun
  Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang~Min Yoo,
  Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim,
  Jisu Jeong, Yong~Goo Yeo, Dong hyun Ham, Do-Hyoung Park, Min~Young Lee,
  Jaewoo Kang, Inho Kang, Jung-Woo Ha, Woo~Chul Park, and Nako Sung. 2021.
\newblock What changes can large-scale language models bring? intensive study
  on hyperclova: Billions-scale korean generative pretrained transformers.
\newblock \emph{ArXiv}, abs/2109.04650.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}.

\bibitem[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{lai2017large}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock \emph{arXiv preprint arXiv:1704.04683}.

\bibitem[{Lashgar et~al.(2013)Lashgar, Baniasadi, and
  Khonsari}]{Lashgar2013WarpSI}
Ahmad Lashgar, Amirali Baniasadi, and Ahmad Khonsari. 2013.
\newblock Warp size impact in gpus: large or small?
\newblock In \emph{GPGPU@ASPLOS}.

\bibitem[{Lauren{\c{c}}on et~al.(2022)Lauren{\c{c}}on, Saulnier, Wang, Akiki,
  del Moral, Scao, Werra, Mou, Ponferrada, Nguyen, Frohberg, {\v{S}}a{\v{s}}ko,
  Lhoest, McMillan-Major, Dupont, Biderman, Rogers, allal, Toni, Pistilli,
  Nguyen, Nikpoor, Masoud, Colombo, de~la Rosa, Villegas, Thrush, Longpre,
  Nagel, Weber, Mu{\~n}oz, Zhu, Strien, Alyafeai, Almubarak, Chien,
  Gonzalez-Dios, Soroa, Lo, Dey, Suarez, Gokaslan, Bose, Adelani, Phan, Tran,
  Yu, Pai, Chim, Lepercq, Ilic, Mitchell, Luccioni, and Jernite}]{roots}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
  Albert~Villanova del Moral, Teven~Le Scao, Leandro~Von Werra, Chenghao Mou,
  Eduardo~Gonz{\'a}lez Ponferrada, Huu Nguyen, J{\"o}rg Frohberg, Mario
  {\v{S}}a{\v{s}}ko, Quentin Lhoest, Angelina McMillan-Major, G{\'e}rard
  Dupont, Stella Biderman, Anna Rogers, Loubna~Ben allal, Francesco~De Toni,
  Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre
  Colombo, Javier de~la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre,
  Sebastian Nagel, Leon Weber, Manuel~Romero Mu{\~n}oz, Jian Zhu, Daniel~Van
  Strien, Zaid Alyafeai, Khalid Almubarak, Vu~Minh Chien, Itziar Gonzalez-Dios,
  Aitor Soroa, Kyle Lo, Manan Dey, Pedro~Ortiz Suarez, Aaron Gokaslan, Shamik
  Bose, David~Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny
  Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and
  Yacine Jernite. 2022.
\newblock \href {https://openreview.net/forum?id=UoEw6KigkUn} {The bigscience
  {ROOTS} corpus: A 1.6{TB} composite multilingual dataset}.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}.

\bibitem[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}.

\bibitem[{Levesque et~al.(2012)Levesque, Davis, and
  Morgenstern}]{levesque2012winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012.
\newblock The winograd schema challenge.
\newblock In \emph{Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}.

\bibitem[{Levine et~al.(2020)Levine, Wies, Sharir, Bata, and
  Shashua}]{levine2020limits}
Yoav Levine, Noam Wies, Or~Sharir, Hofit Bata, and Amnon Shashua. 2020.
\newblock Limits to depth efficiencies of self-attention.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:22640--22651.

\bibitem[{Lieber et~al.(2021)Lieber, Sharir, Lenz, and Shoham}]{J1WhitePaper}
Opher Lieber, Or~Sharir, Barak Lenz, and Yoav Shoham. 2021.
\newblock Jurassic-1: Technical details and evaluation.
\newblock Technical report, AI21 Labs.

\bibitem[{Lin et~al.(2021)Lin, Mihaylov, Artetxe, Wang, Chen, Simig, Ott,
  Goyal, Bhosale, Du, Pasunuru, Shleifer, Koura, Chaudhary, O'Horo, Wang,
  Zettlemoyer, Kozareva, Diab, Stoyanov, and Li}]{XGLM}
Xi~Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
  Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
  Pasunuru, Sam Shleifer, Punit~Singh Koura, Vishrav Chaudhary, Brian O'Horo,
  Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Ves Stoyanov, and
  Xian Li. 2021.
\newblock Few-shot learning with multilingual language models.
\newblock \emph{ArXiv}, abs/2112.10668.

\bibitem[{Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and
  Zhang}]{liu2020logiqa}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.
  2020.
\newblock \href {http://arxiv.org/abs/2007.08124} {Logiqa: {A} challenge
  dataset for machine reading comprehension with logical reasoning}.
\newblock \emph{CoRR}, abs/2007.08124.

\bibitem[{Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer}]{liu2018generating}
Peter~J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer. 2018.
\newblock Generating wikipedia by summarizing long sequences.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal}]{mihaylov2press2021train018openbookqa}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In \emph{EMNLP}.

\bibitem[{Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary,
  Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro
  et~al.}]{narayanan2021efficient}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, et~al. 2021.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--15.

\bibitem[{{Ortiz Su{\'a}rez} et~al.(2019){Ortiz Su{\'a}rez}, Sagot, and
  Romary}]{ortiz2019oscar}
Pedro~Javier {Ortiz Su{\'a}rez}, Beno{\^i}t Sagot, and Laurent Romary. 2019.
\newblock \href {https://doi.org/10.14618/ids-pub-9021} {Asynchronous pipelines
  for processing huge corpora on medium to low resource infrastructures}.
\newblock Proceedings of the Workshop on Challenges in the Management of Large
  Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 -- 16, Mannheim.
  Leibniz-Institut f{\"u}r Deutsche Sprache.

\bibitem[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez. 2016.
\newblock \href {https://doi.org/10.18653/v1/P16-1144} {The {LAMBADA} dataset:
  Word prediction requiring a broad discourse context}.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1525--1534,
  Berlin, Germany. Association for Computational Linguistics.

\bibitem[{Pilehvar and os{'{e} } Camacho{-}Collados(2018)}]{pilehavar2018wic}
Mohammad~Taher Pilehvar and os{'{e} } Camacho{-}Collados. 2018.
\newblock \href {http://arxiv.org/abs/1808.09121} {Wic: 10, 000 example pairs
  for evaluating context-sensitive representations}.
\newblock \emph{CoRR}, abs/1808.09121.

\bibitem[{Press et~al.(2022)Press, Smith, and Lewis}]{press2021alibi}
Ofir Press, Noah Smith, and Mike Lewis. 2022.
\newblock \href {https://openreview.net/forum?id=R8sQPpGCv0} {Train short, test
  long: Attention with linear biases enables input length extrapolation}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Press and Wolf(2017)}]{tying}
Ofir Press and Lior Wolf. 2017.
\newblock \href {https://aclanthology.org/E17-2025} {Using the output embedding
  to improve language models}.
\newblock In \emph{Proceedings of the 15th Conference of the {E}uropean Chapter
  of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages 157--163, Valencia, Spain. Association for Computational Linguistics.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}.

\bibitem[{Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2019.
\newblock \href {http://arxiv.org/abs/1910.10683} {Exploring the limits of
  transfer learning with a unified text-to-text transformer}.
\newblock \emph{CoRR}, abs/1910.10683.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}.

\bibitem[{Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le}]{ramachandran2017searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le. 2017.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}.

\bibitem[{Rosset(2020)}]{rossettnlg}
Corby Rosset. 2020.
\newblock \href
  {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}
  {Turing-nlg: A 17-billion-parameter language model by microsoft}.

\bibitem[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and
  Choi}]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{arXiv preprint arXiv:1907.10641}.

\bibitem[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Scao, Raja, Dey, BARI, Xu, Thakker, Sharma, Szczechla,
  Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'e}vry, Fries, Teehan,
  Biderman, Gao, Bers, Wolf, and Rush}]{Sanh2021MultitaskPT}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang~A. Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja,
  Manan Dey, M~SAIFUL BARI, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma,
  Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti
  Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng
  Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala
  Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'e}vry,
  Jason~Alan Fries, Ryan Teehan, Stella~Rose Biderman, Leo Gao, T.~G.~Owe Bers,
  Thomas Wolf, and Alexander~M. Rush. 2021.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{ArXiv}, abs/2110.08207.

\bibitem[{Shazeer(2020)}]{shazeer2020swiglu}
Noam Shazeer. 2020.
\newblock \href {http://arxiv.org/abs/2002.05202} {Glu variants improve
  transformer}.

\bibitem[{Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti et~al.}]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, et~al. 2022.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts}]{socher2013sst}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew Ng, and Christopher Potts. 2013.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642.

\bibitem[{Su et~al.(2021)Su, Lu, Pan, Wen, and Liu}]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu. 2021.
\newblock \href {https://arxiv.org/abs/2104.09864} {Roformer: Enhanced
  transformer with rotary position embedding}.
\newblock \emph{arXiv preprint arXiv:2104.09864}.

\bibitem[{Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler}]{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}.

\bibitem[{Tay et~al.(2021)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang,
  Yogatama, Vaswani, and Metzler}]{Tay2021ScaleEI}
Yi~Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung~Won
  Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.
  2021.
\newblock Scale efficiently: Insights from pre-training and fine-tuning
  transformers.
\newblock \emph{ArXiv}, abs/2109.10686.

\bibitem[{Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du et~al.}]{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al. 2022.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008.

\bibitem[{Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman. 2019.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In the Proceedings of ICLR.

\bibitem[{Wang et~al.(2022)Wang, Roberts, Hesslow, Scao, Chung, Beltagy,
  Launay, and Raffel}]{wang2022language}
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven~Le Scao, Hyung~Won Chung,
  Iz~Beltagy, Julien Launay, and Colin Raffel. 2022.
\newblock What language model architecture and pretraining objective work best
  for zero-shot generalization?
\newblock \emph{arXiv preprint arXiv:2204.05832}.

\bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le}]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}.

\bibitem[{Wu et~al.(2021)Wu, Zhao, Yu, Zhang, Shen, Liu, Li, Zhu, Luo, Xu
  et~al.}]{wu2021yuan}
Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng
  Li, Hong Zhu, Jiangang Luo, Liang Xu, et~al. 2021.
\newblock Yuan 1.0: Large-scale pre-trained language model in zero-shot and
  few-shot learning.
\newblock \emph{arXiv preprint arXiv:2110.04725}.

\bibitem[{Xue et~al.(2020)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel}]{mT5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel. 2020.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock \emph{arXiv preprint arXiv:2010.11934}.

\bibitem[{Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel}]{Xue2021mT5AM}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel. 2021.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{NAACL}.

\bibitem[{Zaken et~al.(2021)Zaken, Ravfogel, and Goldberg}]{zaken2021bitfit}
Elad~Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.
\newblock Bitfit: Simple parameter-efficient fine-tuning for transformer-based
  masked language-models.
\newblock \emph{arXiv preprint arXiv:2106.10199}.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi}]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1472} {{H}ella{S}wag: Can a
  machine really finish your sentence?}
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 4791--4800, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia
  et~al.}]{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al. 2022.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}.

\bibitem[{Zeng et~al.(2021)Zeng, Ren, Su, Wang, Liao, Wang, Jiang, Yang, Wang,
  Zhang et~al.}]{zeng2021pangu}
Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi~Liao, Zhiwei Wang, Xin Jiang,
  ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et~al. 2021.
\newblock Pangu-$\alpha$: Large-scale autoregressive pretrained chinese
  language models with auto-parallel computation.
\newblock \emph{arXiv preprint arXiv:2104.12369}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin et~al.}]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}.

\end{thebibliography}
