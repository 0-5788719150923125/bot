\section{Conclusions}

We propose a transfer learning based approach to cross-learn dialects of English, using a neural G2P model with a Transformer architecture, and attain high accuracy for dialects with smaller available data. We evaluate the PER across two dialects of English, sub-setting the dictionaries in a realistic manner to create artificially small training datasets, allowing for evaluation across dataset size. Our experiments show that for small dictionaries, transfer learning from models pretrained on larger dialects is a powerful method to significantly improve PER and quality. We observe that as the dictionary size increases, training a model from scratch quickly improves. However, depending on the dictionary, and the amount of training sentence-phoneme pairs it can generate, fine-tuning a model can lead to high accuracy. In the future, we plan to investigate application of multilingual approaches to multiple dialects, and instead create multi-dialect G2P models for multi-dialect/multi-speaker TTS models.