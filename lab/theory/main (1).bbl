\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[{Bai et~al.(2019)Bai, Ding, Bian, Chen, Sun, and Wang}]{simgnn2019}
Bai, Y.; Ding, H.; Bian, S.; Chen, T.; Sun, Y.; and Wang, W. 2019.
\newblock SimGNN: A Neural Network Approach to Fast Graph Similarity
  Computation.
\newblock WSDM '19, 384–392. New York, NY, USA: Association for Computing
  Machinery.
\newblock ISBN 9781450359405.
\newblock \doi{10.1145/3289600.3290967}.
\newblock \urlprefix\url{https://doi.org/10.1145/3289600.3290967}.

\bibitem[{Cheng et~al.(2020)Cheng, Zhang, He, Chen, Cheng, and
  Lu}]{cheng2020shiftgcn}
Cheng, K.; Zhang, Y.; He, X.; Chen, W.; Cheng, J.; and Lu, H. 2020.
\newblock Skeleton-Based Action Recognition with Shift Graph Convolutional
  Network.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}.

\bibitem[{Choi et~al.(2008)Choi, Cho, Han, and
  Yang}]{hcinter/978-3-540-78566-8_10}
Choi, J.; Cho, Y.-i.; Han, T.; and Yang, H.~S. 2008.
\newblock A View-Based Real-Time Human Action Recognition System as an
  Interface for Human Computer Interaction.
\newblock In Wyeld, T.~G.; Kenderdine, S.; and Docherty, M., eds.,
  \emph{Virtual Systems and Multimedia}, 112--120. Berlin, Heidelberg: Springer
  Berlin Heidelberg.
\newblock ISBN 978-3-540-78566-8.

\bibitem[{Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song,
  Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller}]{choromanski2021rethinking}
Choromanski, K.~M.; Likhosherstov, V.; Dohan, D.; Song, X.; Gane, A.; Sarlos,
  T.; Hawkins, P.; Davis, J.~Q.; Mohiuddin, A.; Kaiser, L.; Belanger, D.~B.;
  Colwell, L.~J.; and Weller, A. 2021.
\newblock Rethinking Attention with Performers.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \urlprefix\url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 4171--4186. Minneapolis,
  Minnesota: Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock \urlprefix\url{https://www.aclweb.org/anthology/N19-1423}.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly
  et~al.}]{dosovitskiy2020image}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
  Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al.
  2020.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929} .

\bibitem[{Elfwing, Uchibe, and Doya(2018)}]{elfwing2018sigmoid}
Elfwing, S.; Uchibe, E.; and Doya, K. 2018.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock \emph{Neural Networks} 107: 3--11.

\bibitem[{Fey(2021{\natexlab{a}})}]{torch_scatter}
Fey, M. 2021{\natexlab{a}}.
\newblock {PyTorch Scatter}.
\newblock \urlprefix\url{https://github.com/rusty1s/pytorch_scatter}.

\bibitem[{Fey(2021{\natexlab{b}})}]{torch_sparse}
Fey, M. 2021{\natexlab{b}}.
\newblock {PyTorch Sparse}.
\newblock \urlprefix\url{https://github.com/rusty1s/pytorch_sparse}.

\bibitem[{Fey and Lenssen(2019)}]{pytorch_geometric_Fey2019}
Fey, M.; and Lenssen, J.~E. 2019.
\newblock Fast Graph Representation Learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}.

\bibitem[{{Htike} et~al.(2014){Htike}, {Khalifa}, {Mohd Ramli}, and
  {Abushariah}}]{vsurveilance}
{Htike}, K.~K.; {Khalifa}, O.~O.; {Mohd Ramli}, H.~A.; and {Abushariah}, M.
  A.~M. 2014.
\newblock Human activity recognition for video surveillance using sequences of
  postures.
\newblock In \emph{The Third International Conference on e-Technologies and
  Networks for Development (ICeND2014)}, 79--82.
\newblock \doi{10.1109/ICeND.2014.6991357}.

\bibitem[{Huang et~al.(2019)Huang, Wang, Huang, Huang, Wei, and
  Liu}]{huang2019ccnet}
Huang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y.; and Liu, W. 2019.
\newblock Ccnet: Criss-cross attention for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 603--612.

\bibitem[{Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret}]{katharopoulos20a}
Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F. 2020.
\newblock Transformers are {RNN}s: Fast Autoregressive Transformers with Linear
  Attention.
\newblock In III, H.~D.; and Singh, A., eds., \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, 5156--5165. PMLR.
\newblock
  \urlprefix\url{http://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[{{Ke} et~al.(2017){Ke}, {Bennamoun}, {An}, {Sohel}, and
  {Boussaid}}]{tdskrepke}
{Ke}, Q.; {Bennamoun}, M.; {An}, S.; {Sohel}, F.; and {Boussaid}, F. 2017.
\newblock A New Representation of Skeleton Sequences for 3D Action Recognition
  4570--4579.
\newblock \doi{10.1109/CVPR.2017.486}.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Kingma, D.~P.; and Ba, J. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980} .

\bibitem[{Li et~al.(2019)Li, Chen, Chen, Zhang, Wang, and Tian}]{Li_2019_CVPR}
Li, M.; Chen, S.; Chen, X.; Zhang, Y.; Wang, Y.; and Tian, Q. 2019.
\newblock Actional-Structural Graph Convolutional Networks for Skeleton-Based
  Action Recognition.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}.

\bibitem[{Liu, Tu, and Liu(2017)}]{ts3dliu}
Liu, H.; Tu, J.; and Liu, M. 2017.
\newblock Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action
  Recognition .

\bibitem[{Liu et~al.(2019)Liu, Shahroudy, Perez, Wang, Duan, and
  Kot}]{liu2019ntu}
Liu, J.; Shahroudy, A.; Perez, M.; Wang, G.; Duan, L.-Y.; and Kot, A.~C. 2019.
\newblock Ntu rgb+ d 120: A large-scale benchmark for 3d human activity
  understanding.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}
  42(10): 2684--2701.

\bibitem[{Liu et~al.(2020)Liu, Zhang, Chen, Wang, and
  Ouyang}]{liu2020disentangling}
Liu, Z.; Zhang, H.; Chen, Z.; Wang, Z.; and Ouyang, W. 2020.
\newblock Disentangling and Unifying Graph Convolutions for Skeleton-Based
  Action Recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 143--152.

\bibitem[{Omote, Tamura, and Ninomiya(2019)}]{omote-etal-2019-dependency}
Omote, Y.; Tamura, A.; and Ninomiya, T. 2019.
\newblock Dependency-Based Relative Positional Encoding for Transformer {NMT}.
\newblock In \emph{Proceedings of the International Conference on Recent
  Advances in Natural Language Processing (RANLP 2019)}, 854--861. Varna,
  Bulgaria: INCOMA Ltd.
\newblock \doi{10.26615/978-954-452-056-4_099}.
\newblock \urlprefix\url{https://www.aclweb.org/anthology/R19-1099}.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}]{pytorch}
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen,
  T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.;
  DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.;
  Bai, J.; and Chintala, S. 2019.
\newblock PyTorch: An Imperative Style, High-Performance Deep Learning Library.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle
  Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural
  Information Processing Systems 32}, 8024--8035. Curran Associates, Inc.
\newblock
  \urlprefix\url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[{Plizzari, Cannici, and Matteucci(2020)}]{plizzari2020spatial}
Plizzari, C.; Cannici, M.; and Matteucci, M. 2020.
\newblock Spatial temporal transformer network for skeleton-based action
  recognition.
\newblock \emph{arXiv preprint arXiv:2008.07404} .

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever}]{radford2018improving}
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.
\newblock Improving language understanding by generative pre-training .

\bibitem[{Ramachandran, Zoph, and Le(2017)}]{swish2017}
Ramachandran, P.; Zoph, B.; and Le, Q.~V. 2017.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941} .

\bibitem[{Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He}]{deepspeed2020kdd}
Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020.
\newblock DeepSpeed: System Optimizations Enable Training Deep Learning Models
  with Over 100 Billion Parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, KDD '20, 3505–3506. New York, NY,
  USA: Association for Computing Machinery.
\newblock ISBN 9781450379984.
\newblock \doi{10.1145/3394486.3406703}.
\newblock \urlprefix\url{https://doi.org/10.1145/3394486.3406703}.

\bibitem[{Shahroudy et~al.(2016)Shahroudy, Liu, Ng, and
  Wang}]{shahroudy2016ntu}
Shahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. 2016.
\newblock Ntu rgb+ d: A large scale dataset for 3d human activity analysis.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 1010--1019.

\bibitem[{Shen et~al.(2021)Shen, Zhang, Zhao, Yi, and Li}]{shen2021efficient}
Shen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2021.
\newblock Efficient Attention: Attention with Linear Complexities.
\newblock In \emph{WACV}. IEEE.

\bibitem[{{Shi} et~al.(2019){Shi}, {Zhang}, {Cheng}, and {Lu}}]{directed2019}
{Shi}, L.; {Zhang}, Y.; {Cheng}, J.; and {Lu}, H. 2019.
\newblock Skeleton-Based Action Recognition With Directed Graph Neural
  Networks.
\newblock In \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 7904--7913.
\newblock \doi{10.1109/CVPR.2019.00810}.

\bibitem[{Shi et~al.(2019{\natexlab{a}})Shi, Zhang, Cheng, and
  Lu}]{shi2019skeleton}
Shi, L.; Zhang, Y.; Cheng, J.; and Lu, H. 2019{\natexlab{a}}.
\newblock Skeleton-based action recognition with directed graph neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 7912--7921.

\bibitem[{Shi et~al.(2019{\natexlab{b}})Shi, Zhang, Cheng, and
  Lu}]{2sagcn2019cvpr}
Shi, L.; Zhang, Y.; Cheng, J.; and Lu, H. 2019{\natexlab{b}}.
\newblock Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based
  Action Recognition.
\newblock In \emph{CVPR}.

\bibitem[{Shiv and Quirk(2019)}]{neurips2019treepositional}
Shiv, V.; and Quirk, C. 2019.
\newblock Novel positional encodings to enable tree-based transformers.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle
  Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.
\newblock
  \urlprefix\url{https://proceedings.neurips.cc/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf}.

\bibitem[{Song et~al.(2020)Song, Zhang, Shan, and Wang}]{song2020stronger}
Song, Y.-F.; Zhang, Z.; Shan, C.; and Wang, L. 2020.
\newblock Stronger, Faster and More Explainable: A Graph Convolutional Baseline
  for Skeleton-Based Action Recognition.
\newblock In \emph{Proceedings of the 28th ACM International Conference on
  Multimedia (ACMMM)}, 1625--1633. New York, NY, USA: Association for Computing
  Machinery.
\newblock ISBN 9781450379885.
\newblock \doi{10.1145/3394171.3413802}.
\newblock \urlprefix\url{https://doi.org/10.1145/3394171.3413802}.

\bibitem[{Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler}]{tay2020efficient}
Tay, Y.; Dehghani, M.; Bahri, D.; and Metzler, D. 2020.
\newblock Efficient Transformers: A Survey.

\bibitem[{Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and
  Salakhutdinov}]{tsai2019TransformerDissection}
Tsai, Y.-H.~H.; Bai, S.; Yamada, M.; Morency, L.-P.; and Salakhutdinov, R.
  2019.
\newblock Transformer Dissection: An Unified Understanding for Transformer's
  Attention via the Lens of Kernel.
\newblock In \emph{EMNLP}. ACL.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{attn2017all}
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.~N.;
  Kaiser, u.; and Polosukhin, I. 2017.
\newblock Attention is All You Need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, 6000–6010. Red Hook, NY, USA:
  Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[{{Vemulapalli}, {Arrate}, and {Chellappa}(2014)}]{lie2014}
{Vemulapalli}, R.; {Arrate}, F.; and {Chellappa}, R. 2014.
\newblock Human Action Recognition by Representing 3D Skeletons as Points in a
  Lie Group.
\newblock In \emph{2014 IEEE Conference on Computer Vision and Pattern
  Recognition}, 588--595.
\newblock \doi{10.1109/CVPR.2014.82}.

\bibitem[{{Wang} et~al.(2012){Wang}, {Liu}, {Wu}, and {Yuan}}]{actionlet2012}
{Wang}, J.; {Liu}, Z.; {Wu}, Y.; and {Yuan}, J. 2012.
\newblock Mining actionlet ensemble for action recognition with depth cameras.
\newblock In \emph{2012 IEEE Conference on Computer Vision and Pattern
  Recognition}, 1290--1297.
\newblock \doi{10.1109/CVPR.2012.6247813}.

\bibitem[{Xie et~al.(2018)Xie, Li, Zhang, Chen, Han, and Liu}]{xiememory}
Xie, C.; Li, C.; Zhang, B.; Chen, C.; Han, J.; and Liu, J. 2018.
\newblock Memory Attention Networks for Skeleton-based Action Recognition.
\newblock 1639--1645.
\newblock \doi{10.24963/ijcai.2018/227}.

\bibitem[{Yan, Xiong, and Lin(2018)}]{yan2018spatial}
Yan, S.; Xiong, Y.; and Lin, D. 2018.
\newblock Spatial temporal graph convolutional networks for skeleton-based
  action recognition.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32.

\bibitem[{{Yong Du}, {Wang}, and {Wang}(2015)}]{rnndu}
{Yong Du}; {Wang}, W.; and {Wang}, L. 2015.
\newblock Hierarchical recurrent neural network for skeleton based action
  recognition.
\newblock In \emph{2015 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 1110--1118.
\newblock \doi{10.1109/CVPR.2015.7298714}.

\bibitem[{Zhang et~al.(2017)Zhang, Lan, Xing, Zeng, Xue, and
  Zheng}]{zhang2017view}
Zhang, P.; Lan, C.; Xing, J.; Zeng, W.; Xue, J.; and Zheng, N. 2017.
\newblock View adaptive recurrent neural networks for high performance human
  action recognition from skeleton data.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, 2117--2126.

\end{thebibliography}
