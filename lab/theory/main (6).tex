\pdfoutput=1
\documentclass[11pt]{article}
% \usepackage[review]{acl}
\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{sidecap}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{verbatim} % for multi-line comments
\usepackage{url}
\usepackage{pifont}
\usepackage{eqparbox}
\usepackage{arydshln} % for dashed line in tables 
\usepackage{bigstrut}
\usepackage{comment}
% \usepackage{minted}
\usepackage[linewidth=1pt]{mdframed}
\usepackage{framed}
\usepackage{xcolor,colortbl}
\usepackage{pgffor}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{makecell}
% \usepackage{arydshln}
% \usepackage{tabularray}
\usepackage{boxedminipage}
\usepackage{xspace}
\usepackage{array}
\usepackage{framed}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
% \usepackage{colortbl}
\usepackage{array}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usepackage{enumitem}
\usepackage{bigstrut}
% \usepackage{libertine}
\usepackage[notextcomp]{stix}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}




\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{soul}
% \usepackage{tgbonum}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\setlength\fboxsep{1pt}
\definecolor{lightergray}{RGB}{230,230,230}
\definecolor{DarkRed}{RGB}{130,25,0}
\definecolor{DarkGreen}{RGB}{30,130,30}
\definecolor{DarkBlue}{RGB}{0,0,250}
\definecolor{purple}{rgb}{0.5,0,1}
\definecolor{dcyan}{rgb}{0.2,0.6,0.5}
\definecolor{darkgreen}{rgb}{0,200,0}
\definecolor{light-gray}{gray}{0.95} % used in table
% \definecolor{darkgreen}{RGB}{0,140,0}
\definecolor{darkred}{RGB}{200,0,0}
\definecolor{lightgreen}{RGB}{231,255,219}
\definecolor{lightred}{RGB}{252,231,234}
\definecolor{lightyellow}{RGB}{250,253,191}
% \definecolor{DarkRed}{RGB}{130,25,0}


\newcommand{\redtext}[1]{\colorbox{lightred}{#1}\xspace}
\newcommand{\greentext}[1]{\colorbox{lightgreen}{#1}\xspace}
\newcommand{\yellowtext}[1]{\colorbox{lightyellow}{#1}\xspace}
\newcommand{\bluetext}[1]{\colorbox{lightblue}{#1}\xspace}


\newcommand{\cmark}{\textcolor{DarkGreen}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%
\newcommand{\todo}[1]{{\color{red} [TODO: {#1}]}}
\newcommand{\yizhong}[1]{\textcolor{violet}{[YZ: #1]}}
\newcommand{\yeganeh}[1]{\textcolor{magenta}{[YK: #1]}}
\newcommand{\swaroop}[1]{\textcolor{cyan}{[SM: #1]}}
\newcommand{\daniel}[1]{\textcolor{blue}{[DK: #1]}}
\newcommand{\hanna}[1]{\textcolor{brown}{[HH: #1]}}
\newcommand{\nascomment}[1]{\textcolor{blue}{\textbf{[#1 -- \textsc{nas}]}}}
\newcommand{\alisa}[1]{\textcolor{purple}{\textbf{[#1 -- \textsc{al}]}}}

% acronyms 
\newcommand{\name}{\textsc{Self-Instruct}}
\newcommand{\dataset}{\textsc{Diverse-Instructions}} % Free-instructions? LM-Instructions
\newcommand{\wildins}{\textsc{Wild-Instructions}}
\newcommand{\supernat}{\textsc{Super-NaturalInstructions}}
% \newcommand{\supernatShort}{\textsc{Sup-NatInst}}
\newcommand{\supernatShort}{\textsc{SuperNI}}
\newcommand{\tkinstruct}{\textsc{T$k$-Instruct}}
\newcommand{\tzero}{\textsc{T$0$}}
\newcommand{\natins}{\textsc{NaturalInstructions}}
\newcommand{\natinsShort}{\textsc{NatInst}}
\newcommand{\crossfit}{\textsc{CrossFit}}
\newcommand{\flan}{\textsc{FLAN}}
\newcommand{\gptthree}{\textsc{GPT3}}
\newcommand{\gptself}{\textsc{GPT3}$_{\textsc{Self-Inst}}$}
% \newcommand{\gptinstruct}[1]{\textsc{GPT3}$^{\text{Instruct}}_{\text{#1}}$}
\newcommand{\gptinstruct}[1]{$\text{InstructGPT}_{\text{#1}}$}
\newcommand{\zest}{\textsc{Zest}}
\newcommand{\promptsource}{\textsc{PromptSource}}
\newcommand{\bigbench}{\textsc{BigBench}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Language Model Self Instructing Enables Better Zero-shot Learners}
\title{Adjusting Language Models to Follow Instructions \\ with (Almost) No Human Annotations}
\title{Machine Self Instructing \\ Enables More Diverse Zero-shot Instruction Following}
\title{Language Model Self Instructing \\ Enables More Diverse Zero-shot Instruction Following}
\title{Language Model Self Instructing \\ Enables More Diverse Instruction Following Behaviors}
\title{\emph{Self-Instructing} Language Models to Align with Diverse Instruction}
\title{Self-Instructing Align Language Models with Diverse Commands}
\title{Self-Instructing Off-the-Shelf Language Models}
\title{Language Model Self-Instructing Enables Diverse Instruction Following}
\title{Adjusting Language Models to Follow Instructions \\ with Light Human Annotation}
\title{Adjusting Language Models to Follow Instructions \\ with Model Self-Instructing}
\title{Language Models are Self-Instructable}
\title{Language Models Self-Instructing}
\title{Language Models Can Self-Instruct}
\title{Self-Instructing Language Models}
\title{Language Models are Zero-shot Self-Instructors}
\title{Language Models Self Instruct}
\title{Self-Instruction of Language Models}
\title{\name{}: Adjusting Language Model to Follow \\ Instructions with its Own Generations}
\title{\name{}: Aligning Language Model \\ with Self Generated Instructions}

%\title{Self-Instruct: Learn to Follow Instructions by Self-creation of Instruction Tasks}
%\title{Self-Instruct: Learn from Instructions by Creating your Own Instruction Dataset}


% use a fancier font style for \tt 
\renewcommand\ttdefault{cmtt}

\author{
Yizhong Wang\textsuperscript{$\clubsuit$} \;\;\;  
Yeganeh Kordi\textsuperscript{$\diamondsuit$} \;\;\;
Swaroop Mishra\textsuperscript{$\heartsuit$} \;\;\; 
\textbf{Alisa Liu}\textsuperscript{$\clubsuit$}\\ 
\textbf{Noah A. Smith}\textsuperscript{$\clubsuit$}\textsuperscript{$+$} \;\;\;
\textbf{Daniel Khashabi}\textsuperscript{$\spadesuit$} \; \;\;
\textbf{Hannaneh Hajishirzi}\textsuperscript{$\clubsuit$}\textsuperscript{$+$}\\
  \textsuperscript{$\clubsuit$}University of Washington \; 
  \textsuperscript{$\diamondsuit$}Tehran Polytechnic \;
  \textsuperscript{$\heartsuit$}Arizona State University \\
  \textsuperscript{$\spadesuit$}Johns Hopkins University\; \textsuperscript{$+$}Allen Institute for AI \\
  {  \texttt{yizhongw@cs.washington.edu} } \\}

\begin{document}
\maketitle
\begin{abstract}


Large ``instruction-tuned'' language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. 
Nevertheless, they depend heavily on 
% ``instructions'' (natural language task definitions) 
human-written instruction data
% collected either from existing NLP datasets or 
that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. 
% \swaroop{Humans have a remarkable ability to learn instruction-following from a few supervised instruction tasks, probably by bootstrapping}.
% Large language models have demonstrated the ability to conduct many tasks in a zero-shot manner by following instructions. Existing studies have shown that such instruction following behavior can be better induced by an instruction tuning stage, where a diverse set of training tasks are used to finetune the model to respond to instructions. However, these tasks and their instructions are either collected from existing NLP datasets or newly written by human annotators. This largely limits their quantity, diversity and creativity, hindering the generality of the tuned model. 
We introduce \name{}, a  framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. 
% We found that large pretrained language models are able to generate creative instructions as well as relatively high-quality input and output pairs when prompted with a small number of seed tasks.
Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model.
% We test out its effectiveness with vanilla \gptthree{} as a case study, and generate over 50K instructions paired up with 80K instances. 
% To test out the effectiveness of \name, we experiment with vanilla \gptthree{} (not instruction-tuned). 
% This leads to  a large number of natural language instructions and corresponding instances (input-output pairs), which we then use to fine-tune \swaroop{instruction-tune} the \gptthree{} model itself. 
% This data is, in turn, used to finetune the \gptthree{} model itself.
% Evaluation on \supernat{} benchmark indicates that the resulting model \gptinstruct{{\color{blue}Self-Ins}}
% results in 33.1\% gain over \gptthree{}, on par with the performance of \gptinstruct{{\color{blue}OpenAI}}
% Evaluation on the \supernat{} benchmark shows that the self-instructed version of GPT3 (a.k.a. \gptself) 
% \hanna{does the model start with gpt and uses your data? or does it start with instructGPT and trained on your data; with this naming it seems that you self train instruct gpt and get performance close to instrutgpt-001}
% \daniel{
% It is the former: start with GPT3. 
% }
% can achieve 33\% absolute improvement over the vanilla GPT3 on the testing NLP tasks, on par with the performance of \gptinstruct{001}
Applying our method to vanilla \gptthree{}, we demonstrate a 33\% absolute improvement over the original model on \supernat{}, on par with the performance of \gptinstruct{001}\footnote{
\label{footnote:gpt:instruct}Unless otherwise specified, our comparisons are with the 
{\tt text-davinci-001} engine. 
We focus on this engine since it is the closest to our experimental setup: supervised fine-tuning with human demonstrations. 
The newer engines are more powerful, though they use more data (e.g., code completion or latest user queries) or algorithms (e.g., PPO) that are difficult to compare with. 
% \nascomment{this point might also need to be made in the experiment section, reader will forget about seeing it here} \alisa{I think this footnote would be better somewhere else, not in the abstract}
}, which is trained with private user data and human annotations.
For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with \name{} outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind \gptinstruct{001}.
% \nascomment{, which is much more expensive?  signal that this would be very hard to beat}. 
\name{} provides an almost annotation-free method for aligning pretrained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning\footnote{Code and data will be available at \url{https://github.com/yizhongw/self-instruct}.}.
% with limited reliance on human annotations 
% We release this large synthetic dataset, our seed tasks, and the human evaluation dataset to facilitate future studies on instruction tuning.


\begin{comment}
In this paper, we introduce a \name{}, a  framework which uses a pretrained language model itself to generate a large number of language instructions 
and corresponding instances (input-output pairs). 
This model-generated data can be used to bootstrap the instruction tuning of the model and enables better instruction following after several generation and training iterations. We show that this \name{} framework is effective for several models at different sizes or with different architectures, and brings consistent improvement as it grows until the size of 100K.
For example, we are able to improve GPT3's performance on xxx by yyy, matching InstructGPT which is trained with private human-written data.\yizhong{briefly compare with GPT3 and InstructGPT}. We believe that this method provides an annotation-free step for improving a pretrained language model before it is used for downstream tasks, and the resulting data will be valuable for future study on instruction tuning.
\end{comment}
\end{abstract}

\section{Introduction}
\label{sec:intro}
The recent  NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions~\citep[i.a.]{mishra2022cross,wei2022finetuned,sanh2022multitask,wang2022benchmarking,ouyang2022training,chung2022scaling}.
These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.
% ;~\citealp{devlin2019bert,raffel2020exploring, brown2020language, chowdhery2022palm} \alisa{not sure these citations are necessary, and hinders readability}
\promptsource{}~\citep{bach2022promptsource} and \supernat{}~\citep{wang2022benchmarking} are two notable recent datasets that use extensive manual annotation for collecting instructions to construct \tzero~\cite{bach2022promptsource,sanh2022multitask} and \tkinstruct~\cite{wang2022benchmarking}. 
% \alisa{I think this previous sentence can be omitted} 
However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them. 
Given these limitations, continuing to improve the quality of instruction-tuned  models 
% \nascomment{this is way too strong:} 
necessitates the development of alternative approaches for supervising instruction-tuned models. 
% \swaroop{comment-the motivation sounds like we will see a unsupervised learning approach in the next paragraph. How about adding the following to glue?}\swaroop{Evidently, Humans are capable of generalizing to unseen instructions with just a handful of supervised instructions. They could potentially be bootstraping the few supervision instances to create a large set of instances which can potentially help in simulation of various instruction-following scenarios}. \yizhong{Making analogy to human is risky, and I like the current motivation that human-annotated data is problematic. Let's keep the current version.}

% \daniel{
% I think the idea addresses two issues: 
% (1) cost of manual annotations 
% (2) lack of diversity in human annnotations. 
% Currently the text mentions (1) but does not cover (2)
% }


% \begin{figure}[t]
%     \centering
%     \includegraphics[scale=0.25, trim=0cm 0cm 0cm 0cm]{figures/teaser-1.png}
%     % figure here if you wanna modify it: https://docs.google.com/presentation/d/1GEYzNofuB_nCjhfFa2imfiye1rp3dgbeN6ALjXz2O2s/edit?usp=sharing 
%     \caption{
%         A high-level overview of \name{}. 
%          The process starts with a small seed set of instructions and their outputs (a). 
%          This seed set is then used to prompt an off-the-shelf LM to generate more language instructions/outputs (b), 
%          followed by filtering low-quality or repeated generations, and then added back to the initial repository of instructions. 
%          This bootstrapping cycle continues until there is little yield in each new cycle. 
%     }
%     \label{fig:teaster}
% \end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/data_generation_pipeline_v5.pdf}
    \caption{
     A high-level overview of \name{}. 
     The process starts with a small seed set of tasks (one instruction and one input-output instance for each task) as the task pool. 
     Random tasks are sampled from the task pool, and used to prompt an off-the-shelf LM to generate both new instructions and corresponding instances, 
     followed by filtering low-quality or similar generations, and then added back to the initial repository of tasks. The resulting data can be used for the instruction tuning of the language model itself later to follow instructions better. Tasks shown in the figure are generated by \gptthree{}. See \autoref{tab:generated_tasks} for more creative examples.
     %\daniel{the generated tasks don’t seem to be that innovative/creative/diverse (they look like typical NLP task) — which goes against what we’re saying about the diversification of tasks. %\nascomment{what is the little robot icon?  I don't understand; label it}}
    % Overview of the pipeline of using a language model to generate instruction data (instruction-input-output triples) and the resulting data is used for instruction tuning of the language model itself. 
    % Texts highlighted in green are generated by GPT3 language model (``davinci'' engine).
    }
    \label{fig:pipeline}
\end{figure*}

In this work, we introduce \name, a semi-automated process for instruction-tuning a pretrained LM using instructional signals from the model itself. 
% for extracting instructional signals from language models, which can subsequently be used to turning them into strong instruction-followers (see Fig.\ref{fig:teaster}). 
The overall process is an iterative bootstrapping algorithm (see \autoref{fig:pipeline}), which starts off with a limited (e.g., 175 in our study) seed set of manually-written instructions that are used to guide the overall generation. 
In the first phase, the model is prompted to generate instructions for new tasks. 
This step leverages the existing collection of instructions to create 
% more diverse \nascomment{I don't think our experiments demonstrate that the instructions are more diverse, tone this down} 
more broad-coverage instructions that define (often new) tasks.
Given the newly-generated set of instructions, the framework also creates input-output instances for them, which can be later used for supervising the instruction tuning. 
% model\todo{carefully define what these are and why they're needed.}.
Finally, various measures are used to prune low-quality and repeated instructions, before adding them to the task pool. 
This process can be repeated for many interactions until reaching a large number of tasks.
% the new additions are negligibly different. 
% \todo{it would be good to incorporate examples in the above paragraph}


To evaluate \name{} empirically, we run this framework on \gptthree{}~\cite{brown2020gpt3}, which is a vanilla LM (\S\ref{sec:generatd-data}).
The iterative \name{} process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs. 
We observe that the resulting data provides a diverse range of creative tasks and over 50\% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (\S\ref{subsec:diversity}).
On this resulting data, we build \gptself{} by fine-tuning \gptthree{} (i.e., the same model used for generating the instructional data). 
We evaluate \gptself{} in comparison to various other models on both typical NLP tasks included in \supernat{}~\cite{wang2022benchmarking}, and a set of new instructions that are created for novel usage of instruction-following models (\S\ref{sec:superni_results}). 
The \supernatShort{} results indicate that 
\gptself{} outperforms \gptthree{} (the original model) by a large margin (+33.1\%) and nearly matches the performance of \gptinstruct{001}. Moreover, our human evaluation on the newly-created instruction set shows that \gptself{} demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5\% gap behind \gptinstruct{001}. 
% \todo{mention wild instructions}
% \yeganeh{An analysis of the performance of the \gptself{} on newly developed user-oriented instruction set demonstrates 44\% improvement over \gptthree{} and it's only 5\% less effective than \gptinstruct{001}}

% \todo{para on experimental findings: 
%  - built a model based on GPT3 
%  - the result beats GPT3 on NatIns 
%  - it is on-par with GPT3instruct 
%  - human evaluation
% }


In summary, our contributions are: (1) \name, a method for inducing instruction-following capability 
% for improving \swaroop{the instruction following capability of} 
with minimal human-labeled data;
(2) We demonstrate its effectiveness via extensive instruction-tuning experiments; 
% We conduct extensive experiments to demonstrate that instruction-tuning can keep benefiting from very diverse \nascomment{?} instructions. \swaroop{comment- this point looks weak, since the finding here is not new.} \alisa{agree, perhaps just say ``demonstrate its effectiveness''?}
% (3) We release two by-product datasets: (a) \dataset{} a large dataset of X \yeganeh{52K}\nascomment{don't forget to fill this in}  natural language tasks and instructions resulting from \name{} with \gptthree{} and  \ins{} \nascomment{broken command here}, (b) a collection of manually written tasks about daily activities. 
(3) We release a large synthetic dataset of 52K instructions and a set of manually-written novel tasks for building and evaluating future instruction-following models.




\section{Related Work}
\label{sec:related}

% \daniel{
% % 
% % prompt/instruction generation: all these works generate instructions given a few examples.. The space of tasks is pre-defined and fixed (i.e., they don’t bootstrap new tasks; but we do).
% % \cite{zhou2022large}
% % \cite{ye2022guess}
% % \cite{singh2022explaining}
% % \cite{gu2022learning}
% % \cite{honovich2022instruction}

% Incorporate this: 
% % Self-x: all these works use LMs to bootstrtap interesting inferences (meta-relevant to the purpose of this work). Also relevant to the "Self-training" paragrapg but not quite. So either want to broaden the "self-training" paragrapg to fit these in, or create a new paragraph. 
% \cite{zhao2022pre}
% \cite{zelikman2022star}
% \cite{welleck2022generating}
% \cite{zhou2022prompt}
% }
% \cite{}

\paragraph{Instruction-following language models.}
A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated ``instructional'' data -- datasets containing language instructional commands and their desired outcome based on human judgement~\cite{weller2020learning, mishra2022cross,wang2022benchmarking,wei2022finetuned,sanh2022multitask,ouyang2022training,parmar-etal-2022-boxbart, scialom2022continual, chung2022scaling,luo2022biotabqa,puri2022how,yin2022contintin,chakrabarty2022help,lin2022unsupervised,gupta2022improving,muennighoff2022crosslingual}. 
Additionally, they show a direct correlation between the size and diversity of the ``instructional'' data and the generalizability of resulting models to unseen tasks. 
Since these developments depend on human annotated ``instructional'' data, this poses a bottleneck for progress toward more generalizable models 
\cite[for example see Fig.~5a in][]{wang2022benchmarking}. 
Our work aims to tackle this bottleneck by reducing the dependence on human annotators. 

Additionally, despite the remarkable performance of models like  \gptinstruct{}~\cite{ouyang2022training}, their construction process remains quite opaque. 
% While there is some understanding of their construction (human annotation in model training loop), given its inaccessibility to the broader research community there is limited understanding about the source(s) of success. 
% Setting the difficulty of re-training gigantic models, 
In particular, the role of \emph{data} has remained understudied due to limited transparency and data released by major corporate entities behind these key models. 
% \nascomment{this sentence is repetitive and needs to be trimmed down to essentials:} 
Addressing such challenges necessitates the creation of a large-scale, public dataset covering a broad range of tasks.


% In particular, the role of supervised data has remained understudied due to limited data released by major corporate entities behind major models. In addition,  it is nearly impossible for the research community to extend  and re-train these  gigantic models. Addressing these two challenges necessitates the availability of large-scale public benchmarks of a broad range of NLP tasks and their instructions to facilitate developing and evaluating models that can generalize to unseen tasks.
% \cite{chung2022scaling}\\
    
Instruction-following models have also been of interest in the multi-modal learning literature \cite{fried2018speaker, shridhar2020alfred, min2022film, weir2022one}.
\name{}, as a general approach to expanding data, can potentially also be helpful in those settings; however, this is out of the scope of this work.

    
% TODO: cite some speaker following work.
% \cite{fried2018speaker}
% \cite{shridhar2020alfred}
% \daniel{Is this the message here? 
% Highlight that there is a literature of building instruction-following models in multi-modal or in embodied environments. 
% }

\paragraph{Language models for data generation and augmentation.}
A variety of works have relied on generative LMs for data generation~\cite{schick2021generating,wang2021towards,liu2022wanli,meng2022tuning} or augmentation~\cite{feng2021survey,yang2020generative,mekala2022intermediate}.  For example, \citet{schick2021generating} propose to replace human annotations \emph{of a given task} with prompting large LMs and use the resulting data for fine-tuning (often smaller) models in the context of SuperGLUE tasks~\cite{wang2019superglue}. 
While our work can be viewed as a form of ``augmentation,'' our work differs from this line in that it is \emph{not} specific to a particular task (say, QA or NLI). In contrast, a distinct motivation for \name{} is to bootstrap new task definitions that may not have been defined before by any NLP practitioner (though potentially still important for downstream users).  
% Put differently, within the \nascomment{I don't understand what you mean by guardrails} guardrails for our meta-task (responding to instructions), we prompt LMs to improvise diverse \nascomment{?} and creative instructions. 


    
    % \item An \href{https://openreview.net/forum?id=NiEtU7blzN}{ICLR 2023 submission} on model self improving.
    


\paragraph{Self-training.}
A typical self-training framework ~\cite{he2019revisiting, xie2020self, du2021self, amini2022self, huang2022large} uses trained models to assign labels to unlabeled data and then leverages the newly labeled data to improve the model. In a similar line, \citet {zhou2022prompt} use multiple prompts to specify a single task and propose
to regularize via prompt consistency, encouraging
consistent predictions over the
prompts. This allows either 
finetuning the model  with extra unlabeled
training data, or direct application at inference time.
While \name{} has some similarities with the self-training literature, most self-training methods assume a specific \textit{target task} as well as \textit{unlabeled examples} under it; in contrast, \name{} produces a variety of tasks from scratch.
% there are two key differences: (1) most self-training methods assume the existence of unlabeled examples and focus on assigning labels, and (2) assume a specific target task.
% Unlike most self-training methods, \name{} does not assume any specific target task and instead generates several varieties of tasks from scratch. 
% Nonetheless, self-training methods, such as the self-consistency~\cite{wang2022self} used by \citet{huang2022large} can be leveraged \nascomment{vague -- do we do it or not?  don't say what could be done without being clear whether we do it} in the final step of \name{} to improve the filtering method and subsequently the quality of instruction task collection.



% \name{} leverages pretrained models to build datasets (with instructions) almost from scratch, in contrast to most self-training methods that requires unlabeled examples. (2) \name{} is a task-agnostic method intended to build general-purpose models, in contrast to the most self-training methods that focuses on a model for some specific tasks.

% 1) they focus more on the output part with the assumption that unlabeled examples are available; while we need to generate both input and output fields as well as the instruction; 2) we don’t assume the target task, we need to generate many task from scratch

    % \cite{huang2022large}
    % \cite{amini2022self}
    % \cite{xie2020self}
    % \cite{he2019revisiting}
    % Key differences: 1. we focus on general-purpose models; 2. the initial model is a language model instead of a model built for specific tasks; 3. we use the model to generate instructions, input and output, build datasets from almost scratch. Most self-training methods focus on unlabeled examples but the inputs are given; 4. Self-training can actually be used in our final step to improve the data quality. Leave for future work.


\paragraph{Knowledge distillation.}
Knowledge distillation~\cite{hinton2015distilling,Sanh2019DistilBERTAD, west2021symbolic,luciecharlotte2022teachingsmallmodels} often involves the transfer of knowledge from larger models to smaller ones. \name{} can also be viewed as a form of ``knowledge distillation", however, it differs from this line in the following ways: (1) the source and target of distillation are the same, i.e., a model's knowledge is distilled to itself; (2) the content of distillation is in the form of an instruction task (i.e., instructions that define a task, and a set of examples that instantiate it).

\paragraph{Bootstrapping with limited resources.} A series of recent works use language models to bootstrap some inferences using specialized methods.  
% NPPrompt~\cite{zhao2022pre} uses a model's own embeddings to automatically find relevant words \nascomment{confused here, not sure what you mean} to labels and this way reduces the dependency on verbalizers \nascomment{what are verbalizers?}.
NPPrompt~\cite{zhao2022pre} provides a method to generate predictions for semantic labels without any fine-tuning. It uses a model's own embeddings to automatically find words relevant to the label of the data sample and hence reduces the dependency on manual mapping from model prediction to label (verbalizers).
% relevant words \nascomment{confused here, not sure what you mean} to labels and this way reduces the dependency on verbalizers \nascomment{what are verbalizers?}.
STAR~\cite{zelikman2022star} iteratively leverages a small number of rationale examples and a large dataset without rationales, to bootstrap a model's ability to perform reasoning.  Self-Correction~\cite{welleck2022generating} decouples an imperfect base generator (model) from a separate corrector that learns to iteratively correct imperfect generations and demonstrates improvement over the base generator. Our work instead focuses on bootstrapping new tasks in the instruction paradigm.

\paragraph{Instruction generation.}
A series of recent works~\cite{zhou2022large, ye2022guess, singh2022explaining, honovich2022instruction} generate instructions of a task given a few examples. While \name{} also involves instruction generation, a major difference in our case is it is task-agnostic; we generate new tasks (instructions along with instances) from scratch.


% \daniel{
% 
% prompt/instruction generation: all these works generate instructions given a few examples.. The space of tasks is pre-defined and fixed (i.e., they don’t bootstrap new tasks; but we do).

% \cite{singh2022explaining}
% \cite{gu2022learning}
% \cite{honovich2022instruction}

% Incorporate this: 
% Self-x: all these works use LMs to bootstrtap interesting inferences (meta-relevant to the purpose of this work). Also relevant to the "Self-training" paragrapg but not quite. So either want to broaden the "self-training" paragrapg to fit these in, or create a new paragraph. 
% \cite{zhao2022pre}
% \cite{zelikman2022star}
% \cite{welleck2022generating}
% \cite{zhou2022prompt}
% }
% \cite{}



% Language models have been leveraged in the literature~\cite{petroni2019language, jiang2020can, burns2022discovering} to distill knowledge~\cite{cho2019efficacy, gou2021knowledge}; this knowledge is often used to assist models in downstream tasks.





% Language models have been leveraged in the literature~\cite{petroni2019language, jiang2020can, burns2022discovering} to distill knowledge~\cite{cho2019efficacy, gou2021knowledge}; this knowledge is often used to assist models in downstream tasks. 

% Our framework \name{} instead is focused on distilling knowledge in the form of instruction tasks (i.e. instructions that define a task, and a set of examples that instantiates it) which can potentially help develop a general-purpose model for a broad range of tasks.
% \daniel{
% This paragraph needs to be improved. My understanding is that "distillation" is often studied in the context of transferring knowledge e.g., (next-word probs in distilled-bert paper and many others) or (common-sense knowledge; symbolic knowledge distillation paper) from larger models to smaller ones.
% So there are two differences: 
% (1) The *source/target* of distillation: What we're doing is also a form of distillation, alas from a model to *itself*. 
% (2) The *content( of distillation: we're distilling "instructional" knowledge 
% }


% Model and knowledge Distillation
%     symbolic knowledge distillation 

% \daniel{
% if we want to argue that LMs already know and we just need a way to surface them: 
% \cite{burns2022discovering}
% Some of the arguments from the intro might also be useful here. 
% }



% \section{Language Model \name{}}
\section{Method}
\label{sec:method}

Annotating large-scale instruction data can be challenging for humans because it requires 1) creativity to come up with novel tasks and 2) expertise for writing the labeled instances for each task.
In this section, we detail our process for \name, which refers to the pipeline of generating tasks with a \textbf{vanilla pretrained language model} itself and then conducting instruction tuning with this generated data in order to align the language model to follow instructions better. This pipeline is depicted in \autoref{fig:pipeline}. 
% \yizhong{We should emphasize we use the vanilla language model.}

\subsection{Defining Instruction Data}
\label{subsec:instruction-data-definition}

The instruction data we want to generate contains a set of instructions $\{I_t\}$, each of which defines a task $t$ in natural language. Each task has one or more input-output instances $(X_t, Y_t)$.
A model $M$ is expected to produce the output $y$, given the task instruction $I_t$ and the instance input $x$: $M(I_t, x) = y, \;  \mbox{for} \ (x,y)\in (X_t, Y_t)$.
% For every instance, the model is expected to take the task instruction ${I_t}$ and the instance input $x$, and generate the output $y$. 
% Note that many instructions may not require additional input (e.g., ``write a poem'' itself is a valid request that models should respond to). In such cases, the instance input $x$ can be empty. 
Note that the instruction and instance input does not have a strict boundary in many cases. For example, ``write an essay about school safety'' can be a valid instruction that we expect models to respond to directly, while it can also be formulated as ``write an essay about the following topic'' as the instruction, and ``school safety'' as an instance input. To encourage the diversity of the data format, we allow such instructions that do not require additional input (i.e., $x$ is empty).

% \daniel{Somewhere (here?) we need to clarify how "input" and "instruction" are related or different, given that both are *inputs* to the model.} \yizhong{Check the updated version?}

\subsection{Automatic Instruction Data Generation}
\label{subsec:data-generation}

Our pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data.

\paragraph{Instruction Generation.}
\name{} is based on a finding that large pretrained language models can be prompted to generate new and novel instructions when presented with some existing instructions in the context. This provides us with a way to grow the instruction data from a small set of seed human-written instructions. 
We propose to generate a diverse set of instructions in a bootstrapping fashion. We initiate the task pool with 175 tasks (1 instruction and 1 instance for each task) written by our authors. For every step, we sample 8 task instructions from this pool as in-context examples.
% and use them as the prompt to make language models continue the generation of new instructions. 
Of the 8 instructions, 6 are from the human-written tasks,
 % to guarantee \nascomment{this is too strong, you have no guarantee} the quality
and 2 are from the model-generated tasks in previous steps to promote diversity. 
The prompting template is shown in \autoref{tab:instruction_generation_template}.
% provides the template for combining these instructions as a prompt to the language model. 
% We will show later that language models can successfully generate meaningful and creative instructions when prompted in this way.

\paragraph{Classification Task Identification.}
Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.\footnote{More concretely, we regard tasks that have a limited and small output label space as classification tasks.} 
% 25 out of our 175 seed instructions are classification tasks. We mixed 12 classification seed instructions and 19 non-classification seed instructions as the prompt to ask the language model whether a newly generated instruction is  classification or not. 
We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in \autoref{tab:classification_task_identification_template}. 
% \autoref{tab:classification_task_identification_template} presents our prompt template for this step.

\paragraph{Instance Generation.}
Given the instructions and their task type, we generate instances for each instruction independently. This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output. We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.
% \alisa{confused about this sentence}
% Generating multiple instances is important for the model to learn to not only reply to the instructions, but also look at the input and learn to do reasoning over the input. See \ref{sec:prompting_templates} for the prompting templates.
A natural way to do this is the \textbf{Input-first Approach}, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in \autoref{tab:input-first-generation-template}. 

However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input). Therefore, we additionally propose an
\textbf{Output-first Approach} for classification tasks, where we first generate the possible class labels, and then condition the input generation on each class label. The prompting template is shown in \autoref{tab:output-first-generation-template}.\footnote{In this work, we use a fixed set of seed tasks for prompting the instance generation, and thus only generate a small number of instances per task in one round. Future work can use randomly sampled tasks to prompt the model to generate a larger number of instances in multiple rounds.} We apply the output-first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.

% \daniel{
% Incorporate Yizhong's response: 
% The rationale of doing output-first is, for classification tasks, when generating the output first, we can easily generate all the possible class labels, and then condition the input generation on the class label. If using input first approach, the model's generation is usually biased to one label.
% }

\paragraph{Filtering and Postprocessing.}
\label{subsec:filtering}
To encourage diversity, a new instruction is added to the task pool only when its ROUGE-L overlap with any existing instruction is less than 0.7. 
We also exclude instructions that contain some specific keywords (e.g., images, pictures, graphs) that usually can not be processed by language models. When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs.  


% \subsection{Self Training Algorithm}
% \label{subsec:self-training}

% The original GPT3 model usually cannot generate instances very well, especially for the challenging ones. We can use the self-training algorithm in \url{https://openreview.net/forum?id=SJgdnAVKDH} to bootstrap the model by doing the instance generation and instruction tuning alternatively in multiple rounds.

\subsection{Finetuning the LM to Follow Instructions}
\label{subsec:instruction-tuning}
After the creation of the large-scale instruction data, we use this data to finetune the original language model (i.e., \name{}). To do this, we concatenate the instruction and instance input as a prompt and train the model to generate the instance output in a standard supervised way. To make the model robust to different formats, we use multiple templates to encode the instruction and instance input together. For example, the instruction can be prefixed with ``Task:'' or not, the input can be prefixed with ``Input:'' or not, ``Output:'' can be appended at the end of the prompt, and different numbers of break lines can be put in the middle, etc.
% \nascomment{this is kind of vague.  what do we actually do?}


\input{figures/data_statistics.tex}

\section{\name{} Data from \gptthree{}}
\label{sec:generatd-data}

% So far, we have introduced our method of inducing instruction data from a pretrained language model. 
In this section, we apply our method for inducing instruction data to \gptthree{} as a case study. We use the largest GPT3 language model (``davinci'' engine) accessed through the OpenAI API\footnote{\url{https://openai.com/api/}}. The parameters for making queries are described in Appendix \ref{subsec:query_gpt3_api}. Here we present an overview of the generated data.

\subsection{Statistics}
\label{subsec:statistics}

\autoref{tab:data_statistics} describes the basic statistics of the generated data. We generate a total of over 52K instructions, and more than 82K instances corresponding to these instructions after filtering.
\input{tables/data_statistics.tex}

\subsection{Diversity}
\label{subsec:diversity}
To study what types of instructions are generated and how diverse they are, we identify the verb-noun structure in the generated instructions. We use the Berkeley Neural Parser\footnote{\url{https://parser.kitaev.io/}} \citep{kitaev-klein-2018-constituency,kitaev-etal-2019-multilingual} to parse the instructions, and then extract the verb that is closest to the root of the parse tree as well as its first direct noun object. 26,559 out of the 52,445 instructions contain such structure; other instructions usually contain more complex clauses (e.g., ``Classify whether this tweet contains political content or not.'') or are framed as questions (e.g., ``Which of these statements are true?''). 
We plot the top 20 most common root verbs and their top 4
direct noun objects in \autoref{fig:verb-noun-distribution}, which accounts for 14\% of the entire set. Overall, we see quite diverse intents and textual formats in these instructions.

We further study how the generated instructions differ from the seed instructions that are used to prompt the generation. For each generated instruction, we compute its highest ROUGE-L overlap with the 175 seed instructions. We plot the distribution of these ROUGE-L scores in \autoref{fig:overlap-distribution}, indicating a decent number of new instructions that do not have much overlap with the seeds. 
% \alisa{how to quantify what is ``much overlap''?}
We also demonstrate diversity in length of the instructions, instance inputs, and instance outputs in \autoref{fig:length_distribution}. 
% \alisa{what counts as ``diverse'' length?}

\subsection{Quality}
So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output. 
% Table \ref{tab:data_quality_eval} lists the questions we ask the expert annotator, and to what proportion the evaluated field is valid. 
Evaluation results in \autoref{tab:data_quality_eval} show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in \autoref{tab:generated_tasks} and \autoref{tab:bad-generated-tasks} respectively.

\input{tables/synthetic_data_quality}


\label{subsec:quality}



\section{Experimental Results}
% \section{Zero-shot Evaluation on NLP Tasks}
\label{sec:superni_results}

% \section{Experiments}
% \label{sec:exp}
% \paragraph{LM Eval Harness}
% \paragraph{PromptSource Tasks}
% \paragraph{T0 Eval}
% \paragraph{BigBench}
% \paragraph{EditEval}
% \paragraph{SuperNaturalInstructions}

We conduct experiments to measure and compare the quality of models under various instruction tuning setups. 
We first describe our models and other baselines, followed by our experiments. 

\subsection{\gptself{}: fine-tuning \gptthree{} on its own instruction data}
\label{subsec:self-instruct-gpt3}
% \daniel{
% Here or earlier: 
% We build \gpttself{} based on the data generated in S4.
% Need to flesh out hyper-parameters etc for this training.
% }
With the instruction-generated instruction data, we conduct instruction tuning for the \gptthree{} model itself (the ``davinci'' engine). As we described in \S\ref{subsec:instruction-tuning}, we use various templates to concatenate the instruction and input, and train the model to generate the output. This finetuning is done through the OpenAI finetuning API\footnote{\url{https://beta.openai.com/docs/guides/fine-tuning}}. We use the default hyper-parameters, except that we set the prompt loss weight to 0, and we train the model for 2 epochs. We refer the readers to Appendix~\ref{subsec:finetuning_details} for additional finetuning details. The resulting model is denoted as \gptself{}.

\subsection{Baselines}
% We compare our self-instructed GPT3 with several classes of baselines. 
\paragraph{Off-the-shelf language models.} 
We evaluate T5-LM \cite{lester2021power,raffel2020exploring} and \gptthree{}~\cite{brown2020gpt3} as the vanilla LM baselines (only pre-training, no additional fine-tuning). 
These baselines will indicate the extent to which off-the-shelf LMs are capable of following instructions naturally immediately after pretraining.

\paragraph{Publicly-available instruction-tuned models.}
\tzero{} and \tkinstruct{} are two instruction-tuned models proposed in \citet{sanh2022multitask} and \citet{wang2022benchmarking} respectively, and are demonstrated to be able to follow instructions for many NLP tasks. 
Both of these models are finetuned from the T5~\cite{raffel2020exploring} checkpoints and are publicly available\footnote{\url{https://huggingface.co/bigscience/T0}}\footnote{\url{https://huggingface.co/allenai/tk-instruct-11b-def}}. For both of these models, we use their largest version with 11B parameters.

\paragraph{Instruction-tuned GPT3 models.}
 We evaluate \gptinstruct{}~\cite{ouyang2022training}, 
which is developed by OpenAI based on GPT3 to follow human instructions better and has been found by the community to have impressive zero-shot abilities. 
There are various generations of these models,
where newer ones use more expansive data or algorithmic novelties\footnote{\url{https://beta.openai.com/docs/model-index-for-researchers}}.  
%based on various fine-tuning data or algorithms. 
For our \supernatShort{} experiments in \S\ref{subsec:superni-experiments}, we only compare with their \texttt{text-davinci-001} engine, because their newer engines are trained with the latest user data and are likely to already see the \supernatShort{} evaluation set. For our human evaluation of these models on newly written instructions, we include their 001, 002 and 003 engines for completeness.

Additionally, to compare \name{} training with other publicly available instruction tuning data, we further finetune GPT3 model with data from \promptsource{} and \supernatShort{}, which are used to train the \tzero{} and \tkinstruct{} models. We call them \tzero{} training and \supernatShort{} training for short, respectively. 
To save the training budget, we sampled 50K instances (but covering all their instructions) for each dataset, which has a comparable size to the instruction data we generated. Based on the findings from \citet{wang2022benchmarking} and our early experiments, reducing the number of instances per task does not degrade the model's generalization performance to unseen tasks.
% \alisa{this sounds suspicious... what is the size difference? were the original datasets much bigger?}
% Most of our experiments are based on their \texttt{text-davinci-001} engine, which is closest to our setup (see Footnote~\ref{footnote:gpt:instruct}). 



\subsection{Experiment 1: Zero-Shot Generalization on \supernatShort{} benchmark}
\label{subsec:superni-experiments}
We first evaluate the models' ability to follow instructions on typical NLP tasks in a zero-shot fashion. 
% We use \supernat{}~\cite{wang2022benchmarking} (\supernatShort{} in short) for evaluating models' ability to solve typical NLP tasks by following instructions. 
We use the evaluation set of \supernatShort{} ~\cite{wang2022benchmarking}, which consists of 119 tasks with 100 instances in each task. 
% There are a total of 11,810 instances in this evaluation set.
In this work, we mainly focus on the zero-shot setup, i.e., the model is prompted with the definition of the tasks only, without in-context demonstration examples.
For all our requests to the \gptthree{} variants, we use the deterministic generation mode (temperature as 0 and no nucleus sampling) without specific stop sequences.

% The evaluation set of \supernatShort{} contains 119 tasks in 12 categories, with 100 instances in each task.

\input{tables/superni_results.tex}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth, trim={0 20 0 10}]{figures/human_instruction_results.pdf}
    \caption{
    Performance of GPT3 model and its instruction-tuned variants, evaluated by human experts on our 252 user-oriented instructions (\S\ref{sec:user_instructions}). Human evaluators are instructed to rate the models' responses into four levels.
    % \daniel{Added this; check if it is right:}
    The results indicate that \gptself{} outperforms 
    all the other \gptthree{} variants trained on publicly available instruction datasets. 
    Additionally, \gptself{} scores nearly as good as \gptinstruct{001} (c.f., \autoref{footnote:gpt:instruct}).
    }
    \label{fig:human_instructions_results}
\end{figure*}

\paragraph{Results.}
% Table~\ref{tab:superni_results} compares the performance of different models on the evaluation set of \supernatShort{}. 
We make the following observations from the results in \autoref{tab:superni_results}. 
\name{} 
% significantly \nascomment{do not use the word `significantly' if you did not do a significance test!}  
boosts the instruction-following ability of \gptthree{} by a large margin. The vanilla \gptthree{} model basically cannot follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation. 
Compared with other models that are not specifically trained for \supernatShort{}, \gptself{} achieves better performance than \tzero{} or the \gptthree{} finetuned on the \tzero{} training set, which takes tremendous human labeling efforts. Notably, \gptself{} also nearly matches the performance of \gptinstruct{001}, which is trained with private user data and human-annotated labels.

Models trained on \supernatShort{} training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting. However, we show that \name{} still brings in additional gains when combined with the \supernatShort{} training set, proving its value as complementary data. 
% We will show that models trained with \name{} demonstrate even more broad \nascomment{what does `broad' mean here?  cut it} instruction following than those trained on with T0 or \supernatShort{} in Section \ref{sec:user_instructions}.



\subsection{Experiment 2: Generalization to User-oriented Instructions on Novel Tasks}
\label{sec:user_instructions}

Despite the comprehensiveness of \supernatShort{} in collecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications. 
% These instructions are newly written by three of our co-authors. 
We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions. \autoref{tab:case_study} presents a small portion of the 252 tasks. The whole test set will be available upon request.

% The current benchmarks for evaluating models often do not accurately reflect user  needs or instructions found in real-world applications. In order to better assess the practical value of NLP models, we propose a new benchmark for evaluating their ability to generalize to user-oriented instructions. This benchmark consists of a collection of instructions from real-world applications such as productivity, writing, and collaboration tools, as well as social media and entertainment applications. These instructions cover a wide range of tasks, styles, and formats commonly found in real-world settings and provide a more challenging testbed for NLP models. These instructions represent a diverse set of real-world tasks that NLP models may not have encountered during training, making them a valuable resource for evaluating the ability of NLP models to generalize to new and unfamiliar instructions. By assessing the performance of NLP models on these instructions, we can better understand their ability to handle the diverse and often open-ended tasks found in real-world applications. \nascomment{way too vague here.  I have no idea how you produced these instructions!  I remember talking about crowdsourcing this, but if you did that you need to give a lot more detail about what you asked people to do and why!}

\paragraph{Human evaluation setup.}
Evaluating models' performance on this evaluation set of diverse tasks is extremely challenging because different tasks require different expertise. Indeed, many of these tasks cannot be measured by automatic metrics or even be judged by normal crowdworkers (e.g., writing a program, or converting first-order logic into natural language). To get a more faithful evaluation, we asked the authors of the instructions to judge model predictions. The evaluators were asked to rate the output based on whether it accurately and effectively completes the task. We implemented a four-level rating system for categorizing the quality of the models' outputs, defined as follows:

\begin{itemize}[noitemsep, leftmargin=*]
    \item \textsc{Rating-A:} The response is valid and satisfying.
    \item \textsc{Rating-B:} The response is acceptable but has minor errors or imperfections that can be improved.
    \item \textsc{Rating-C:} The response is relevant and responds to the instruction, but it has significant errors in the content. For example, GPT3 might generate a valid output first, but continue to generate other irrelevant things.
    \item \textsc{Rating-D:} The response is irrelevant or invalid, including repetition of the input, totally irrelevant output, etc.
\end{itemize}

% Level 1 indicates that the response is irrelevant or invalid, such as a repetition of the input or totally irrelevant output. Level 2 indicates that the response is relevant and responds to the instruction, but contains significant errors in the content, for example, the model generates a valid output first but continues to generate other irrelevant responses. Level 3 indicates that the response is acceptable but has minor errors or imperfections that could be improved. Finally, Level 4 indicates that the response is valid and satisfying. This four-level system allowed us to systematically evaluate the performance of models on the user-oriented instructions and provided a detailed understanding of the model's capabilities and limitations.



%\todo{One paragraph describing how we set up the human evaluation, and what we evaluate. If we have an interface, let's include it in the appendix.}
%To encourage the practical value of these instruction-based models, we can come up instructions from the perspective of real-world applications (e.g., Grammarly, MS Office, Google docs, \url{https://ludwig.guru/}, Yelp, LinkedIn, Amazon, Netflix, Overleaf, Spotify). E.g., for Spotify, we can write instructions such as "Recommend 10 songs to listen during road trip."
%Ask Yeganeh to come up with 50 applications, and think of tasks in these applications, write instructions \& instances.
%Aiming for 500 instructions and instances written by experts. 
%Human evaluation to get the performance.
\paragraph{Results.}
\autoref{fig:human_instructions_results} provides the performance of \gptthree{} model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla \gptthree{} language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance, 
% In most cases, the model simply repeats the input and cannot stop its generation until it reaches the generation length limit. 
% Consistent with the previous experiment on \supernatShort{} the \name{} significantly enhanced the performance of the GPT3 model.
% All instruction-tuned models demonstrate a better ability than \gptthree{} to follow these newly written testing instructions. 
Nonetheless, \gptself{} (i.e.,  \gptthree{} model fine-tuned with \name) outperforms those counterparts trained on \tzero{} or \supernatShort{} by a large margin, demonstrating the value of the generated data despite the noise. 
Compared with \gptinstruct{001} (c.f. \autoref{footnote:gpt:instruct}), \gptself{} is quite close in the performance---if we count acceptable response with minor imperfections (\textsc{Rating-3}) as valid, \gptself{} is only 5\% behind \gptinstruct{001}. Lastly, our evaluation confirms the impressive instruction-following ability of \gptinstruct{002} \& \gptinstruct{003} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in \citet{ouyang2022training}.
% \alisa{need to emphasize how much less resources was required for self-instruct.}

% Also, it outperformed the models trained on the labeled data (T0 Training and \supernatShort{} Training). Our results demonstrate that \gptself{} achieves a performance that is comparable to that of the \gptinstruct{001} model, although there is still a large gap from the \gptinstruct{002} and \gptinstruct{003} models.
% \name{} performed better than the model trained on \name{} and \supernatShort{}. We conjecture that the observed results may be attributed to the preponderance of classification tasks in \supernatShort{}. 

%\nascomment{from here you start calling it ``self-instructing'' but I thought the name was self-instruct (with smallcaps) ... be consistent!} 

\subsection{Example Predictions from \gptself{}}
\label{sec:example-predictions}
% \subsection{Case study} \nascomment{I think you should retitle this section ``Examples''}
% To provide a more intuitive sense of how good our model is in following instructions and what is its limitation, 
We present a selection of user-oriented tasks, the corresponding \gptself{}-produced responses and annotator ratings in \autoref{tab:case_study}. %The tasks include queries about making a decision, work management, and logical inference. 
% Overall, these are many cases we found the \gptself{} model provides acceptable or even satisfying responses. 
We see that even for responses rated as level 2, the model demonstrates extensive steps in solving the task, even though its final output is incorrect. 
% While the \name{} model was able to accurately provide the requested information for most tasks, there were some instances in which errors were observed. 
% Despite these errors, the overall performance of the \name{} model demonstrates it is effective in handling user-oriented tasks.
\input{tables/case_study.tex}


%\todo{Yeganeh, could you create a big table with perfect generations from the self-instructing model, as well as some error examples where the model demonstrates some typical mistakes? All predictions are here:
% \href{https://docs.google.com/spreadsheets/d/1Cl69ki30Hfr90hMEzm2GqF4Fx2TAKZqJZL-Qfm_d758/edit#gid=702154844}{here}}


% \subsection{Effectiveness across Models}
% Try GPT-J (smaller size, openness) and T5-LM (encoder-decoder architecture).


% \section{Effect of Instruction Data Size}

% For data size, we need to plot the performance change against the number of generated instructions.

% For data quality, we can use the quality annotations from \ref{subsec:quality} and only use the high-quality instances for instruction tuning.

\section{Discussion and Limitation}

\subsection{Why does \name{} work?}
It is worthwhile to  reflect on the role that high-quality human feedback plays 
% the fundamentals that have  
in enabling the recent successes on instruction-tuning LMs~\cite{mishra2022cross,wang2022benchmarking,wei2022finetuned,sanh2022multitask,ouyang2022training}. 
% Fundamentally what do these models learn from observing human-annotated instructions? 
Here are two extreme hypotheses: 

\newcommand{\subscript}[2]{$#1 _ #2$}
\begin{enumerate}[label=(\subscript{H}{{\arabic*}})]
    \item  \emph{Human feedback} is a \emph{necessary and indispensable} aspect of instruction-tuning as LMs need to learn about issues that were not quite learned during pre-training. 
    % LMs learn about tasks and how to address them from human feedback. 
    % In other words, models learn about aspects of language that is not quite learned during pre-training and hence instruction-tuning with human-feedback is an indispensable step.   
    \item  \emph{Human feedback} is an \emph{optional} aspect of instruction-tuning as LMs are already quite familiar with instructions  from their pre-training. Observing the human feedback is merely a lightweight process for aligning their pre-training distribution/objective which might be replaceable with a different process. 
\end{enumerate}
While the reality probably lies somewhere in between these two extremes, we conjecture 
% \nascomment{I think you should use the verb `conjecture' -- hypothesize suggests that you're going to carry out an experiment to test it, which you don't do} 
that it is closer to \subscript{H}{2}, particularly for larger models.
This intuition, that LMs already know much about language instructions, is a key motivation for \name{} and is also supported by its  empirical success.


\subsection{Broader Impact}

Beyond the immediate focus of this paper, we believe that \name{} may help bring more transparency to what happens ``behind the scenes'' of widely-used instruction-tuned models like \gptinstruct{}. 
% \nascomment{rest of this paragraph is too wordy and repetitive.  rework to make the point concisely.} \alisa{took a stab at it}
% The parameters of these models and their training data remain hidden, limiting research progress that might otherwise prosper.
% Unfortunately, such industrial models remain behind the API walls as their datasets are not released, and hence there is little understanding of their constructions. There is no doubt that such a lack of transparency is not conducive to productive research progress.
% Now the burden falls on the shoulders of academia to better understand the source of success in these models and strive for even better, yet open, models.
Unfortunately, such industrial models remain behind the API walls as their datasets are not released, and hence there is little understanding of their constructions and why they demonstrate impressive capabilities.
% These industrial models remain behind the API walls and their datasets are not released. 
% There is no doubt that such lack of transparency is not conducive to productive research progress.
The burden now falls on academia to better understand the source of success in these models and strive for better -- yet open -- models. We believe our findings in this paper demonstrate the importance of diverse instruction data, and our large synthetic dataset can be the first step toward higher-quality data for building better instruction-following models.


% Better understanding various generations of GPT3Instruct 
% \todo{Discuss how instructGPT is trained - private user data and a lot of annotations blabla}
% \todo{Want to argue that we're isolating the algorithmic contribution from data contribution.} \yizhong{rewrote, check?}


\subsection{Limitations of \name}
Here, we discuss some limitations of this work to inspire future research in this direction.  

\paragraph{Tail phenomena.} 
\name{} depends on LMs, and it will inherit all the limitations that carry over with LMs. 
As recent studies have shown~\cite{razeghi2022impact,kandpal2022large},  \emph{tail phenomena} pose a serious challenge to the success of LMs. In other words, LMs' largest gains correspond to the frequent uses of languages (head of the language use distribution), and there are minimal gains in the low-frequency contexts. 
Similarly, in the context of this work, it would not be surprising if the majority of the gains by \name{} are skewed toward 
% popular \nascomment{I don't understand what makes a task  or instructions ``popular'' so I don't know what this means} 
tasks or instructions that present more frequently in the pre-training corpus. 
As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.



\paragraph{Dependence on large models.} 
Because of \name's dependence on the inductive biases extracted from LMs, it might work best for larger models. 
If true, this may create barriers to access for those who may not have large computing resources. 
We hope future studies will carefully study the gains as a function of model size or various other parameters. 
It is worthwhile to note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger model~\cite{wei2022finetuned}. 
% \daniel{Note: similar issue with human annotations }

\paragraph{Reinforcing LM biases.}
% It is very likely that A point of concern for the authors is whether \name{} 
% \nascomment{I don't understand what this sentence is trying to say.} The idea enabling \name{} is about iterative amplification of LM's inductive biases  \nascomment{ you seem to be mixing up ``inductive bias'' with stereotype kinds of biases.  don't do that} that pertain to instruction-following. 
A point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases (stereotypes or slurs about genders, races, etc.). 
Relatedly, one observed challenge in this process is the algorithm's difficulty in producing balanced labels, which reflected models' prior biases. 
We hope future work will hash out such details to better understand the pros and cons of the approach. 


 % \paragraph{Acceptability to LM poisoning.}

% \begin{itemize}
    % \item Some kills can never be learned
    % \item Some wrong behaviors are reinforced. does it reinforce biases?  
    % \item Might only work with large models?
    % \item Difficulty in generating datasets with balanced labels
    % \item Tail phenomena 
    % \item Obviously "there is no free lunch"; what is the catch?
% \end{itemize}

\section{Conclusion}
We introduce \name{}, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it. Our method conducts instruction-tuning of the original model on the pruned subset of generated samples. On experimenting with vanilla \gptthree{}, we observe a 33\% absolute improvement over the original model on \supernat{}. This performance is on par with \gptinstruct{001}
, which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with \name{} outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind \gptinstruct{001}. We hope \name{} can serve as the first step to align pretrained language models to follow human instructions, and future work can build on top of this data to improve instruction-following models.

\section*{Acknowledgements}
The authors would like to thank Sewon Min, Eric Wallace, Ofir Press, and other members of UWNLP and AllenNLP for their constructive feedback. 
DK is supported with a gift from the Allen Institute for AI. 

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

% \clearpage
\appendix
\onecolumn
\begin{center}
{\Large \textbf{Supplemental Material}}
\end{center}
\section{Implementation Details}
\label{sec:implementaion}

\subsection{Querying the GPT3 API}
\label{subsec:query_gpt3_api}

We use different sets of hyper-parameters when querying GPT3 API for different purposes. These hyper-parameters are found to work well with the GPT3 model (``davinci'' engine) and the other instruction-tuned \gptthree{} variants. We listed them in \autoref{tab:query-gpt3-parameters}.

\begin{table}[h]
    \centering
    \small
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\toprule
  Experiments $\downarrow$                               & Temp. & Top\_P & Freq. Penalty & Presence Penalty & Beam Size & Max Length & Stop Sequences                                                             \\ \midrule
Generating instructions          & 0..7        & 0.5    & 0                 & 2                & 1         & 1024                  & "\textbackslash{}n\textbackslash{}n", "\textbackslash{}n16", "16.", "16 ." \\
Identifying clf. tasks & 0           & 0      & 0                 & 0                & 1         & 3                     & "\textbackslash{}n", "Task:"                                               \\
Generating instances             & 0           & 0      & 0                 & 1.5              & 1         & 300                   & "Task:"                                                                    \\
Evaluating models    & 0           & 0      & 0                 & 0                & 0         & 1024                  & None (default) \\
\bottomrule
\end{tabular}
    \caption{Hyper-parameters for querying OpenAI API in different experiments.}
    \label{tab:query-gpt3-parameters}
\end{table}

\subsection{Finetuning GPT3}
\label{subsec:finetuning_details}
\gptself{} and some of our baselines are finetuned from \gptthree{} model (``davinci'' engine with 175B parameters). We conduct this finetuning via OpenAI's finetuning API\footnote{\url{https://beta.openai.com/docs/guides/fine-tuning}}. While the details of how the model is finetuned with this API are not currently available (e.g., which parameters are updated, or what the optimizer is), we tune all our models with the default hyper-parameters of this API so that the results are comparable. We only set the ``prompt\_loss\_weight'' to 0 since we find this works better in our case, and every finetuning experiment is trained for two epochs to avoid overfitting the training tasks. Finetuning is charged based on the number of tokens in the training file. Finetuning \gptself{} from the \gptthree{} model took \$338. 

\section{Prompting Templates for Data Generation}
\label{sec:prompting_templates}

\name{} relies on a number of prompting templates in order to elicit the generation from language models. Here we provide our four templates for generating the instruction (\autoref{tab:instruction_generation_template}), classifying whether an instruction represents a classification task or not (\autoref{tab:classification_task_identification_template}), generating non-classification instances with the input-first approach (\autoref{tab:input-first-generation-template}), and generating classification instances with the output-first approach (\autoref{tab:output-first-generation-template}).
\input{tables/instruction_generation_template.tex}
\input{tables/classification_task_or_not_template.tex}
\input{tables/input_first_template.tex}
\input{tables/output_first_template.tex}


\clearpage
\section{Task and Instance Examples from the Generated Instruction Data}
\input{tables/examples.tex}
\input{tables/error_examples.tex}

\end{document}
