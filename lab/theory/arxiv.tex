\documentclass{article} % For LaTeX2e
\usepackage{style/custom_iclr2021_base}
\usepackage{style/neurips_2020}
\usepackage{times}
\usepackage{booktabs}
\usepackage{caption}
\captionsetup[table]{position=bottom}
\usepackage{natbib}
\bibliographystyle{style/iclr2021_conference}
\input{style/math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage[ruled,vlined,noend]{algorithm2e}
\usepackage[normalem]{ulem}
\usepackage{float}
\usepackage{etoc}

\usepackage{xcolor}
\usepackage{enumitem}

\title{Pretrained Transformers As \\ Universal Computation Engines}

\author{Kevin Lu \\
UC Berkeley \\
\texttt{kzl@berkeley.edu} \\
\\
\textbf{Pieter Abbeel} \\
UC Berkeley \\
\texttt{pabbeel@cs.berkeley.edu}
\And
Aditya Grover \\
Facebook AI Research \\
\texttt{adityagrover@fb.com} \\
\\
\textbf{Igor Mordatch} \\
Google Brain \\
\texttt{imordatch@google.com}
}

\begin{document}

\maketitle

\begin{abstract}
\input{sections/abstract}
\end{abstract}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/main/performance.pdf}
    \vspace{0.5em}
    \includegraphics[width=0.9\linewidth]{figures/main/legend.pdf}
    \caption{
    A \emph{frozen} language-pretrained transformer (FPT) -- without finetuning the self-attention and feedforward layers -- can achieve strong performance compared to a transformer fully trained from scratch on a downstream modality on benchmarks from literature \citep{tay2020lra, rap2019tape}.
    We show results on diverse classification tasks (see Section \ref{sec:tasks}): numerical computation (Bit Memory/XOR, ListOps), image classification (MNIST, CIFAR-10), and protein fold prediction (Homology).
    We also show results for a fully trained LSTM to provide a baseline.
    }
    \label{fig:main_result}
\end{figure}

\clearpage

\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}
\tableofcontents

\clearpage

\input{sections/intro}

\input{sections/method}

\input{sections/results}

\input{sections/related_work}

\input{sections/conclusion}

\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

We would like to thank Luke Metz, Kimin Lee, Fangchen Liu, Roshan Rao, Aravind Srinivas, Nikita Kitaev, Daniel Freeman, Marc'Aurelio Ranzato, Jacob Andreas, and Ashish Vaswani for valuable feedback and discussions.
We would also like to thank members of the community for providing feedback online on an earlier version of this paper.

\clearpage

\input{sections/tables}

\clearpage

\bibliography{citations}
\addcontentsline{toc}{section}{References}

\clearpage

\appendix

\input{sections/appendix}

\end{document}
