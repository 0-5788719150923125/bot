@inproceedings{understanding,
    title={Understanding the Role of Self Attention for Efficient Speech Recognition},
    author={Shim, Kyuhong and Choi, Jungwook and Sung, Wonyong},
    booktitle={International Conference on Learning Representations},
    year={2022},
}

@inproceedings{usefulness,
  title={On the usefulness of self-attention for automatic speech recognition with transformers},
  author={Zhang, Shucong and Loweimi, Erfan and Bell, Peter and Renals, Steve},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={89--96},
  year={2021},
  organization={IEEE}
}

@article{stochastic,
  title={Stochastic attention head removal: A simple and effective method for improving transformer based asr models},
  author={Zhang, Shucong and Loweimi, Erfan and Bell, Peter and Renals, Steve},
  journal={arXiv preprint arXiv:2011.04004},
  year={2020}
}

@inproceedings{yang20i,
  author={Shuwen Yang and Andy T. Liu and Hung-yi Lee},
  title={Understanding Self-Attention of Self-Supervised Audio Transformers},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={3785--3789},
  doi={10.21437/Interspeech.2020-2231}
}

@inproceedings{layer-wise,
  title={Layer-Wise Analysis of a Self-Supervised Speech Representation Model},
  author={Pasad, Ankita and Chou, Ju-Chieh and Livescu, Karen},
  booktitle={IEEE Automatic Speech Recognition and Understanding Workshop-ASRU 2021},
  year={2021}
}

@inproceedings{conformer,
  title={Conformer: Convolution-augmented Transformer for Speech Recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  year={2020},
  booktitle={Proc. Interspeech 2020},
  pages={5036--5040},
}

@inproceedings{librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2022},
}

@inproceedings{prelu,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{pushing-semi,
  title={Pushing the limits of semi-supervised learning for automatic speech recognition},
  author={Zhang, Yu and Qin, James and Park, Daniel S and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V and Wu, Yonghui},
  journal={arXiv preprint arXiv:2010.10504},
  year={2020}
}

@inproceedings{transformer-transducer,
  title={Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss},
  author={Zhang, Qian and Lu, Han and Sak, Hasim and Tripathi, Anshuman and McDermott, Erik and Koo, Stephen and Kumar, Shankar},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7829--7833},
  year={2020},
  organization={IEEE}
}

@inproceedings{transformer-xl,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@inproceedings{pe-jhpark,
  title={Convolution-based attention model with positional encoding for streaming speech recognition on embedded devices},
  author={Park, Jinhwan and Kim, Chanwoo and Sung, Wonyong},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={30--37},
  year={2021},
  organization={IEEE}
}

@inproceedings{ctc,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006}
}

@inproceedings{sentencepiece,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={66--71},
  year={2018}
}

@article{swish,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{pushing,
  title={Pushing the limits of non-autoregressive speech recognition},
  author={Ng, Edwin G and Chiu, Chung-Cheng and Zhang, Yu and Chan, William},
  journal={arXiv preprint arXiv:2104.03416},
  year={2021}
}

@article{efficient-conformer,
  title={Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition},
  author={Burchi, Maxime and Vielzeuf, Valentin},
  journal={arXiv preprint arXiv:2109.01163},
  year={2021}
}

@inproceedings{sparse-conformer,
  author={Xiong Wang and Sining Sun and Lei Xie and Long Ma},
  title={{Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={4578--4582},
  doi={10.21437/Interspeech.2021-415}
}

@inproceedings{simplified-sa,
  title={Simplified self-attention for transformer-based end-to-end speech recognition},
  author={Luo, Haoneng and Zhang, Shiliang and Lei, Ming and Xie, Lei},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={75--81},
  year={2021},
  organization={IEEE}
}

@inproceedings{synthesizer,
  title={Transformer-based end-to-end speech recognition with local dense synthesizer attention},
  author={Xu, Menglong and Li, Shengqiang and Zhang, Xiao-Lei},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5899--5903},
  year={2021},
  organization={IEEE}
}

@inproceedings{unispeech,
  title={Unispeech: Unified speech representation learning with labeled and unlabeled data},
  author={Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  booktitle={International Conference on Machine Learning},
  pages={10937--10947},
  year={2021},
  organization={PMLR}
}

@article{wav2vec2,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{xlsr,
  title={Unsupervised cross-lingual representation learning for speech recognition},
  author={Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  journal={arXiv preprint arXiv:2006.13979},
  year={2020}
}

@article{tera,
  title={Tera: Self-supervised learning of transformer encoder representation for speech},
  author={Liu, Andy T and Li, Shang-Wen and Lee, Hung-yi},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={2351--2366},
  year={2021},
  publisher={IEEE}
}

@inproceedings{acpc,
  author={Jan Chorowski and Grzegorz Ciesielski and Jarosław Dzikowski and Adrian Łańcucki and Ricard Marxer and Mateusz Opala and Piotr Pusz and Paweł Rychlikowski and Michał Stypułkowski},
  title={{Aligned Contrastive Predictive Coding}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={976--980},
  doi={10.21437/Interspeech.2021-1544}
}

@inproceedings{wang2020large,
  title={Large-scale unsupervised pre-training for end-to-end spoken language understanding},
  author={Wang, Pengwei and Wei, Liangchen and Cao, Yong and Xie, Jinghui and Nie, Zaiqing},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7999--8003},
  year={2020},
  organization={IEEE}
}

@inproceedings{ling2020bertphone,
  title={Bertphone: Phonetically-aware encoder representations for utterance-level speaker and language recognition},
  author={Ling, Shaoshi and Salazar, Julian and Liu, Yuzong and Kirchhoff, Katrin and Amazon, A},
  booktitle={Proc. Odyssey 2020 the speaker and language recognition workshop},
  pages={9--16},
  year={2020}
}

@inproceedings{rpe-asr,
  author={Ngoc-Quan Pham and Thanh-Le Ha and Tuan-Nam Nguyen and Thai-Son Nguyen and Elizabeth Salesky and Sebastian Stüker and Jan Niehues and Alex Waibel},
  title={{Relative Positional Encoding for Speech Recognition and Direct Translation}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={31--35},
  doi={10.21437/Interspeech.2020-2526}
}

@article{cape,
  title={Cape: Encoding relative positions with continuous augmented positional embeddings},
  author={Likhomanenko, Tatiana and Xu, Qiantong and Synnaeve, Gabriel and Collobert, Ronan and Rogozhnikov, Alex},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{speechstew,
  title={Speechstew: Simply mix all available speech recognition data to train one large neural network},
  author={Chan, William and Park, Daniel and Lee, Chris and Zhang, Yu and Le, Quoc and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:2104.02133},
  year={2021}
}