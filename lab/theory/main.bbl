% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{transformer-transducer}
Q.~Zhang, H.~Lu, H.~Sak, A.~Tripathi, E.~McDermott, S.~Koo, and S.~Kumar,
  ``Transformer transducer: A streamable speech recognition model with
  transformer encoders and rnn-t loss,'' in \emph{ICASSP 2020-2020 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 7829--7833.

\bibitem{conformer}
A.~Gulati, J.~Qin, C.-C. Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang,
  Z.~Zhang, Y.~Wu, and R.~Pang, ``Conformer: Convolution-augmented transformer
  for speech recognition,'' in \emph{Proc. Interspeech 2020}, 2020, pp.
  5036--5040.

\bibitem{pushing}
E.~G. Ng, C.-C. Chiu, Y.~Zhang, and W.~Chan, ``Pushing the limits of
  non-autoregressive speech recognition,'' \emph{arXiv preprint
  arXiv:2104.03416}, 2021.

\bibitem{pushing-semi}
Y.~Zhang, J.~Qin, D.~S. Park, W.~Han, C.-C. Chiu, R.~Pang, Q.~V. Le, and Y.~Wu,
  ``Pushing the limits of semi-supervised learning for automatic speech
  recognition,'' \emph{arXiv preprint arXiv:2010.10504}, 2020.

\bibitem{speechstew}
W.~Chan, D.~Park, C.~Lee, Y.~Zhang, Q.~Le, and M.~Norouzi, ``Speechstew: Simply
  mix all available speech recognition data to train one large neural
  network,'' \emph{arXiv preprint arXiv:2104.02133}, 2021.

\bibitem{yang20i}
S.~Yang, A.~T. Liu, and H.~yi~Lee, ``Understanding self-attention of
  self-supervised audio transformers,'' in \emph{Proc. Interspeech 2020}, 2020,
  pp. 3785--3789.

\bibitem{usefulness}
S.~Zhang, E.~Loweimi, P.~Bell, and S.~Renals, ``On the usefulness of
  self-attention for automatic speech recognition with transformers,'' in
  \emph{2021 IEEE Spoken Language Technology Workshop (SLT)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2021, pp. 89--96.

\bibitem{stochastic}
------, ``Stochastic attention head removal: A simple and effective method for
  improving transformer based asr models,'' \emph{arXiv preprint
  arXiv:2011.04004}, 2020.

\bibitem{understanding}
K.~Shim, J.~Choi, and W.~Sung, ``Understanding the role of self attention for
  efficient speech recognition,'' in \emph{International Conference on Learning
  Representations}, 2022.

\bibitem{swish}
S.~Elfwing, E.~Uchibe, and K.~Doya, ``Sigmoid-weighted linear units for neural
  network function approximation in reinforcement learning,'' \emph{Neural
  Networks}, vol. 107, pp. 3--11, 2018.

\bibitem{prelu}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Delving deep into rectifiers: Surpassing
  human-level performance on imagenet classification,'' in \emph{Proceedings of
  the IEEE international conference on computer vision}, 2015, pp. 1026--1034.

\bibitem{rpe-asr}
N.-Q. Pham, T.-L. Ha, T.-N. Nguyen, T.-S. Nguyen, E.~Salesky, S.~Stüker,
  J.~Niehues, and A.~Waibel, ``{Relative Positional Encoding for Speech
  Recognition and Direct Translation},'' in \emph{Proc. Interspeech 2020},
  2020, pp. 31--35.

\bibitem{cape}
T.~Likhomanenko, Q.~Xu, G.~Synnaeve, R.~Collobert, and A.~Rogozhnikov, ``Cape:
  Encoding relative positions with continuous augmented positional
  embeddings,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, 2021.

\bibitem{transformer-xl}
Z.~Dai, Z.~Yang, Y.~Yang, J.~G. Carbonell, Q.~Le, and R.~Salakhutdinov,
  ``Transformer-xl: Attentive language models beyond a fixed-length context,''
  in \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, 2019, pp. 2978--2988.

\bibitem{pe-jhpark}
J.~Park, C.~Kim, and W.~Sung, ``Convolution-based attention model with
  positional encoding for streaming speech recognition on embedded devices,''
  in \emph{2021 IEEE Spoken Language Technology Workshop (SLT)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2021, pp. 30--37.

\bibitem{librispeech}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``Librispeech: an asr corpus
  based on public domain audio books,'' in \emph{2015 IEEE international
  conference on acoustics, speech and signal processing (ICASSP)}, 2022, pp.
  5206--5210.

\bibitem{sentencepiece}
T.~Kudo and J.~Richardson, ``Sentencepiece: A simple and language independent
  subword tokenizer and detokenizer for neural text processing,'' in
  \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing: System Demonstrations}, 2018, pp. 66--71.

\bibitem{ctc}
A.~Graves, S.~Fern{\'a}ndez, F.~Gomez, and J.~Schmidhuber, ``Connectionist
  temporal classification: labelling unsegmented sequence data with recurrent
  neural networks,'' in \emph{Proceedings of the 23rd international conference
  on Machine learning}, 2006, pp. 369--376.

\bibitem{efficient-conformer}
M.~Burchi and V.~Vielzeuf, ``Efficient conformer: Progressive downsampling and
  grouped attention for automatic speech recognition,'' \emph{arXiv preprint
  arXiv:2109.01163}, 2021.

\bibitem{sparse-conformer}
X.~Wang, S.~Sun, L.~Xie, and L.~Ma, ``{Efficient Conformer with Prob-Sparse
  Attention Mechanism for End-to-End Speech Recognition},'' in \emph{Proc.
  Interspeech 2021}, 2021, pp. 4578--4582.

\bibitem{simplified-sa}
H.~Luo, S.~Zhang, M.~Lei, and L.~Xie, ``Simplified self-attention for
  transformer-based end-to-end speech recognition,'' in \emph{2021 IEEE Spoken
  Language Technology Workshop (SLT)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2021, pp. 75--81.

\bibitem{synthesizer}
M.~Xu, S.~Li, and X.-L. Zhang, ``Transformer-based end-to-end speech
  recognition with local dense synthesizer attention,'' in \emph{ICASSP
  2021-2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  5899--5903.

\bibitem{wav2vec2}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~33, pp. 12\,449--12\,460, 2020.

\bibitem{xlsr}
A.~Conneau, A.~Baevski, R.~Collobert, A.~Mohamed, and M.~Auli, ``Unsupervised
  cross-lingual representation learning for speech recognition,'' \emph{arXiv
  preprint arXiv:2006.13979}, 2020.

\bibitem{tera}
A.~T. Liu, S.-W. Li, and H.-y. Lee, ``Tera: Self-supervised learning of
  transformer encoder representation for speech,'' \emph{IEEE/ACM Transactions
  on Audio, Speech, and Language Processing}, vol.~29, pp. 2351--2366, 2021.

\bibitem{acpc}
J.~Chorowski, G.~Ciesielski, J.~Dzikowski, A.~Łańcucki, R.~Marxer, M.~Opala,
  P.~Pusz, P.~Rychlikowski, and M.~Stypułkowski, ``{Aligned Contrastive
  Predictive Coding},'' in \emph{Proc. Interspeech 2021}, 2021, pp. 976--980.

\bibitem{unispeech}
C.~Wang, Y.~Wu, Y.~Qian, K.~Kumatani, S.~Liu, F.~Wei, M.~Zeng, and X.~Huang,
  ``Unispeech: Unified speech representation learning with labeled and
  unlabeled data,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  10\,937--10\,947.

\bibitem{ling2020bertphone}
S.~Ling, J.~Salazar, Y.~Liu, K.~Kirchhoff, and A.~Amazon, ``Bertphone:
  Phonetically-aware encoder representations for utterance-level speaker and
  language recognition,'' in \emph{Proc. Odyssey 2020 the speaker and language
  recognition workshop}, 2020, pp. 9--16.

\end{thebibliography}
