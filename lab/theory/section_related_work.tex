\section{Discussion and Related Work}

\input{table_related_work}

\paragraph{Reasoning Ability of LLMs} \looseness=-1
Several studies have shown that pre-trained models usually are not good at reasoning~\citep{brown2020language,megatron,gopher}, but its ability can be substantially increased by making them produce step-by-step reasoning, either by fine-tuning~\citep{yourself,gsm8k,star,scratchpad} or few-shot prompting~\citep{cot_wei,cot_wei_sc,palm} (See \autoref{tab:related_work} for summary of related work).
Unlike most prior work, we focus on zero-shot prompting and
show that a single fixed trigger prompt substantially increases the zero-shot reasoning ability of LLMs across a variety of tasks requiring complex multi-hop thinking (\autoref{tab:main_results}), especially when the model is scaled up (\autoref{fig:model_size}). 
It also generates reasonable and understandable \CoT across diverse tasks (\autoref{appx:further_experiment}), even when the final prediction is wrong (\autoref{appx:error_analysis}). 
Similar to our work, \citet{prompt1} demonstrate a prompt, ``Let's solve this problem by splitting it into steps'', would facilitate the multi-step reasoning in a simple arithmetic problem. However, they treated it as a task-specific example and did not evaluate quantitatively on diverse reasoning tasks against baselines. 
\citet{selftalk} propose to decompose a commonsense question into a series of information seeking question, such as ``what is the definition of \texttt{[X]}''. It does not require demonstrations but requires substantial manual prompt engineering per each reasoning task. 
Our results strongly suggest that LLMs are decent zero-shot reasoners, while prior work~\citep{cot_wei} often emphasize only few-shot learning and task-specific in-context learning, e.g. no zero-shot baselines were reported. 
Our method does not require time-consuming fine-tuning or expensive sample engineering, and can be combined with any pre-trained LLM, serving as the strongest zero-shot baseline for all reasoning tasks. 

\paragraph{Zero-shot Abilities of LLMs} \looseness=-1 \citet{Radford2019LanguageMA} show that LLMs have excellent zero-shot abilities in many \textit{system-1} tasks, including reading comprehension, translation, and summarization. 
\citet{sanh2022multitask,instructgpt} show that such zero-shot abilities of LLMs can be increased by explicitly fine-tuning models to follow instructions. 
Although these work focus on the zero-shot performances of LLMs, we focus on many \textit{system-2} tasks beyond \textit{system-1} tasks, considered a grand challenge for LLMs given flat scaling curves. 
%Our results show that Instruct-GPT3 substantially outperforms the vanilla GPT-3 regardless of the prompt strategy (\autoref{tab:model_size}), indicating that instruction tuning benefits the reasoning ability of LLMs. 
In addition, \ours is orthogonal to instruction tuning; it increases zero-shot performance for Instruct GPT3, vanilla GPT3, and PaLM (See \autoref{fig:model_size}). 

% \paragraph{Extension of~\citep{cot_wei}} While we focus on the original chain of thought prompting, several studies investigate its property and propose a method to improve the performance. 
% \citep{cot_wei_sc} proposes to produce multiple chains of thought and return the most consistent final answer in the set. They show such a simple ensemble increases across various reasoning tasks, including commonsense reasoning, where the original chain of thought prompting still struggled. 
% \citet{ye2022unreliability} shows that the explanation generated by the model itself may not even be factually grounded in the input, but these flawed explanations can still be helpful to evaluate the quality of predictions and to calibrate the model predictions using scores that are approximately assess the reliability of explanations. 
% Unlike our paper, these works only focus on a few-shot setup. 
% In addition, these advanced techniques can be combined with \ours. 
% Similar to our works, \citep{lampinen2022can} reported zero-shot performance. \sg{shall we remove this? or drastically shorten this?} \yi{let's remove for a while. }

\paragraph{From Narrow (task-specific) to Broad (multi-task) Prompting}
Most prompts are task-specific. While few-shot prompts are naturally so due to task-specific in-context samples~\citep{brown2020language,cot_wei}, majority of zero-shot prompts have also focused on per-task engineering (of templates)~\citep{liu2021pre,prompt1}. Borrowing terminologies from ~\citet{chollet2019measure} which builds on hierarchical models of intelligence~\citep{mcgrew2005cattell,johnson2005structure}, these prompts are arguably eliciting ``narrow generalization'' or task-specific skills from LLMs. On the other hand, our method is a \textit{multi-task} prompt and elicits ``broad generalization'' or broad cognitive abilities in LLMs, such as logical reasoning or \textit{system-2} itself. We hope our work can serve as a reference for accelerating not just logical reasoning research with LLMs, but also discovery of other broad cognitive capabilities within LLMs. 

% \paragraph{Task-agnostic and universal prompting}
% While most prompting is task-specific~\citep{liu2021pre}, there are examples like ours that seem to prompt generic high-level knowledge from LLMs that are universally applicable across multiple tasks. ... \sg{mention unreal engine trick and i am a professional translator?}

\paragraph{Training Dataset Details}
\label{appx:limitation_dataset}
A limitation of the work is the lack of public information on the details of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs InstructGPT~\citep{instructgpt}, and data for PaLM models~\citep{palm}. However, big performance increases from \theirsz to \ours in all recent large models (InstructGPT 001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and non-arithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a task-agnostic multi-step reasoning capability for generic problem solving. While most results are based on InstructGPT since it is the best performing open-access LLM, key results are reproduced on PaLM, and dataset details in InstructGPT (Appendix A, B, and F in~\citet{instructgpt}) also confirm that it is not specially engineered for multi-step reasoning. 

\paragraph{Limitation and Social Impact}
 Our work is based on prompting methods for large language models. LLMs have been trained on large corpora from various sources on the web
 (also see ``Training Dataset Details''),
 and have shown to capture and amplify biases found in the training data. Prompting is a method that looks to take advantage of the patterns captured by language models conducive to various tasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way to probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context learning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.
 %a potentially beneficial factor for further analyses in this area.
 % Such zero-shot probing can help testing the models for safety, fairness, morality, etc. \mr{let's not say this to not trigger people}



% This work has impact in the field of natural lan- guage processing, and develops a more efficient approach for learning performant generative mod- els. As with much of language technology has the potential to be both used for good and used maliciously. We also experiment with pretraining, learning representations in an unsupervised way, which is likely to capture and amplify biases found in the data. However, our approach has a potential positive impact given the lower cost and energy expenditure needed to train our proposed model.

%  \paragraph{Connection to instruction following models}
%  
%  \paragraph{Prompting large language models}
%  Until recently, the pre-train and the fine-tuning procedure is de-fact standard to adapt the model to downstream tasks, but it is largely replaced by prompt-based learning (prompting). 
%  Instead of updating the parameters of language models using datasets of the downstream task, prompting reformulates the downstream tasks into a language model format. 
%  For example, \citet{Radford2019LanguageMA} convert summarizing as a language modeling task by appending ``TL;DR:'' to the end of an article. 
%  Given the prompt ``TL;DR:'', language models generate subsequent sentences that summarize the article as a consequence of implicit multitask learning in language models' pre-training. 
%  
%  
%  The advantage of prompting over fine-tuning is that, given an appropriate prompt to solve each task, a single language model can solve various tasks. 
%  However, designing a good prompt is not usually straightforward; many studies try to find out good prompts for each task, which is called \textit{prompt engineering}. 
%  In general, existing prompt engineering strategy can be divided into two categories: (1) manual template engineering \citep{schick2020s,prompt1}, which uses pre-defined templates, and (2) automated template learning \citep{gao2020making,shin2020autoprompt}, which searches or optimizes templates using hold-out examples. 
%  
%  In addition, there is a large body of work looking at tackling prompting in the few-shot setting, also known as \emph{in-context learning} (ICL) \citep{brown2020language}, in which a few examples of a given task can be fed to the model as context. \citep{brown2020language,min2021metaicl,min2021noisy,lu2021fantastically,sanh2021multitask}. Furthermore, many works have also looked at exploring certain properties of in-context learning such as label imbalance \citep{min2021metaicl}, template learning versus label correctness \citep{min2022rethinking}, exploring the relationship between continuous and discrete prompts \citep{khashabi2021prompt}, as well as resources for massively multi-task ICL \citep{wang2022benchmarking}.
%  
%  
%  In constant to much of previous work, We propose to add a simple pre-defined sentence, ``Let's think step by step'', before answering the question, and thus can be categorized into manual template engineering. 
%  However, unlike most existing studies that design different templates for each task \citet{Radford2019LanguageMA}, this study is fundamentally different in that a single prompt is useful for solving many tasks. 
%  In other words, our proposed prompt elicits higher-level cognitive capabilities (multi-step reasoning) \citep{chollet2019measure}, by describing such capability by language modeling tasks. 
%  
%  % \sg{parts of (or the whole) these paragraphs should go to Background instead.}
%  % In the presence of demonstrations, we can augment the prompt with demonstrations.  
%  % %i.e., we can condition language models by \textit{answered prompt}, sequence of input and output examples, and then ask them to generate output given new input \citep{gpt3}. 
%  % For example, instead of just feeding a prompt ``1+2=?'', the prompt can be prefaced by a few demonstrations such as ``2+3=5. 4+2=6. $\cdots$''. 
%  % This approach sometimes referred to as \textit{prompt augmentation} \citep{liu2021pre}. 
%  % % This type of inference has been referred to as \textit{in-context learning} \citep{gpt3}. 
%  % We call this setup as \textit{few-shot} setup as we need to prepare several demonstrations to solve new tasks and distinguish \textit{zero-shot} setup.  
%  % Few-shot prompting naturally outperforms zero-shot prompting on a wide range of tasks, suggesting that LLMs are good in-context few-shot learners \citep{gpt3}. 
%  % However, the few-sot approach has obvious drawbacks, i.e., we need to prepare hand-crafted demonstrations for each task that LLMs would be encountered. 
%  % Moreover, as prior works demonstrated, the performance of few-shot prompting is sensitive to the choice of demonstrations \citep{gao2020making,liu2021makes,lu2021fantastically}, and even the order of examples \citep{lu2021fantastically,kumar2021reordering}. 
%  % There has been little understanding of which aspects of demonstrations contribute to downstream task performance \citep{min2022rethinking}. 
%  % 
%  % \cite{gpt3}, \cite{megatron}, and \cite{gopher} have empirically found that just scaling out model size has limited performance gains on the benchmarks that require multi-step reasoning, such as mathematics or logical reasoning. 
%  
%  \subsection*{Chain of thought prompting}
%  Recently, \cite{cot_wei} proposed chain of thought prompting to facilitate multi-step reasoning from LLMs. 
%  As shown in Fig. \ref{fig_overview_1}, the chain of thought prompting feeds LLMs with step-by-step reasoning examples rather than standard question and answer examples. 
%  Prior works \citep{cot_wei,cot_wei_sc,palm} show that \CoT prompting facilitate LLMs to generate coherent reasoning path and it helps language models to reach correct conclusion especially when combined with larger models (LaMDA \cite{lamda}, GPT-3 \cite{gpt3} or PaLM \cite{palm}.). 
%  Note that several works \cite{scratchpad,gsm8k,yourself} had shown that fine-tuning LLMs on datasets containing reasoning processes can increase the reasoning performance. However, as \cite{star} has pointed out, it is infeasible and expensive to construct such a dataset for every task. 
%  
%  From the point of prompting research, prior \CoT prompting can be categorized as few-shot prompting, as \CoT uses demonstrations augmented by a step-by-step reasoning process. 
%  In other words, prior works indicate that LLMs are good in-context few-shot reasoners; they can solve complex reasoning tasks given properly prompted few demonstrations. 
%  However, it is still under-investigated whether the LLMs are good zero-shot reasoners, except for a few exceptions \citep{selftalk,prompt1}. 
%  Similar to our works, \citet{selftalk} proposes a way to execute one-step reasoning with pre-trained LLMs in a zero-shot manner. 
%  However, they hand-crafted reasoning templates for each task and differ from our works that show a single prompt elicits the reasoning ability of LLMs across many tasks. 
%  From the methodological perspective, our works are most similar to \cite{prompt1} that show the existence of a meta prompt that can facilitate LLMs to understand the task at hand and generate appropriate sentences. 
%  However, they did not provide quantitative results and, therefore, still unclear how LLMs are good at reasoning in a zero-shot setup. 


% may be due to fact that LLMs usually struggled to solve reasoning tasks even in few-shot setup, which should be much easier than zero-shot setup, before the invention of \CoT prompting. 




% \cite{scratchpad}, \cite{gsm8k}, and \cite{yourself} have shown that the performance of language models on some tasks that require multi-step reasoning are improved by supervised training on manually curated training datasets, which include reasoning process as well as questions and the answers. However, As \cite{star} have pointed out, it is infeasible and expensive to construct such a dataset for every task.
% 
% More recently, \cite{cot_wei}, \cite{cot_wei_sc}, and \cite{palm} demonstrated that \CoT reasoning is also effective under in-context learning setting by eliciting the reasoning ability of language models by prompting few-shot examples into the model input. 
% In these researches, CoT is validated to be effective on Large Language Models (LLMs), such as LaMDA \cite{lamda}, GPT-3 \cite{gpt3} or PaLM \cite{palm}.
% \cite{star} has also shown that the step-by-step reasoning sentence generated by the pre-trained model in few-shot base (rationale) and its update (rationalization) can be used as supervised learning samples. However, this approach has two drawbacks. First, just like fine-tuning approaches, we need to create or prepare \CoT question-reasoning-answer paired examples for every task in advance. This will become a critical property when applying LLMs to practical setting, where LLMs needs immediate adaptation to infinite variety of tasks.

% Zero-shot based \CoT approach has not yet been fully explored thus far. \cite{selftalk} proposes a way to execute one-step \CoT with pre-trained LLMs in a zero-shot manner, although the proposed template is task specific so that the method needs to change the template depending on tasks. Moreover, as the method just lets the model fill in the partial blank of the specified text, it may result in restricting the intrinsic reasoning ability of the model. \kojima{\cite{prompt1} also found that a zero-shot based \CoT approach like ours is effective by showing some examples. However, it was not fully evaluated quantitatively.}
%\mr{IMO it's better to add the related work at the end for this type of paper. The idea is simple, so preliminaries are not super important to help reviewers understand the idea. In this case, it distracts from the main point}\yi{Agree, and section title should be ``Discussion and Related Works'' or something like that. }



% \yi{Relation to prompt decomposition and prompt generation}
% The advantage of prompting over fine-tuning is that, given a appropriate prompts to solve each task, a single language model can solve various tasks. 
% However, desiging prompts introduces difficulty to design the proper prompts. 

% machel? as you complete table 1, it would be nice to fill this. and mention how CoT inst is different from most prompt works (task specific vs task agnostic prompts, reference multi-hop reasoning's perceived difficulty) 



% % \sg{this paragraph should go related work.} 
% Similar to our work, prior studies have already investigated the zero-shot ability of LLMs. 
% For example, many studies focus on manual prompt engineering and show that designed instruction enhances the zero-shot ability of LLMs, especially on system-1 tasks \citep{todo}. 
% \sg{mention unreal engine trick and i am a professional translator?}
% 
% \subsection*{Connection to instruction-following models}
% 
% More recently, several studies \citep{todo} propose to train LLMs to follow instructions, and it significantly improves performances. 
% Although our work is similar to those in spirit that focuses on LLMs' zero-shot performance, it differs in some aspects. 
% (1) We focus on the zero-shot ability of LLMs on reasoning tasks, which was regarded as difficult task \yi{better sentence}. (2) Our chain-of-though instruction describes the process (multi-step thinking) that might share across tasks, rather than describing tasks as with most prior works \yi{should add one more sentence to emphasize this point}. 
% In summary, our work suggests that the zero-shot reasoning performance of LLMs was drastically underestimated in the community due to the lack of good zero-shot baselines. 
% 
% \subsection*{Dark knowledge and distillation}

% The concept of dark knowledge~\citep{}

% ... 
% shane

% gpt distill occam's razor https://proceedings.neurips.cc/paper/2021/file/0cd6a652ed1f7811192db1f700c8f0e7-Paper.pdf
% https://arxiv.org/abs/2203.14465


%\subsection{Prompt Strategy}
% unreal engine
% I'm a professional translator
%\cite{prompt1}
%\cite{prompt2}

