\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Methodology}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Tasks}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Architecture}{section.2}% 4
\BOOKMARK [1][-]{section.3}{Empirical Evaluations}{}% 5
\BOOKMARK [2][-]{subsection.3.1}{Can pretrained language models transfer to different modalities?}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.2}{What is the importance of the pretraining modality?}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.3}{How important is the transformer architecture compared to LSTM architecture?}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.4}{Does language pretraining improve compute efficiency over random initialization?}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.5}{Do the frozen attention layers attend to modality-specific tokens?}{section.3}% 10
\BOOKMARK [2][-]{subsection.3.6}{Does freezing the transformer prevent overfitting or underfitting?}{section.3}% 11
\BOOKMARK [2][-]{subsection.3.7}{Does performance scale with model size?}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.8}{Can performance be attributed simply to better statistics for initialization?}{section.3}% 13
\BOOKMARK [2][-]{subsection.3.9}{Can we train a transformer by only finetuning the output layer?}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.10}{What is the role of model depth in token mixing?}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.11}{Can training more parameters improve performance?}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.12}{Which parameters of the model are important to finetune?}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.13}{Is finetuning layer norm necessary for FPT to perform well?}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.14}{How well do the trends hold across other transformer models?}{section.3}% 19
\BOOKMARK [1][-]{section.4}{Related Work and Discussion}{}% 20
\BOOKMARK [2][-]{subsection.4.1}{Transformers in multimodal settings}{section.4}% 21
\BOOKMARK [2][-]{subsection.4.2}{Transformers in transfer settings}{section.4}% 22
\BOOKMARK [2][-]{subsection.4.3}{Pretraining and finetuning of transformer models}{section.4}% 23
\BOOKMARK [2][-]{subsection.4.4}{Self-attention layers as optimization steps}{section.4}% 24
\BOOKMARK [2][-]{subsection.4.5}{Global workspace theory}{section.4}% 25
\BOOKMARK [2][-]{subsection.4.6}{Reservoir computing}{section.4}% 26
\BOOKMARK [1][-]{section.5}{Conclusion}{}% 27
\BOOKMARK [1][-]{section*.25}{Acknowledgements}{}% 28
\BOOKMARK [1][-]{section*.32}{References}{}% 29
\BOOKMARK [1][-]{section*.32}{Appendix}{}% 30
\BOOKMARK [1][-]{appendix.A}{Summary of arXiv Updates}{}% 31
\BOOKMARK [1][-]{appendix.B}{Background on Transformers}{}% 32
\BOOKMARK [2][-]{subsection.B.1}{Self-Attention}{appendix.B}% 33
\BOOKMARK [2][-]{subsection.B.2}{Positional Embeddings}{appendix.B}% 34
\BOOKMARK [2][-]{subsection.B.3}{Layer Norm}{appendix.B}% 35
\BOOKMARK [2][-]{subsection.B.4}{Pretraining Objective}{appendix.B}% 36
\BOOKMARK [2][-]{subsection.B.5}{Model Sizes}{appendix.B}% 37
\BOOKMARK [1][-]{appendix.C}{Experimental Details}{}% 38
\BOOKMARK [1][-]{appendix.D}{Details by Table}{}% 39
\BOOKMARK [2][-]{subsection.D.1}{Can pretrained language models transfer to different modalities?}{appendix.D}% 40
\BOOKMARK [2][-]{subsection.D.2}{What is the importance of the pretraining modality?}{appendix.D}% 41
\BOOKMARK [2][-]{subsection.D.3}{How important is the transformer architecture compared to LSTM architecture?}{appendix.D}% 42
\BOOKMARK [2][-]{subsection.D.4}{Does language pretraining improve compute efficiency over random initialization?}{appendix.D}% 43
