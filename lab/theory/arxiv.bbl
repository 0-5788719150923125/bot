\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramson et~al.(2020)Abramson, Ahuja, Brussee, Carnevale, Cassin,
  Clark, Dudzik, Georgiev, Guy, Harley, et~al.]{abramson2020imitating}
Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin,
  Stephen Clark, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, et~al.
\newblock Imitating interactive intelligence.
\newblock \emph{arXiv preprint arXiv:2012.05672}, 2020.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017l3}
Jacob Andreas, Dan Klein, and Sergey Levine.
\newblock Learning with latent language.
\newblock \emph{arXiv preprint arXiv:1711.00482}, 2017.

\bibitem[Artetxe et~al.(2019)Artetxe, Ruder, and Yogatama]{artetxe2019cross}
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
\newblock On the cross-lingual transferability of monolingual representations.
\newblock \emph{arXiv preprint arXiv:1910.11856}, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layernorm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baars(1993)]{baars1993gwt}
Bernard~J Baars.
\newblock \emph{A cognitive theory of consciousness}.
\newblock Cambridge University Press, 1993.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deq}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock \emph{arXiv preprint arXiv:1909.01377}, 2019.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big.
\newblock In \emph{Proceedings of the 2020 Conference on Fairness,
  Accountability, and Transparency; Association for Computing Machinery: New
  York, NY, USA}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020gpt3}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Radford, Child, Wu, Jun, Luan,
  and Sutskever]{chen2020imagegpt}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1691--1703. PMLR, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2018)Chen, Lucic, Houlsby, and
  Gelly]{chen2018selfmodulation}
Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly.
\newblock On self modulation for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1810.01365}, 2018.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020{\natexlab{b}}.

\bibitem[Choi et~al.(2020)Choi, Grover, Singh, Shu, and Ermon]{choi2020fair}
Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon.
\newblock Fair generative modeling via weak supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1887--1898. PMLR, 2020.

\bibitem[Dai \& Le(2015)Dai and Le]{dai2015semi}
Andrew~M Dai and Quoc~V Le.
\newblock Semi-supervised sequence learning.
\newblock \emph{arXiv preprint arXiv:1511.01432}, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Donahue et~al.(2016)Donahue, Kr{\"a}henb{\"u}hl, and
  Darrell]{donahue2016bigan}
Jeff Donahue, Philipp Kr{\"a}henb{\"u}hl, and Trevor Darrell.
\newblock Adversarial feature learning.
\newblock \emph{arXiv preprint arXiv:1605.09782}, 2016.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[El-Gebali et~al.(2019)El-Gebali, Mistry, Bateman, Eddy, Luciani,
  Potter, Qureshi, Richardson, Salazar, Smart, Sonnhammer, Hirsh, Paladin,
  Piovesan, Tosatto, and Finn]{elgebali2019pfam}
Sara El-Gebali, Jaina Mistry, Alex Bateman, Sean~R Eddy, Aur{\'{e}}lien
  Luciani, Simon~C Potter, Matloob Qureshi, Lorna~J Richardson, Gustavo~A
  Salazar, Alfredo Smart, Erik L~L Sonnhammer, Layla Hirsh, Lisanna Paladin,
  Damiano Piovesan, Silvio C~E Tosatto, and Robert~D Finn.
\newblock {The Pfam protein families database in 2019}.
\newblock \emph{Nucleic Acids Research}, 47\penalty0 (D1):\penalty0 D427--D432,
  2019.
\newblock ISSN 0305-1048.
\newblock \doi{10.1093/nar/gky995}.

\bibitem[Fox et~al.(2013)Fox, Brenner, and Chandonia]{fox2013scop}
Naomi~K Fox, Steven~E Brenner, and John-Marc Chandonia.
\newblock Scope: Structural classification of proteins—extended, integrating
  scop and astral data and classification of new structures.
\newblock \emph{Nucleic acids research}, 42\penalty0 (D1):\penalty0 D304--D309,
  2013.

\bibitem[Frankle et~al.(2020)Frankle, Schwab, and Morcos]{frankle2020batchnorm}
Jonathan Frankle, David~J Schwab, and Ari~S Morcos.
\newblock Training batchnorm and only batchnorm: On the expressive power of
  random features in cnns.
\newblock \emph{arXiv preprint arXiv:2003.00152}, 2020.

\bibitem[Goh et~al.(2021)Goh, Voss, Amodei, Carter, Petrov, Wang, Cammarata,
  and Olah]{goh2021multimodal}
Gabriel Goh, Chelsea Voss, Daniela Amodei, Shan Carter, Michael Petrov,
  Justin~Jay Wang, Nick Cammarata, and Chris Olah.
\newblock Multimodal neurons in artificial neural networks.
\newblock 2021.

\bibitem[Goyal \& Bengio(2020)Goyal and Bengio]{goyal2020inductive}
Anirudh Goyal and Yoshua Bengio.
\newblock Inductive biases for deep learning of higher-level cognition.
\newblock \emph{arXiv preprint arXiv:2011.15091}, 2020.

\bibitem[Grover et~al.(2019)Grover, Song, Agarwal, Tran, Kapoor, Horvitz, and
  Ermon]{grover2019bias}
Aditya Grover, Jiaming Song, Alekh Agarwal, Kenneth Tran, Ashish Kapoor, Eric
  Horvitz, and Stefano Ermon.
\newblock Bias correction of learned generative models using likelihood-free
  importance weighting.
\newblock \emph{arXiv preprint arXiv:1906.09531}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gelu}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and
  McCandlish]{hernandez2021scaling}
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock \emph{arXiv preprint arXiv:2102.01293}, 2021.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hou et~al.(2018)Hou, Adhikari, and Cheng]{hou2018deepsf}
Jie Hou, Badri Adhikari, and Jianlin Cheng.
\newblock Deepsf: deep convolutional neural network for mapping protein
  sequences to folds.
\newblock \emph{Bioinformatics}, 34\penalty0 (8):\penalty0 1295--1303, 2018.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019adapter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019.

\bibitem[Jaeger(2001)]{jaeger2001echo}
Herbert Jaeger.
\newblock The “echo state” approach to analysing and training recurrent
  neural networks-with an erratum note.
\newblock \emph{Bonn, Germany: German National Research Center for Information
  Technology GMD Technical Report}, 148\penalty0 (34):\penalty0 13, 2001.

\bibitem[Jaeger \& Haas(2004)Jaeger and Haas]{jaeger2004harnessing}
Herbert Jaeger and Harald Haas.
\newblock Harnessing nonlinearity: Predicting chaotic systems and saving energy
  in wireless communication.
\newblock \emph{science}, 304\penalty0 (5667):\penalty0 78--80, 2004.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Tunyasuvunakool, Ronneberger, Bates, Žídek, Bridgland, Meyer, Kohl,
  Potapenko, Ballard, Cowie, Romera-Paredes, Nikolov, Jain, Adler, Back,
  Petersen, Reiman, Steinegger, Pacholska, Silver, Vinyals, Senior,
  Kavukcuoglu, Kohli, and Hassabis]{jumper2021alphafold}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex
  Bridgland, Clemens Meyer, Simon A~A Kohl, Anna Potapenko, Andrew~J Ballard,
  Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain,
  Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger,
  Michalina Pacholska, David Silver, Oriol Vinyals, Andrew~W Senior, Koray
  Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis.
\newblock High accuracy protein structure prediction using deep learning.
\newblock 2021.

\bibitem[Kaiser et~al.(2017)Kaiser, Gomez, Shazeer, Vaswani, Parmar, Jones, and
  Uszkoreit]{kaiser2017multitask}
Lukasz Kaiser, Aidan~N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion
  Jones, and Jakob Uszkoreit.
\newblock One model to learn them all.
\newblock \emph{arXiv preprint arXiv:1706.05137}, 2017.

\bibitem[Kiela et~al.(2019)Kiela, Bhooshan, Firooz, Perez, and
  Testuggine]{kiela2019supervised}
Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, and Davide Testuggine.
\newblock Supervised multimodal bitransformers for classifying images and text.
\newblock \emph{arXiv preprint arXiv:1909.02950}, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009cifar}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2020)Li, Ponti, Vuli{\'c}, and
  Korhonen]{li2020communication}
Yaoyiran Li, Edoardo~M Ponti, Ivan Vuli{\'c}, and Anna Korhonen.
\newblock Emergent communication pretraining for few-shot machine translation.
\newblock \emph{arXiv preprint arXiv:2011.00890}, 2020.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock \emph{arXiv preprint arXiv:1908.02265}, 2019.

\bibitem[Lu et~al.(2020)Lu, Goswami, Rohrbach, Parikh, and
  Lee]{lu2020vilbertmulti}
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
\newblock 12-in-1: Multi-task vision and language representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10437--10446, 2020.

\bibitem[Miconi et~al.(2018)Miconi, Stanley, and Clune]{miconi2018hebbian}
Thomas Miconi, Kenneth Stanley, and Jeff Clune.
\newblock Differentiable plasticity: training plastic neural networks with
  backpropagation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3559--3568. PMLR, 2018.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018cpc}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Papadimitriou \& Jurafsky(2020)Papadimitriou and
  Jurafsky]{papadimitriou2020music}
Isabel Papadimitriou and Dan Jurafsky.
\newblock Pretraining on non-linguistic structure as a tool for analyzing
  learning bias in language models.
\newblock \emph{arXiv preprint arXiv:2004.14601}, 2020.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu, Gulcehre,
  Jayakumar, Jaderberg, Kaufman, Clark, Noury,
  et~al.]{parisotto2020stabilizing}
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre,
  Siddhant Jayakumar, Max Jaderberg, Raphael~Lopez Kaufman, Aidan Clark, Seb
  Noury, et~al.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7487--7498. PMLR, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{arXiv preprint arXiv:1912.01703}, 2019.

\bibitem[Perez et~al.(2018)Perez, Strub, De~Vries, Dumoulin, and
  Courville]{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Ponti et~al.(2019)Ponti, Vuli{\'c}, Cotterell, Reichart, and
  Korhonen]{ponti2019towards}
Edoardo~Maria Ponti, Ivan Vuli{\'c}, Ryan Cotterell, Roi Reichart, and Anna
  Korhonen.
\newblock Towards zero-shot language modeling.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  2893--2903, 2019.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018gpt}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{Image}, 2:\penalty0 T2, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavolv, Goh, Gray, Chen, Child, Misra,
  Mishkin, Krueger, Agarwal, and Sutskever]{ramesh2021dalle}
Aditya Ramesh, Mikhail Pavolv, Gabriel Goh, Scott Gray, Mark Chen, Rewon Child,
  Vedant Misra, Pamela Mishkin, Gertchen Krueger, Sandhini Agarwal, and Ilya
  Sutskever.
\newblock Dall·e: Creating images from text, 2021.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Gruber, Holzleitner, Pavlovi{\'c}, Sandve, Greiff,
  et~al.]{ramsauer2020hopfield}
Hubert Ramsauer, Bernhard Sch{\"a}fl, Johannes Lehner, Philipp Seidl, Michael
  Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovi{\'c}, Geir~Kjetil
  Sandve, Victor Greiff, et~al.
\newblock Hopfield networks is all you need.
\newblock \emph{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem[Rao et~al.(2019)Rao, Bhattacharya, Thomas, Duan, Chen, Canny, Abbeel,
  and Song]{rap2019tape}
Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi~Chen, John Canny,
  Pieter Abbeel, and Yun~S Song.
\newblock Evaluating protein transfer learning with tape.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Rao et~al.(2021)Rao, Liu, Verkuil, Meier, Canny, Abbeel, Sercu, and
  Rives]{rao2021msa}
Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John~F. Canny, Pieter
  Abbeel, Tom Sercu, and Alexander Rives.
\newblock Msa transformer.
\newblock \emph{bioRxiv}, 2021.
\newblock \doi{10.1101/2021.02.12.430858}.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017adapter}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock \emph{arXiv preprint arXiv:1705.08045}, 2017.

\bibitem[Rumelhart et~al.(1985)Rumelhart, Hinton, and
  Williams]{rumelhart1985rnn}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for
  Cognitive Science, 1985.

\bibitem[Sheng et~al.(2019)Sheng, Chang, Natarajan, and Peng]{sheng2019woman}
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng.
\newblock The woman worked as a babysitter: On biases in language generation.
\newblock \emph{arXiv preprint arXiv:1909.01326}, 2019.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock \emph{arXiv preprint arXiv:2010.15980}, 2020.

\bibitem[Strodthoff et~al.(2020)Strodthoff, Wagner, Wenzel, and
  Samek]{strodthoff2020udsmprot}
Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek.
\newblock Udsmprot: universal deep sequence models for protein classification.
\newblock \emph{Bioinformatics}, 36\penalty0 (8):\penalty0 2401--2409, 2020.

\bibitem[Tanaka et~al.(2019)Tanaka, Yamane, H{\'e}roux, Nakane, Kanazawa,
  Takeda, Numata, Nakano, and Hirose]{tanaka2019reservoir}
Gouhei Tanaka, Toshiyuki Yamane, Jean~Benoit H{\'e}roux, Ryosho Nakane, Naoki
  Kanazawa, Seiji Takeda, Hidetoshi Numata, Daiju Nakano, and Akira Hirose.
\newblock Recent advances in physical reservoir computing: A review.
\newblock \emph{Neural Networks}, 115:\penalty0 100--123, 2019.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2020lra}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020.

\bibitem[Touvron et~al.(2020)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J\'egou]{touvron2020deit}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv\'e J\'egou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock \emph{arXiv preprint arXiv:2012.12877}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wightman(2019)]{wightman2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.

\bibitem[Wu et~al.(2021)Wu, Rabe, Li, Ba, Grosse, and Szegedy]{wu2021lime}
Yuhuai Wu, Markus Rabe, Wenda Li, Jimmy Ba, Roger Grosse, and Christian
  Szegedy.
\newblock Lime: Learning inductive bias for primitives of mathematical
  reasoning.
\newblock \emph{arXiv preprint arXiv:2101.06223}, 2021.

\end{thebibliography}
