\section{Related Works}
While older G2P systems were rule-based~\cite{elovitz1976automatic}, joint-sequence models \cite{BISANI2008434} became the new normal, as they do not require expert labor to specify hundreds of rules. Neural sequence-to-sequence methods when applied to G2P take the form of LSTM-based encoder-decoder models \cite{rao2015grapheme}. While they were initially used to predict pronunciation of individual words, as neural machine translation evolved, newer methods were applied to G2P.

LSTM-based encoder-decoder models that augment attention between encoder and decoder, allow the models to convert whole sequences at once, instead of dealing with single words \cite{peters2017massively}. As transformer-based architecture became popular \cite{Vaswani2017AttentionIA}, they were also applied to G2P \cite{yolchuyeva2020transformer}; particularly in multilingual domain \cite{yu2020multilingual,vesik2020model}. Training a transformer-based model for dialectal G2P has been shown in \cite{blandon2019multilingual}, however, they train a model for a single dialect using a multilingual G2P, and do not investigate transfer-learning, or multiple dialects.

For low-resource languages, the dataset that a single language provides did not suffice the large data requirements of transformer-based architectures, so such systems are trained on multiple languages, to help alleviate the data requirements. Our focus, rather than on the multiple language case, lies at handling dialects of a single language -- English. Here, several dialects have extremely limited datasets, and we study how the transformer model behave on cross-dialect transfer learning (in English), as the size of the training dictionary varies. To the best of our knowledge, the problem of single language cross-dialect G2P transfer has not been studied in earlier works.
% 
% 
%==============================================================================================
% The oldest G2P systems were rule-based \cite{elovitz1976automatic}.  Joint-sequence models \cite{BISANI2008434} became the new standard, not requiring expert labor to specify hundreds of rules.
% 
% Later, neural Sequence-to-Sequence methods were applied to G2P, taking the form of LSTM-based Encoder-Decoder models \cite{rao2015grapheme}.  They were initially used to predict the pronunciation of individual words.  However, as neural machine translation grew and innovated, new methods were applied to G2P.  First, the LSTM Encoder-Decoder model was augmented with attention between the encoder and decoder, allowing the models to now convert whole sequences at once, instead of dealing with single words \cite{peters2017massively}.  As the Transformer architecture became popular \cite{Vaswani2017AttentionIA}, it too was applied to G2P \cite{yolchuyeva2020transformer}, particularly in the multilingual domain \cite{yu2020multilingual} \cite{vesik2020model}.  The Transformer was applied to the multilingual setting in part due to its large data requirements, which not all languages can match, so training the systems on multiple languages helped alleviate that burden.
% 
% We focus on a similar problem of limited data. However, our focus is narrower than multiple languages, we look at handling dialects of a single language, English.  As many dialects have extremely limited data, we study how the Transformer model behaves for cross-dialect transfer learning while varying the size of the training dictionary.
% 
% 