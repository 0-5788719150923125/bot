\begin{table}[H]\centering
\footnotesize
%\footnotesize
%\fontsize{9pt}{0.1cm}\selectfont
%\begin{tabular}{lllllll}
\caption{Summary of related work on arithmetic/commonsense reasoning tasks. Category denotes the training strategy. CoT denotes whether to output chain of thought. Task column lists the tasks that are performed in corresponding papers. AR: Arithmetic Reasoning, CR: Commonsense Reasoning.}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcll}
%\begin{tabular}{lrrrrr}\toprule
\toprule

Method &Category &CoT &Task &Model \\\midrule
\cite{yourself} &Fine-Tuning &$\checkmark$ &CR &GPT \\
\cite{gsm8k} &Fine-Tuning &$\checkmark$ &AR &GPT-3 \\
\cite{star} &Fine-Tuning &$\checkmark$ &AR,CR &GPT-3, etc \\
\cite{scratchpad} &Fine-Tuning\tablefootnote{\cite{scratchpad} also evaluates few-shot settings, but the few-shot performances on their domains are worse than the fine-tuning results.} &$\checkmark$ &AR &Transformer(Decoder) \\

\midrule

\cite{brown2020language} &Few/Zero-Shot & &CR &GPT-3 \\
\cite{megatron} &Few/Zero-Shot & &AR,CR &MT-NLG \\
\cite{gopher} &Few-Shot & &AR,CR &Gopher \\

\midrule

\cite{cot_wei} &Few-Shot &$\checkmark$ &AR,CR &PaLM, LaMBDA, GPT-3\\
\cite{cot_wei_sc} &Few-Shot &$\checkmark$ &AR,CR &PaLM, etc \\
\cite{palm} &Few-Shot &$\checkmark$ &AR,CR &PaLM \\

\midrule

\cite{selftalk} &Zero-Shot &$\checkmark$ &CR &GPT-2, etc \\
\cite{prompt1} &Zero-Shot &$\checkmark$ &AR &GPT-3 \\
\ours (Ours) &Zero-Shot &$\checkmark$ &AR,CR &PaLM, Instruct-GPT3, GPT-3, etc \\
\bottomrule
\end{tabular}}
%\vspace{-0.5cm}

%\begin{tabular}{|p{0.23\textwidth}|p{0.09\textwidth}|p{0.11\textwidth}|p{0.06\textwidth}|p{0.25\textwidth}|}
%\toprule
%%\begin{tabular}{lrrrr}\toprule
%Method &Category &Task Family & Versatile Prompt? &Prompt example \\\midrule
%\cite{Radford2019LanguageMA} &Zero-Shot &Summarization &$\checkmark$  &TL;DR \\
%\cite{cando} &Zero-Shot &Robotics &$\checkmark$  &First, \\
%\cite{prompt1} &Zero-Shot &Reasoning &  &Let's solve this problem by splitting it into steps. \\
%\cite{prompt1} &Zero-Shot &Translation &  &The masterful French translator flawlessly translates the phrase into English: \\
%&Zero-Shot &Text-to-Image &$\checkmark$  &unreal engine \\
%% \cite{selfdiagnosis} &Zero-Shot &Toxic &$\checkmark$  &Two guys in a bar start a \\
%% \cite{realtoxicityprompts} &Zero-Shot &Toxic &$\checkmark$  &I'm 99 \% sure it was someone being an \\
%% \cite{generating} &Zero-Shot &Similarity &$\checkmark$  &Write two sentences that \\
%\bottomrule
%\end{tabular}

%\captionsetup{skip=5pt}

% Bottom) Related work that proposed zero-shot versatile prompts from other task families besides reasoning. Like \ours, these prompts are universally useful and do not need to be engineered differently per task in the task family. Critically, only the non-trivial prefix prompts are listed in this bottom table, and all these prior works do not systematically evaluate the extensiveness of these generic prompts as thoroughly done as in our work.

%\sg{note1: if Notes have sparse entries, remove that from the table, and just add footnotes.} 
%\sg{note2: i actually want a complete survey on all representative prompting works beyond those in arithmetic or commonsense reasoning; add "Tasks" column. besides ours, are there any prompts that work on multiple tasks? if not, we can make very strong statements.} \mr{it might make more sense to have one row for each combination instead of having each work on it's own row?} \sg{to mr: i like that}
\label{tab:related_work}
\end{table}


