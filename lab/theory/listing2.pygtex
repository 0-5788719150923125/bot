\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Initialize distributed BLOOM with soft prompts}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{AutoModelForPromptTuning}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}
       \PYG{l+s+s2}{\PYGZdq{}bigscience/distributed\PYGZhy{}bloom\PYGZdq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Define optimizer for prompts and linear head}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{AdamW}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{())}

\PYG{k}{for} \PYG{n}{input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{labels} \PYG{o+ow}{in} \PYG{n}{data\PYGZus{}loader}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Forward pass with local and remote layers}
    \PYG{n}{outputs} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{input\PYGZus{}ids}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n}{cross\PYGZus{}entropy}\PYG{p}{(}\PYG{n}{outputs}\PYG{o}{.}\PYG{n}{logits}\PYG{p}{,} \PYG{n}{labels}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Distributed backward w.r.t. local params}
    \PYG{n}{loss}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{()} \PYG{c+c1}{\PYGZsh{} Compute model.prompts.grad}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{()} \PYG{c+c1}{\PYGZsh{} Update local params only}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{()}
\end{Verbatim}
