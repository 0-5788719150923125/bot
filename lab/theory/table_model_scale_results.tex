\begin{table}[h]\centering
\caption{Model scale study. Evaluation metric is accuracy on MultiArith dataset. 
%175B-1 : text-davinci-001, 175B-2 : text-davinci-002. 
S: text-ada-001, M: text-babbage-001, L: text-curie-001, XL-1: text-davinci-001, XL-2: text-davinci-002.
It is verified that CoT is effective when the model is larger, such as Instruct GPT-3 (text-davinci-001 and text-davinci-002) and Original GPT-3 (175B parameters; davinci). In this experiment, the order of performance (ascending) is \theirsz, \theirsf (8samples), \ours, and \theirs (8samples) for davinci and text-davinci-002.}
\footnotesize

\begin{tabular}{lcc}\toprule
&\scalebox{0.93}{Original GPT-3 (0.3B / 1.3B / 6.7B / 175B)} &\scalebox{0.93}{Instruct GPT-3 (S / M / L / XL-1 / XL-2)} \\\midrule
\theirsz & 2.0 / 1.3 / 1.5 / 3.3 &3.7 / 3.8 / 4.3 / 8.0 / 17.7 \\
\theirsf & 5.2 / 5.2 / 4.0 / 8.1 &3.0 / 2.2 / 4.8 / 14.0 / 33.7 \\
\ours &1.7 / 2.2 / 2.3 / \textbf{19.0} &2.0 / 3.7 / 3.3 / \textbf{47.8} / \textbf{78.7} \\
\theirs &4.3 / 1.8 / 6.3 / \textbf{44.3} &2.5 / 2.5 / 3.8 / \textbf{36.8} / \textbf{93.0} \\
\bottomrule
\end{tabular}

\vspace*{8pt}

\begin{tabular}
{p{0.18\textwidth}p{0.13\textwidth}p{0.16\textwidth}p{0.13\textwidth}p{0.10\textwidth}p{0.10\textwidth}}
\toprule
%&\multicolumn{5}{c}{Other LMs} \\\cmidrule{2-6}
&\scalebox{0.91}{GPT-2} (1.5B)
&\scalebox{0.91}{GPT-Neo (2.7B)}
&\scalebox{0.91}{GPT-J (6B)} 
&\scalebox{0.91}{T0 (11B)} 
&\scalebox{0.91}{OPT (13B)} \\
\midrule
\theirsz &3.2 &3.0 &2.7 &2.8 &3.7 \\
\ours &2.2 &1.3 &2.5 &3.2 &2.2 \\
\bottomrule
\end{tabular}

\label{tab:model_size}
\end{table}