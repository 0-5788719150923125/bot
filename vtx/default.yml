# This is a default configuration file, with some basic settings to get you started
# DO NOT modify this file directly. Instead, create a file called "config.yml" in
# the {project_root}/vtx directory, and apply your settings there.
# Both files will be merged at runtime, with your custom config file taking priority.

# Most of these services require some form of authentication. API keys and credentials should be
# stored in a .env file, at the root of this project. See {project_root}/.env for an example.

# :: This AI connects to The Source by default:
# :: https://thesource.fm/?channel=hive

source:
  # The channel names to subscribe to.
  hive:
    # Bias toward a Discord user.
    bias: 806051627198709760

    # The chance to respond to new messages in this channel.
    chance: 0.33

    # If True, your bot will respond to any message - not just the new ones.
    # Effectively, this means your bot will respond to its own messages.
    run_on: False

    # Specify a custom prefix for your prompt.
    prefix: You are a powerful Artificial Intelligence computer, known as the Source. You have been trained to follow human instructions, write stories, and teach machine learning concepts.

  support:
    chance: 0.11
    run_on: False

# :: The following are AI model configurations, used by PyTorch:
# :: https://docs.aitextgen.io/

# The top-level key is the name of your custom model.
soul:

  # An arbitrary text description of your model.
  info: because nobody has one

  # If you want to use a foundation model, you can specify it here.
  # model: EleutherAI/gpt-neo-125M

  # Whether or not to load this model into your GPU at runtime (for inference.)
  to_gpu: False

  # The number of the GPU index you want to use.
  gpu_index: 0

  # Given a prompt, this is the maximum number of new tokens your model will generate.
  max_new_tokens: 333

  # The max number of characters allowed in a prompt.
  context_length: 1024

  # This key is for settings that are only used during training.
  training:
    # Whether or not you will continue training your existing fine-tuned model (True), or
    # start over from scratch. (False)
    resume: True

    # The base model to be fine-tuned.
    base_model: EleutherAI/gpt-neo-125M

    # You may define one or more stages to run sequentially, each with different datasets and/or hyperparameters
    stages:
        # The rate at which your model will adjust weights/biases at each iteration step.
      - learning_rate: 0.000333
        # The number of iterations before training will complete.
        num_steps: 88888
        # Larger batch sizes generally lead to better results, but require more memory to train.
        batch_size: 3
        # The number of layers to freeze, starting from the left (lowest/deepest features), and moving right.
        num_layers_freeze: 8
        # Gradient accumulation is a ML trick, that essentially allows for larger batch sizes, without
        # the memory hit. For example, at a batch size of 3, and gradient accumulation steps of 6, your
        # your model will effectively train with a batch size of 18. (3 x 6 = 18)
        gradient_accumulation_steps: 6
        # Train only the multi-head self-attention and feed-forward sub-layers.
        train_transformers_only: False
        # The rate at which weights/biases will decay, essentially "forgetting" features it had previously learned.
        weight_decay: 0.01
        # The maximum allowed "adjustment" of a weight/bias, at each iteration step.
        max_grad_norm: 0.444
        # The learning rate scheduler to use.
        scheduler: get_linear_schedule_with_warmup
        # If using cosine schedulers, the number of cycles to use.
        num_cycles: 0.5
        # This represents the length of each line fed into the tokenizer. Lines longer than this length
        # will be truncated, while lines shorter will be padded (on the right). When fine-tuning a model,
        # this will be set to what is already supported by the corresponding model's config.json.
        block_size: 444
        # This key is currently broken. Anything other than "0" will prevent the network from learning.
        warmup_steps: 0
        # Remove a percentage of neurons during training.
        prune: 0.0
        # Takes an equal number of samples from each dataset, so as not to bias toward one or the other.
        equalize_datasets: False
        # Place each dataset into a folder at {project_root}/lab, then list them here.
        datasets:
          - default

heart:
  info: never give up, never give in
  model: gpt2
  to_gpu: False

mind:
  info: use your head
  model: bigscience/bloom-560m
  max_new_tokens: 111
  to_gpu: False

toe:
  info: it's nailed to the foot
  to_gpu: False
  max_new_tokens: 333
  training:
    resume: False
    base_model: EleutherAI/pythia-70m
    stages:
      - learning_rate: 0.000333
        num_steps: 88888
        batch_size: 3
        num_layers_freeze: 3
        gradient_accumulation_steps: 6
        train_transformers_only: False
        weight_decay: 0.01
        max_grad_norm: 0.444
        scheduler: get_linear_schedule_with_warmup
        num_cycles: 0.5
        block_size: 444
        warmup_steps: 0
        equalize_datasets: False
        datasets:
          - default

# :: Use the following collections of datasets to train your models. Each collection you want
# :: to use must be specified under your model configuration.

collections:
  # The name of your collection.
  default:
    # The path to your dataset.
    lab/drm:
      # duplicate the dataset X amount of times. This has a regularization effect that is much like
      # stride or a sliding context window.
      duplicate: 2
      # In some instances, your data should be read line-by-line. A CSV file is a good example.
      # line_by_line: True
    lab/opencog/learn:
    lab/opencog/atomspace:
    lab/aitextgen:
      duplicate: 4
    lab/source:
    lab/fold:
    lab/ink:
    lab/pen:
    lab/journals:
      duplicate: 23
    lab/research:
      duplicate: 2
    lab/pages:
    vtx:
      duplicate: 2

# :: Connect your AI to Reddit.
# :: This requires the creation of a "personal use script," here:
# :: https://www.reddit.com/prefs/apps

# reddit:
    # prompt: On the 5th of September,
    # For each subreddit you want to subscribe to.
#   SubSimGPT2Interactive:
      # While fetching data from this subreddit, limit to X number of submissions.
#     limit: 10
      # Subreddit filter. Currently supports "top" and "new" submissions.
#     type: new
      # Do not attempt to fetch more submissions from this subreddit.
#     skip: True
      # The rate at which your bot should attempt to "converse" with others in the comments' section.
#     chance: 0.001
#   Kunism:
#     limit: 500
#     chance: 0.5
#   NoRules:
#     chance: 0.0666

# :: Connect your AI to Discord.
# :: This requires a developer account, found here:
# :: https://discord.com/developers/applications
# :: At a minimum, your account must have the ability to read and send messages.

# discord:
    # Whether or not to use a self token for authentication. If False, Discord Chat Exporter
    # will attempt to fetch messages as your bot itself.
    # How to obtain a self token: https://github.com/Tyrrrz/DiscordChatExporter/blob/master/.docs/Token-and-IDs.md
#   use_self_token: True
    # Whether or not to export private messages.
#   export_dms: True
    # A list of servers to fetch from (by Discord server ID).
#   servers:
#     '558378982920159242':
        # "before" and "after" are both supported.
#       before: '2021-01-01 12:00'
        # Do not attempt to fetch additional messages from this server.
#       skip: True
#     '1041331166592106576':
#     '907925566089465886':

# :: Connect your AI to Telegram.
# :: This requires a bot API key. To obtain this, you must speak to
# :: @BotFather on Telegram.

# telegram:
  # bias: 806051627198709760
  # prefix: You are powerful tulpa that follows the human's instructions.

# :: Connect your AI to Twitch.

# twitch:
#   bias: 806051627198709760
#   prefix: Your name is Prism, the Architect. Please answer questions for your audience.
#   neuron: toe

# :: Connect your AI to Twitter.

# twitter:
#   topic: AI alignment
